{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cb846e-a71b-4713-8530-cd418c53f9fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802e9c8-a8ca-4301-861b-511508e92ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class LoRATuningModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, lora_rank=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "            r=lora_rank,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        \n",
    "        self.base_model = get_peft_model(self.base_model, lora_config)\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        auc = 0.5\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, auc, cm\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'microsoft/unixcoder-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing LoRA PEFT Model...\")\n",
    "    model = LoRATuningModel(model_name, num_classes=2, lora_rank=8).to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING LORA PEFT\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, _ = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val F1:   {val_f1:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1, test_auc, test_cm = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  Loss:      {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Precision: {test_prec:.4f}\")\n",
    "    print(f\"  Recall:    {test_rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {test_auc:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LORA PEFT COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f45def-795b-4b30-9fd2-d53ccfb9f489",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prefix tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f7122-75ec-41ad-b1ca-0cfe7996ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class PrefixEncoder(nn.Module):\n",
    "    def __init__(self, prefix_length, d_model):\n",
    "        super().__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.prefix_embedding = nn.Parameter(torch.randn(prefix_length, d_model))\n",
    "        \n",
    "        self.prefix_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, d_model)\n",
    "        )\n",
    "        \n",
    "        nn.init.normal_(self.prefix_embedding, std=0.02)\n",
    "        \n",
    "    def forward(self, batch_size):\n",
    "        prefix_tokens = self.prefix_embedding.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        prefix_output = self.prefix_mlp(prefix_tokens)\n",
    "        return prefix_output\n",
    "\n",
    "\n",
    "class PrefixTuningModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, prefix_length=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        self.prefix_encoder = PrefixEncoder(prefix_length, self.d_model)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.prefix_length = prefix_length\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        prefix_embeddings = self.prefix_encoder(batch_size)\n",
    "        \n",
    "        inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "        \n",
    "        inputs_embeds = torch.cat([prefix_embeddings, inputs_embeds], dim=1)\n",
    "        \n",
    "        prefix_attention_mask = torch.ones(batch_size, self.prefix_length, dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "        attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        \n",
    "        outputs = self.base_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, self.prefix_length, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        auc = 0.5\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, auc, cm\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'microsoft/unixcoder-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('//traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/Utestcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing Prefix-Tuning Model...\")\n",
    "    model = PrefixTuningModel(model_name, num_classes=2, prefix_length=20).to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=5e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING PREFIX-TUNING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, _ = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val F1:   {val_f1:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1, test_auc, test_cm = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  Loss:      {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Precision: {test_prec:.4f}\")\n",
    "    print(f\"  Recall:    {test_rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {test_auc:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREFIX-TUNING COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c55d4-3058-4010-9d1c-ca42cc3ef70f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BitFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d6458-2daa-488b-8d45-91334a4cc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class BitFitModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "            if 'bias' in name:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        auc = 0.5\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, auc, cm\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'microsoft/unixcoder-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing BitFit Model...\")\n",
    "    model = BitFitModel(model_name, num_classes=2).to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING BITFIT\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1, val_auc, _ = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val F1:   {val_f1:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1, test_auc, test_cm = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  Loss:      {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Precision: {test_prec:.4f}\")\n",
    "    print(f\"  Recall:    {test_rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {test_auc:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(test_cm)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BITFIT COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0f3fa-215e-4f4b-977b-a56acd862530",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Adpater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986dc774-770e-4896-bc50-1467caad108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score, \n",
    "    confusion_matrix, classification_report, average_precision_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class AdapterLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, adapter_size=64):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(hidden_size, adapter_size)\n",
    "        self.up_project = nn.Linear(adapter_size, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        nn.init.normal_(self.down_project.weight, std=0.01)\n",
    "        nn.init.zeros_(self.down_project.bias)\n",
    "        nn.init.normal_(self.up_project.weight, std=0.01)\n",
    "        nn.init.zeros_(self.up_project.bias)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.down_project(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.up_project(hidden_states)\n",
    "        return hidden_states + residual\n",
    "\n",
    "\n",
    "class AdapterTuningModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, adapter_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        self.adapters = nn.ModuleList([\n",
    "            AdapterLayer(self.d_model, adapter_size) \n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._register_adapter_hooks()\n",
    "        \n",
    "    def _register_adapter_hooks(self):\n",
    "        def create_hook(adapter_layer):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, tuple):\n",
    "                    hidden_states = output[0]\n",
    "                else:\n",
    "                    hidden_states = output\n",
    "                    \n",
    "                adapted_hidden_states = adapter_layer(hidden_states)\n",
    "                \n",
    "                if isinstance(output, tuple):\n",
    "                    return (adapted_hidden_states,) + output[1:]\n",
    "                else:\n",
    "                    return adapted_hidden_states\n",
    "            return hook\n",
    "        \n",
    "        for idx, layer in enumerate(self.base_model.encoder.layer):\n",
    "            self.adapter_hooks = []\n",
    "            hook = layer.register_forward_hook(create_hook(self.adapters[idx]))\n",
    "            self.adapter_hooks.append(hook)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'microsoft/unixcoder-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing Adapter Tuning Model...\")\n",
    "    model = AdapterTuningModel(model_name, num_classes=2, adapter_size=64).to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING ADAPTER TUNING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_labels, val_preds, val_probs = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_metrics = compute_comprehensive_metrics(val_labels, val_preds, val_probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_metrics['accuracy']:.4f} | Val F1 (Macro): {val_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_metrics['f1_macro'] > best_f1:\n",
    "            best_f1 = val_metrics['f1_macro']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_labels, test_preds, test_probs = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    test_metrics = compute_comprehensive_metrics(test_labels, test_preds, test_probs)\n",
    "    \n",
    "    print_comprehensive_metrics(test_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ADAPTER TUNING COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1518e64-7df7-4230-a08e-d6842298cda1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TS PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f7c33-eca0-459d-9221-6ecd20abb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row['func'])\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4, alpha=1.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return result * self.scaling\n",
    "\n",
    "\n",
    "class TSPEFTLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank=4, alpha=1.0, dropout=0.0, s=4e-5, lambda_reg=1e-5, beta1=0.9, beta2=0.98, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.base_layer.requires_grad_(False)\n",
    "        \n",
    "        self.lora = LoRALayer(\n",
    "            base_layer.in_features,\n",
    "            base_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        self.s = s\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.register_buffer('tau', torch.tensor(0.0))\n",
    "        self.register_buffer('m', torch.tensor(0.0))\n",
    "        self.register_buffer('v', torch.tensor(0.0))\n",
    "        self.register_buffer('step', torch.tensor(0))\n",
    "        \n",
    "    def compute_relative_magnitude(self, base_output, lora_output):\n",
    "        base_norm = torch.norm(base_output, p=2, dim=-1, keepdim=True)\n",
    "        lora_norm = torch.norm(lora_output, p=2, dim=-1, keepdim=True)\n",
    "        r_i = lora_norm / (base_norm + self.eps)\n",
    "        return r_i.squeeze(-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        base_output = self.base_layer(x)\n",
    "        lora_output = self.lora(x)\n",
    "        \n",
    "        if not self.training:\n",
    "            r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "            gate = (r_i >= self.tau).float().unsqueeze(-1)\n",
    "            return base_output + gate * lora_output\n",
    "        \n",
    "        r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "        gate = (r_i >= self.tau).float()\n",
    "        \n",
    "        gated_output = base_output + gate.unsqueeze(-1) * lora_output\n",
    "        \n",
    "        self._cache_for_backward = {\n",
    "            'r_i': r_i,\n",
    "            'gate': gate,\n",
    "            'lora_output': lora_output,\n",
    "            'base_output': base_output,\n",
    "        }\n",
    "        \n",
    "        return gated_output\n",
    "    \n",
    "    def compute_threshold_gradient(self, grad_output):\n",
    "        if not hasattr(self, '_cache_for_backward'):\n",
    "            return 0.0\n",
    "            \n",
    "        cache = self._cache_for_backward\n",
    "        r_i = cache['r_i']\n",
    "        gate = cache['gate']\n",
    "        lora_output = cache['lora_output']\n",
    "        \n",
    "        mu_i = (grad_output * lora_output).sum(dim=-1)\n",
    "        \n",
    "        consistency_mask = ((mu_i >= 0).float() == gate).float()\n",
    "        sparsity_mask = gate\n",
    "        \n",
    "        grad_loss = -self.s * (consistency_mask * mu_i).sum()\n",
    "        grad_sparsity = -self.s * (sparsity_mask * self.lambda_reg).sum()\n",
    "        \n",
    "        g_k = grad_loss + grad_sparsity\n",
    "        \n",
    "        return g_k.item()\n",
    "    \n",
    "    def update_threshold(self, grad_output, lr=1.0):\n",
    "        if not self.training:\n",
    "            return\n",
    "            \n",
    "        g_k = self.compute_threshold_gradient(grad_output)\n",
    "        \n",
    "        self.step += 1\n",
    "        \n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g_k\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g_k ** 2)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1 ** self.step.item())\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.step.item())\n",
    "        \n",
    "        tau_update = lr * self.s * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "        self.tau = torch.clamp(self.tau + tau_update, min=0.0)\n",
    "        \n",
    "        if hasattr(self, '_cache_for_backward'):\n",
    "            delattr(self, '_cache_for_backward')\n",
    "\n",
    "\n",
    "class TSPEFTVulnerabilityModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, rank=32, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.hidden_size\n",
    "        \n",
    "        encoder = self.base_model.encoder\n",
    "        self.ts_peft_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, layer in enumerate(encoder.layer):\n",
    "            q_proj = layer.attention.self.query\n",
    "            v_proj = layer.attention.self.value\n",
    "            \n",
    "            self.ts_peft_layers[f'encoder_q_{i}'] = TSPEFTLayer(\n",
    "                q_proj, rank=rank, alpha=alpha, dropout=dropout, s=s, lambda_reg=lambda_reg\n",
    "            )\n",
    "            self.ts_peft_layers[f'encoder_v_{i}'] = TSPEFTLayer(\n",
    "                v_proj, rank=rank, alpha=alpha, dropout=dropout, s=s, lambda_reg=lambda_reg\n",
    "            )\n",
    "            \n",
    "            layer.attention.self.query = self.ts_peft_layers[f'encoder_q_{i}']\n",
    "            layer.attention.self.value = self.ts_peft_layers[f'encoder_v_{i}']\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def encode_with_ts_peft(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def update_thresholds(self, lr=1.0):\n",
    "        for layer in self.ts_peft_layers.values():\n",
    "            if hasattr(layer, '_cache_for_backward') and layer.training:\n",
    "                grad_output = torch.ones_like(layer._cache_for_backward['base_output'])\n",
    "                layer.update_threshold(grad_output, lr)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        encoded = self.encode_with_ts_peft(input_ids, attention_mask)\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            return loss, logits\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, _ = model(input_ids, attention_mask, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        model.update_thresholds(lr=1.0)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return all_labels, all_preds, all_probs\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'microsoft/unixcoder-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing TS-PEFT Model for Vulnerability Detection...\")\n",
    "    model = TSPEFTVulnerabilityModel(model_name, num_classes=2, rank=32, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING TS-PEFT FOR VULNERABILITY DETECTION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        labels, preds, probs = evaluate(model, test_loader, device)\n",
    "        metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1_binary']:.4f} | AUC: {metrics['roc_auc_binary']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['f1_binary'] > best_f1:\n",
    "            best_f1 = metrics['f1_binary']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    labels, preds, probs = evaluate(model, test_loader, device)\n",
    "    final_metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "    \n",
    "    print_comprehensive_metrics(final_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TS-PEFT VULNERABILITY DETECTION COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367be233-df24-43f7-9216-69081164596a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GateRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61a85d-06c0-4de9-b416-ac65ad77a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row['func'])\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class GatingModule(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.gate_linear = nn.Linear(input_dim, 1, bias=True)\n",
    "        nn.init.zeros_(self.gate_linear.weight)\n",
    "        nn.init.zeros_(self.gate_linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate_linear(x)\n",
    "        gate_values = torch.sigmoid(gate_logits)\n",
    "        return gate_values\n",
    "\n",
    "\n",
    "class GateRALayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(input_dim, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, output_dim))\n",
    "        \n",
    "        self.gating_module = GatingModule(input_dim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.base_layer(x)\n",
    "        \n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, hidden_dim = x.shape\n",
    "            x_2d = x.reshape(-1, hidden_dim)\n",
    "        else:\n",
    "            x_2d = x\n",
    "            batch_size = None\n",
    "            seq_len = None\n",
    "        \n",
    "        gate_values = self.gating_module(x_2d)\n",
    "        \n",
    "        lora_output = x_2d @ self.lora_A @ self.lora_B\n",
    "        lora_output = self.dropout_layer(lora_output)\n",
    "        \n",
    "        gated_lora_output = gate_values * lora_output * self.scaling\n",
    "        \n",
    "        if batch_size is not None and seq_len is not None:\n",
    "            gated_lora_output = gated_lora_output.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        final_output = base_output + gated_lora_output\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "\n",
    "class GateRAVulnerabilityModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, rank=16, alpha=16.0, dropout=0.0, entropy_reg_weight=0.01):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.entropy_reg_weight = entropy_reg_weight\n",
    "        \n",
    "        self.gatera_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, layer in enumerate(self.base_model.encoder.layer):\n",
    "            q_proj = layer.attention.self.query\n",
    "            v_proj = layer.attention.self.value\n",
    "            \n",
    "            q_gatera = GateRALayer(\n",
    "                q_proj, rank=rank, alpha=alpha, dropout=dropout, \n",
    "                input_dim=self.hidden_size, output_dim=self.hidden_size\n",
    "            )\n",
    "            v_gatera = GateRALayer(\n",
    "                v_proj, rank=rank, alpha=alpha, dropout=dropout,\n",
    "                input_dim=self.hidden_size, output_dim=self.hidden_size\n",
    "            )\n",
    "            \n",
    "            self.gatera_layers[f'encoder_q_{i}'] = q_gatera\n",
    "            self.gatera_layers[f'encoder_v_{i}'] = v_gatera\n",
    "            \n",
    "            layer.attention.self.query = q_gatera\n",
    "            layer.attention.self.value = v_gatera\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def compute_entropy_loss(self, gate_values):\n",
    "        eps = 1e-8\n",
    "        gate_values = torch.clamp(gate_values, eps, 1.0 - eps)\n",
    "        entropy = -gate_values * torch.log(gate_values) - (1 - gate_values) * torch.log(1 - gate_values)\n",
    "        return entropy.mean()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        if labels is not None:\n",
    "            classification_loss = F.cross_entropy(logits, labels)\n",
    "            \n",
    "            if self.training and self.entropy_reg_weight > 0:\n",
    "                total_entropy_loss = 0.0\n",
    "                gate_count = 0\n",
    "                \n",
    "                for name, layer in self.gatera_layers.items():\n",
    "                    if hasattr(layer, 'gating_module'):\n",
    "                        try:\n",
    "                            dummy_input = torch.randn(\n",
    "                                256,\n",
    "                                layer.lora_A.shape[0],\n",
    "                                device=input_ids.device\n",
    "                            )\n",
    "                            gate_vals = layer.gating_module(dummy_input)\n",
    "                            entropy_loss = self.compute_entropy_loss(gate_vals)\n",
    "                            total_entropy_loss += entropy_loss\n",
    "                            gate_count += 1\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "                \n",
    "                if gate_count > 0:\n",
    "                    avg_entropy_loss = total_entropy_loss / gate_count\n",
    "                    total_loss = classification_loss + self.entropy_reg_weight * avg_entropy_loss\n",
    "                else:\n",
    "                    total_loss = classification_loss\n",
    "            else:\n",
    "                total_loss = classification_loss\n",
    "            \n",
    "            return total_loss, logits\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, _ = model(input_ids, attention_mask, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return all_labels, all_preds, all_probs\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'microsoft/unixcoder-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing GateRA Model for Vulnerability Detection...\")\n",
    "    model = GateRAVulnerabilityModel(model_name, num_classes=2, rank=16, alpha=16.0, dropout=0.0, entropy_reg_weight=0.01).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING GateRA FOR VULNERABILITY DETECTION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        labels, preds, probs = evaluate(model, test_loader, device)\n",
    "        metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1_binary']:.4f} | AUC: {metrics['roc_auc_binary']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['f1_binary'] > best_f1:\n",
    "            best_f1 = metrics['f1_binary']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    labels, preds, probs = evaluate(model, test_loader, device)\n",
    "    final_metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "    \n",
    "    print_comprehensive_metrics(final_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GateRA VULNERABILITY DETECTION COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
