{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c93dee5-6887-486b-8283-99b114c1de93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GateRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e5aa5-a1a8-490f-b510-bd751b832b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, balanced_accuracy_score, \n",
    "    matthews_corrcoef, roc_auc_score, average_precision_score, \n",
    "    f1_score, jaccard_score, cohen_kappa_score, log_loss, brier_score_loss\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Optional, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GateRAConfig:\n",
    "    rank: int = 16\n",
    "    alpha: float = 16.0\n",
    "    dropout: float = 0.0\n",
    "    target_modules: List[str] = None\n",
    "    entropy_reg_weight: float = 0.01\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"]\n",
    "\n",
    "\n",
    "class GatingModule(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.gate_linear = nn.Linear(input_dim, 1, bias=True)\n",
    "        nn.init.zeros_(self.gate_linear.weight)\n",
    "        nn.init.zeros_(self.gate_linear.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate_logits = self.gate_linear(x)\n",
    "        gate_values = torch.sigmoid(gate_logits)\n",
    "        return gate_values\n",
    "\n",
    "\n",
    "class GateRALayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_layer: nn.Module,\n",
    "        rank: int,\n",
    "        alpha: float,\n",
    "        dropout: float,\n",
    "        input_dim: int,\n",
    "        output_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(input_dim, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, output_dim))\n",
    "        \n",
    "        self.gating_module = GatingModule(input_dim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.base_layer.weight\n",
    "    \n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.base_layer.bias if hasattr(self.base_layer, 'bias') else None\n",
    "    \n",
    "    @property\n",
    "    def in_features(self):\n",
    "        return self.base_layer.in_features\n",
    "    \n",
    "    @property\n",
    "    def out_features(self):\n",
    "        return self.base_layer.out_features\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base_output = self.base_layer(x)\n",
    "        \n",
    "        original_shape = x.shape\n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, hidden_dim = x.shape\n",
    "            x_flat = x.view(-1, hidden_dim)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            batch_size, seq_len = None, None\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            x_flat_fp32 = x_flat.float()\n",
    "            gate_values = self.gating_module(x_flat_fp32)\n",
    "            \n",
    "            lora_output = torch.matmul(x_flat_fp32, self.lora_A.float())\n",
    "            lora_output = torch.matmul(lora_output, self.lora_B.float())\n",
    "            lora_output = self.dropout_layer(lora_output)\n",
    "            \n",
    "            modulated_output = (gate_values * lora_output * self.scaling).to(base_output.dtype)\n",
    "        \n",
    "        if batch_size is not None and seq_len is not None:\n",
    "            modulated_output = modulated_output.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return base_output + modulated_output\n",
    "    \n",
    "\n",
    "class CodeCloneDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GateRACodeT5(nn.Module):\n",
    "    def __init__(self, model_name: str, gatera_config: GateRAConfig, num_classes: int = 2, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Initializing model across {num_gpus} GPUs\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(\"Loading base LLM model...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float32, \n",
    "            cache_dir=cache_dir, \n",
    "            low_cpu_mem_usage=True, \n",
    "            device_map='auto'\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        if hasattr(self.llm, 'gradient_checkpointing_enable'):\n",
    "            self.llm.gradient_checkpointing_enable()\n",
    "        self.gatera_config = gatera_config\n",
    "\n",
    "        encoder_layers = len(base_model.encoder.block)\n",
    "        decoder_layers = len(base_model.decoder.block)\n",
    "        total_layers = encoder_layers + decoder_layers\n",
    "        print(f\"Model has {encoder_layers} encoder layers and {decoder_layers} decoder layers (total: {total_layers})\")\n",
    "        self.layer_devices = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer_device = next(base_model.encoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Encoder Layer {i} -> {layer_device}\")\n",
    "        for i in range(decoder_layers):\n",
    "            layer_device = next(base_model.decoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Decoder Layer {i} -> {layer_device}\")\n",
    "        embed_device = next(base_model.encoder.embed_tokens.parameters()).device\n",
    "        final_device = next(base_model.decoder.final_layer_norm.parameters()).device\n",
    "        print(f\"Embeddings -> {embed_device}\")\n",
    "        print(f\"Final Layer -> {final_device}\")\n",
    "        self.embed_device = str(embed_device)\n",
    "        self.final_device = str(final_device)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.gatera_layers = nn.ModuleDict()\n",
    "        self._inject_gatera_layers()\n",
    "        self.num_classes = num_classes\n",
    "        hidden_size = base_model.config.d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 4, num_classes)\n",
    "        ).to(self.primary_device).float()\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Model initialization complete!\")\n",
    "    \n",
    "    def _inject_gatera_layers(self):\n",
    "        trainable_params = 0\n",
    "        layer_counter = 0\n",
    "        for name, module in self.llm.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                layer_name = name.split('.')[-1]\n",
    "                \n",
    "                should_inject = any(target in layer_name for target in self.gatera_config.target_modules)\n",
    "                \n",
    "                if should_inject:\n",
    "                    input_dim = module.in_features\n",
    "                    output_dim = module.out_features\n",
    "                    \n",
    "                    gatera_layer = GateRALayer(\n",
    "                        base_layer=module,\n",
    "                        rank=self.gatera_config.rank,\n",
    "                        alpha=self.gatera_config.alpha,\n",
    "                        dropout=self.gatera_config.dropout,\n",
    "                        input_dim=input_dim,\n",
    "                        output_dim=output_dim\n",
    "                    )\n",
    "                    \n",
    "                    layer_device = next(module.parameters()).device\n",
    "                    gatera_layer = gatera_layer.to(layer_device)\n",
    "                    \n",
    "                    safe_key = f\"gatera_layer_{layer_counter}\"\n",
    "                    self.gatera_layers[safe_key] = gatera_layer\n",
    "                    layer_counter += 1\n",
    "                    \n",
    "                    parent = self.llm\n",
    "                    if parent_name:\n",
    "                        for part in parent_name.split('.'):\n",
    "                            parent = getattr(parent, part)\n",
    "                    setattr(parent, layer_name, gatera_layer)\n",
    "                    \n",
    "                    trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                    trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "        \n",
    "        print(f\"GateRA enabled with {trainable_params:,} trainable parameters across {len(self.gatera_layers)} layers\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        input_ids = input_ids.to(self.embed_device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.embed_device)\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            decoder_input_ids=input_ids, \n",
    "            output_hidden_states=True, \n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden_states = outputs.decoder_hidden_states[-1]\n",
    "        if str(hidden_states.device) != self.primary_device:\n",
    "            hidden_states = hidden_states.to(self.primary_device)\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        entropy_loss = None\n",
    "        if labels is not None:\n",
    "            if labels.device != logits.device:\n",
    "                labels = labels.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            task_loss = loss_fct(logits, labels)\n",
    "            \n",
    "            total_entropy_loss = 0.0\n",
    "            gate_count = 0\n",
    "            batch_size = input_ids.shape[0]\n",
    "            \n",
    "            if len(self.gatera_layers) > 0:\n",
    "                sample_layer = next(iter(self.gatera_layers.values()))\n",
    "                if hasattr(sample_layer, 'gating_module') and hasattr(sample_layer, 'lora_A'):\n",
    "                    try:\n",
    "                        dummy_input = torch.randn(\n",
    "                            min(batch_size, 2),\n",
    "                            sample_layer.lora_A.shape[0],\n",
    "                            device=sample_layer.lora_A.device,\n",
    "                            dtype=torch.float32\n",
    "                        )\n",
    "                        gate_vals = sample_layer.gating_module(dummy_input)\n",
    "                        eps = 1e-8\n",
    "                        gate_vals = torch.clamp(gate_vals, eps, 1.0 - eps)\n",
    "                        entropy = -gate_vals * torch.log(gate_vals) - (1 - gate_vals) * torch.log(1 - gate_vals)\n",
    "                        total_entropy_loss = entropy.mean()\n",
    "                        gate_count = 1\n",
    "                        del dummy_input, gate_vals, entropy\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if gate_count > 0:\n",
    "                entropy_loss = total_entropy_loss\n",
    "            else:\n",
    "                entropy_loss = torch.tensor(0.0, device=task_loss.device)\n",
    "            \n",
    "            loss = task_loss + self.gatera_config.entropy_reg_weight * entropy_loss\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss, 'entropy_loss': entropy_loss, 'hidden_states': hidden_states}\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model: GateRACodeT5):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    gatera_params = sum(p.numel() for p in model.gatera_layers.parameters() if p.requires_grad)\n",
    "    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "    trainable_params = gatera_params + classifier_params\n",
    "    print(f\"\\n{'='*80}\\nPARAMETER STATISTICS\\n{'='*80}\")\n",
    "    print(f\"Total Frozen LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable GateRA Parameters: {gatera_params:,}\")\n",
    "    print(f\"Trainable Classifier Parameters: {classifier_params:,}\")\n",
    "    print(f\"Total Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    print(f\"GateRA Rank: {model.gatera_config.rank}\")\n",
    "    print(f\"GateRA Alpha: {model.gatera_config.alpha}\")\n",
    "    print(f\"Entropy Regularization Weight: {model.gatera_config.entropy_reg_weight}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\\nGPU MEMORY USAGE\\n{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"GPU {i}: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(csv_path, label_col, func1_col, func2_col):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded dataset: {len(df)} samples\")\n",
    "        print(f\"Label distribution: {df[label_col].value_counts().to_dict()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_training_data(csv_path, tokenizer, max_length=512, label_col='label', func1_col='func1', func2_col='func2'):\n",
    "    df = load_dataset(csv_path, label_col, func1_col, func2_col)\n",
    "    if df is None:\n",
    "        return []\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        func1 = str(row[func1_col])\n",
    "        func2 = str(row[func2_col])\n",
    "        label = int(row[label_col])\n",
    "        combined_text = f\"Code1: {func1}\\nCode2: {func2}\\nAre these code clones?\"\n",
    "        encoding = tokenizer(\n",
    "            combined_text, \n",
    "            return_tensors='pt', \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        )\n",
    "        train_data.append({\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        })\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(embeddings, labels, k_values=[1, 3, 5, 10]):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(labels)):\n",
    "            true_label = labels[i]\n",
    "            sims = similarities[i].clone()\n",
    "            sims[i] = -float('inf')\n",
    "            top_k_indices = torch.topk(sims, min(k, len(labels)-1)).indices\n",
    "            top_k_labels = labels[top_k_indices]\n",
    "            if true_label in top_k_labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        results[f'recall@{k}'] = correct / total if total > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_mrr(embeddings, labels):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    mrr_sum = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        true_label = labels[i]\n",
    "        sims = similarities[i].clone()\n",
    "        sims[i] = -float('inf')\n",
    "        sorted_indices = torch.argsort(sims, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        for rank, label in enumerate(sorted_labels, 1):\n",
    "            if label == true_label:\n",
    "                mrr_sum += 1.0 / rank\n",
    "                break\n",
    "        count += 1\n",
    "    return mrr_sum / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba=None, embeddings=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape != (2, 2):\n",
    "        cm_2x2 = np.zeros((2, 2))\n",
    "        for i in range(min(cm.shape[0], 2)):\n",
    "            for j in range(min(cm.shape[1], 2)):\n",
    "                cm_2x2[i, j] = cm[i, j]\n",
    "        cm = cm_2x2\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    jacc = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_proba)\n",
    "            pr_auc = average_precision_score(y_true, y_proba)\n",
    "            logloss = log_loss(y_true, np.column_stack([1-y_proba, y_proba]))\n",
    "            brier = brier_score_loss(y_true, y_proba)\n",
    "        except:\n",
    "            roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    else:\n",
    "        roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    results = {\n",
    "        'cm': cm, 'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
    "        'acc': acc, 'bal_acc': bal_acc, 'prec': prec, 'rec': rec,\n",
    "        'specificity': specificity, 'npv': npv, 'fpr': fpr, 'fnr': fnr,\n",
    "        'f1': f1, 'mcc': mcc, 'jacc': jacc, 'kappa': kappa,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc, 'log_loss': logloss, 'brier': brier\n",
    "    }\n",
    "    if embeddings is not None:\n",
    "        recall_metrics = calculate_recall_at_k(embeddings, torch.tensor(y_true))\n",
    "        mrr = calculate_mrr(embeddings, torch.tensor(y_true))\n",
    "        results.update(recall_metrics)\n",
    "        results['mrr'] = mrr\n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=3, learning_rate=1e-3, batch_size=2):\n",
    "    gatera_params = [p for p in model.gatera_layers.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(\n",
    "        [{'params': gatera_params}, {'params': model.classifier.parameters()}], \n",
    "        lr=learning_rate, \n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    dataset = CodeCloneDataset(train_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model.train()\n",
    "    epoch_stats = []\n",
    "    start_time = time.time()\n",
    "    initial_memory = get_memory_usage()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0\n",
    "        total_task_loss = 0\n",
    "        total_entropy_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_count = 0\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(gatera_params + list(model.classifier.parameters()), 1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                if outputs.get('entropy_loss') is not None:\n",
    "                    total_entropy_loss += outputs['entropy_loss'].item()\n",
    "                with torch.no_grad():\n",
    "                    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                    correct += (predictions == labels.to(predictions.device)).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                \n",
    "                del outputs, loss, predictions\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            batch_count += 1\n",
    "            if batch_count % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        avg_entropy = total_entropy_loss / batch_count if batch_count > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        current_memory = get_memory_usage()\n",
    "        epoch_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'entropy_loss': avg_entropy,\n",
    "            'acc': accuracy,\n",
    "            'time': epoch_time,\n",
    "            'memory_mb': current_memory\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Entropy: {avg_entropy:.4f}, Acc: {accuracy:.4f}, Time: {epoch_time:.2f}s, Memory: {current_memory:.2f}MB\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "    total_training_time = time.time() - start_time\n",
    "    peak_memory = max([stat['memory_mb'] for stat in epoch_stats])\n",
    "    memory_increase = peak_memory - initial_memory\n",
    "    print(f\"\\n{'='*80}\\nTRAINING SUMMARY\\n{'='*80}\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Peak Memory Usage: {peak_memory:.2f}MB\")\n",
    "    print(f\"Memory Increase: {memory_increase:.2f}MB\\n{'='*80}\\n\")\n",
    "    return epoch_stats\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, batch_size=2):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_embeddings = []\n",
    "    dataset = CodeCloneDataset(test_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            pooled_embeddings = hidden_states.mean(dim=1)\n",
    "            pooled_embeddings = F.normalize(pooled_embeddings, p=2, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "            all_embeddings.append(pooled_embeddings.cpu())\n",
    "            if len(all_predictions) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    inference_time = time.time() - start_time\n",
    "    samples_per_sec = len(test_data) / inference_time\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    metrics = calculate_comprehensive_metrics(all_labels, all_predictions, all_probabilities, all_embeddings)\n",
    "    metrics['inference_time'] = inference_time\n",
    "    metrics['samples_per_sec'] = samples_per_sec\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_results(metrics, dataset_name):\n",
    "    print(f\"\\n{'='*80}\\nRESULTS: {dataset_name}\\n{'='*80}\\n\\nConfusion Matrix:\\n{metrics['cm']}\\n\\nTrue Negatives (TN): {metrics['tn']}\\nFalse Positives (FP): {metrics['fp']}\\nFalse Negatives (FN): {metrics['fn']}\\nTrue Positives (TP): {metrics['tp']}\\n\\n{'='*80}\\nCLASSIFICATION METRICS\\n{'='*80}\")\n",
    "    print(f\"Accuracy: {metrics['acc']:.4f}\\nBalanced Accuracy: {metrics['bal_acc']:.4f}\\nPrecision: {metrics['prec']:.4f}\\nRecall (Sensitivity): {metrics['rec']:.4f}\\nF1 Score: {metrics['f1']:.4f}\\nSpecificity: {metrics['specificity']:.4f}\\nNegative Predictive Value (NPV): {metrics['npv']:.4f}\\nFalse Positive Rate (FPR): {metrics['fpr']:.4f}\\nFalse Negative Rate (FNR): {metrics['fnr']:.4f}\")\n",
    "    print(f\"\\n{'='*80}\\nADVANCED METRICS\\n{'='*80}\\nJaccard Score: {metrics['jacc']:.4f}\\nMatthews Correlation Coefficient (MCC): {metrics['mcc']:.4f}\\nCohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPROBABILISTIC METRICS\\n{'='*80}\\nROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "    if 'pr_auc' in metrics:\n",
    "        print(f\"Precision-Recall AUC: {metrics['pr_auc']:.4f}\")\n",
    "    if 'log_loss' in metrics:\n",
    "        print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n",
    "    if 'brier' in metrics:\n",
    "        print(f\"Brier Score: {metrics['brier']:.4f}\")\n",
    "    if 'recall@1' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nRETRIEVAL METRICS\\n{'='*80}\\nRecall@1: {metrics['recall@1']:.4f}\\nRecall@3: {metrics['recall@3']:.4f}\\nRecall@5: {metrics['recall@5']:.4f}\\nRecall@10: {metrics['recall@10']:.4f}\\nMean Reciprocal Rank (MRR): {metrics['mrr']:.4f}\")\n",
    "    if 'inference_time' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPERFORMANCE METRICS\\n{'='*80}\\nInference Time: {metrics['inference_time']:.2f}s\\nThroughput: {metrics['samples_per_sec']:.2f} samples/sec\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    cache_dir = '/hf_cache'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\", cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    gatera_config = GateRAConfig(rank=16, alpha=16.0, dropout=0.0, entropy_reg_weight=0.01)\n",
    "    print(\"\\nLoading model with GateRA (Gated LoRA) and pipeline parallelism across GPUs...\")\n",
    "    model = GateRACodeT5(\"Salesforce/codet5p-770m\", gatera_config, num_classes=2, cache_dir=cache_dir)\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\\nTRAINING PHASE\\n{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = create_training_data(train_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(train_data)} training examples\\n\")\n",
    "    if len(train_data) == 0:\n",
    "        print(\"No training data available. Exiting.\")\n",
    "        return\n",
    "    epoch_stats = train_model(model, train_data, num_epochs=5, learning_rate=1e-3, batch_size=4)\n",
    "    print(f\"\\n{'='*80}\\nTESTING PHASE\\n{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = create_training_data(test_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(test_data)} test examples\\n\")\n",
    "    if len(test_data) > 0:\n",
    "        test_metrics = evaluate_model(model, test_data, batch_size=4)\n",
    "        print_results(test_metrics, \"TEST SET\")\n",
    "    print(f\"\\n{'='*80}\\nTRAINING COMPLETE\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88483c4-101d-43a2-a7ab-a2b7bb65df05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prefix tuning codet5+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cabdc-ce03-48ae-b046-fc26d5ed04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, balanced_accuracy_score, \n",
    "    matthews_corrcoef, roc_auc_score, average_precision_score, \n",
    "    f1_score, jaccard_score, cohen_kappa_score, log_loss, brier_score_loss\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class PrefixTuningConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        prefix_length: int = 20,\n",
    "        num_layers: int = 24,\n",
    "        hidden_size: int = 1024,\n",
    "        num_heads: int = 16,\n",
    "        head_dim: int = 64,\n",
    "        reparam_dim: int = 512,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        self.prefix_length = prefix_length\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.reparam_dim = reparam_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "class PrefixEncoder(nn.Module):\n",
    "    def __init__(self, config: PrefixTuningConfig):\n",
    "        super().__init__()\n",
    "        self.prefix_length = config.prefix_length\n",
    "        self.num_layers = config.num_layers\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        self.prefix_tokens = nn.Parameter(\n",
    "            torch.randn(config.num_layers, config.prefix_length, config.reparam_dim)\n",
    "        )\n",
    "        \n",
    "        self.reparam_mlp = nn.Sequential(\n",
    "            nn.Linear(config.reparam_dim, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 2 * config.num_heads * config.head_dim),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_size: int):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "        \n",
    "        prefix_kvs = []\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            layer_prefix = prefix_tokens[:, layer_idx, :, :]\n",
    "            prefix_hidden = self.reparam_mlp(layer_prefix)\n",
    "            \n",
    "            prefix_hidden = prefix_hidden.view(\n",
    "                batch_size, self.prefix_length, 2, self.num_heads, self.head_dim\n",
    "            )\n",
    "            \n",
    "            key = prefix_hidden[:, :, 0, :, :].transpose(1, 2)\n",
    "            value = prefix_hidden[:, :, 1, :, :].transpose(1, 2)\n",
    "            \n",
    "            prefix_kvs.append((key.contiguous(), value.contiguous()))\n",
    "        \n",
    "        return prefix_kvs\n",
    "\n",
    "\n",
    "class CodeCloneDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class PrefixTuningCodeT5(nn.Module):\n",
    "    def __init__(self, model_name: str, prefix_config: PrefixTuningConfig, num_classes: int = 2, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Initializing model across {num_gpus} GPUs\")\n",
    "        \n",
    "        self.primary_device = 'cuda:0'\n",
    "        \n",
    "        print(\"Loading base LLM model...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,\n",
    "            cache_dir=cache_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map='auto'\n",
    "        )\n",
    "        \n",
    "        self.llm = base_model\n",
    "        self.prefix_config = prefix_config\n",
    "        \n",
    "        encoder_layers = len(base_model.encoder.block)\n",
    "        decoder_layers = len(base_model.decoder.block)\n",
    "        total_layers = encoder_layers + decoder_layers\n",
    "        print(f\"Model has {encoder_layers} encoder layers and {decoder_layers} decoder layers (total: {total_layers})\")\n",
    "        \n",
    "        self.layer_devices = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer_device = next(base_model.encoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Encoder Layer {i} -> {layer_device}\")\n",
    "        \n",
    "        for i in range(decoder_layers):\n",
    "            layer_device = next(base_model.decoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Decoder Layer {i} -> {layer_device}\")\n",
    "        \n",
    "        embed_device = next(base_model.encoder.embed_tokens.parameters()).device\n",
    "        final_device = next(base_model.decoder.final_layer_norm.parameters()).device\n",
    "        \n",
    "        print(f\"Embeddings -> {embed_device}\")\n",
    "        print(f\"Final Layer -> {final_device}\")\n",
    "        \n",
    "        self.embed_device = str(embed_device)\n",
    "        self.final_device = str(final_device)\n",
    "        \n",
    "        self.prefix_encoder = PrefixEncoder(prefix_config).to(self.primary_device).float()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(prefix_config.hidden_size, prefix_config.hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(prefix_config.hidden_size // 4, num_classes)\n",
    "        ).to(self.primary_device).float()\n",
    "        \n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.prefix_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self._register_prefix_hooks()\n",
    "        \n",
    "        print(\"Model initialization complete!\")\n",
    "    \n",
    "    def _register_prefix_hooks(self):\n",
    "        self.prefix_kvs = None\n",
    "        \n",
    "        def create_hook(layer_idx):\n",
    "            def hook(module, args, kwargs, output):\n",
    "                if self.prefix_kvs is not None and layer_idx < len(self.prefix_kvs):\n",
    "                    prefix_key, prefix_value = self.prefix_kvs[layer_idx]\n",
    "                    \n",
    "                    device = output[1][0].device if isinstance(output, tuple) and len(output) > 1 and output[1] is not None else self.layer_devices[layer_idx]\n",
    "                    \n",
    "                    prefix_key = prefix_key.to(device)\n",
    "                    prefix_value = prefix_value.to(device)\n",
    "                    \n",
    "                    if isinstance(output, tuple) and len(output) > 1 and output[1] is not None:\n",
    "                        orig_key_value = output[1]\n",
    "                        if isinstance(orig_key_value, tuple) and len(orig_key_value) >= 2:\n",
    "                            orig_key = orig_key_value[0]\n",
    "                            orig_value = orig_key_value[1]\n",
    "                            \n",
    "                            new_key = torch.cat([prefix_key, orig_key], dim=2)\n",
    "                            new_value = torch.cat([prefix_value, orig_value], dim=2)\n",
    "                            \n",
    "                            new_key_value = (new_key, new_value) + orig_key_value[2:] if len(orig_key_value) > 2 else (new_key, new_value)\n",
    "                            output = (output[0], new_key_value) + output[2:] if len(output) > 2 else (output[0], new_key_value)\n",
    "                \n",
    "                return output\n",
    "            return hook\n",
    "        \n",
    "        encoder_layers = len(self.llm.encoder.block)\n",
    "        for layer_idx, layer in enumerate(self.llm.encoder.block):\n",
    "            layer.register_forward_hook(create_hook(layer_idx), with_kwargs=True)\n",
    "        \n",
    "        for layer_idx, layer in enumerate(self.llm.decoder.block):\n",
    "            layer.register_forward_hook(create_hook(encoder_layers + layer_idx), with_kwargs=True)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        input_ids = input_ids.to(self.embed_device)\n",
    "        \n",
    "        self.prefix_kvs = self.prefix_encoder(batch_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.embed_device)\n",
    "            prefix_attention = torch.ones(\n",
    "                batch_size, self.prefix_config.prefix_length, \n",
    "                dtype=attention_mask.dtype, device=attention_mask.device\n",
    "            )\n",
    "            attention_mask = torch.cat([prefix_attention, attention_mask], dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=input_ids,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "        \n",
    "        hidden_states = outputs.decoder_hidden_states[-1]\n",
    "        \n",
    "        if str(hidden_states.device) != self.primary_device:\n",
    "            hidden_states = hidden_states.to(self.primary_device)\n",
    "        \n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.device != logits.device:\n",
    "                labels = labels.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        self.prefix_kvs = None\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss, 'hidden_states': hidden_states}\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model: PrefixTuningCodeT5):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    prefix_params = sum(p.numel() for p in model.prefix_encoder.parameters())\n",
    "    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "    trainable_params = prefix_params + classifier_params\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PARAMETER STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Frozen LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Prefix Parameters: {prefix_params:,}\")\n",
    "    print(f\"Trainable Classifier Parameters: {classifier_params:,}\")\n",
    "    print(f\"Total Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"GPU MEMORY USAGE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"GPU {i}: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(csv_path, label_col, func1_col, func2_col):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded dataset: {len(df)} samples\")\n",
    "        print(f\"Label distribution: {df[label_col].value_counts().to_dict()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_training_data(csv_path, tokenizer, max_length=512, \n",
    "                        label_col='label', func1_col='func1', func2_col='func2'):\n",
    "    df = load_dataset(csv_path, label_col, func1_col, func2_col)\n",
    "    \n",
    "    if df is None:\n",
    "        return []\n",
    "    \n",
    "    train_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        func1 = str(row[func1_col])\n",
    "        func2 = str(row[func2_col])\n",
    "        label = int(row[label_col])\n",
    "        \n",
    "        combined_text = f\"Code1: {func1}\\nCode2: {func2}\\nAre these code clones?\"\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            combined_text,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        train_data.append({\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        })\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(embeddings, labels, k_values=[1, 3, 5, 10]):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    \n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            true_label = labels[i]\n",
    "            sims = similarities[i].clone()\n",
    "            sims[i] = -float('inf')\n",
    "            \n",
    "            top_k_indices = torch.topk(sims, min(k, len(labels)-1)).indices\n",
    "            top_k_labels = labels[top_k_indices]\n",
    "            \n",
    "            if true_label in top_k_labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        results[f'recall@{k}'] = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_mrr(embeddings, labels):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    \n",
    "    mrr_sum = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        true_label = labels[i]\n",
    "        sims = similarities[i].clone()\n",
    "        sims[i] = -float('inf')\n",
    "        \n",
    "        sorted_indices = torch.argsort(sims, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        for rank, label in enumerate(sorted_labels, 1):\n",
    "            if label == true_label:\n",
    "                mrr_sum += 1.0 / rank\n",
    "                break\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    return mrr_sum / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba=None, embeddings=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    \n",
    "    if cm.shape != (2, 2):\n",
    "        cm_2x2 = np.zeros((2, 2))\n",
    "        for i in range(min(cm.shape[0], 2)):\n",
    "            for j in range(min(cm.shape[1], 2)):\n",
    "                cm_2x2[i, j] = cm[i, j]\n",
    "        cm = cm_2x2\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    jacc = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_proba)\n",
    "            pr_auc = average_precision_score(y_true, y_proba)\n",
    "            logloss = log_loss(y_true, np.column_stack([1-y_proba, y_proba]))\n",
    "            brier = brier_score_loss(y_true, y_proba)\n",
    "        except:\n",
    "            roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    else:\n",
    "        roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    \n",
    "    results = {\n",
    "        'cm': cm,\n",
    "        'tn': int(tn),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn),\n",
    "        'tp': int(tp),\n",
    "        'acc': acc,\n",
    "        'bal_acc': bal_acc,\n",
    "        'prec': prec,\n",
    "        'rec': rec,\n",
    "        'specificity': specificity,\n",
    "        'npv': npv,\n",
    "        'fpr': fpr,\n",
    "        'fnr': fnr,\n",
    "        'f1': f1,\n",
    "        'mcc': mcc,\n",
    "        'jacc': jacc,\n",
    "        'kappa': kappa,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'log_loss': logloss,\n",
    "        'brier': brier\n",
    "    }\n",
    "    \n",
    "    if embeddings is not None:\n",
    "        recall_metrics = calculate_recall_at_k(embeddings, torch.tensor(y_true))\n",
    "        mrr = calculate_mrr(embeddings, torch.tensor(y_true))\n",
    "        results.update(recall_metrics)\n",
    "        results['mrr'] = mrr\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=1e-4, batch_size=2):\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.prefix_encoder.parameters()},\n",
    "        {'params': model.classifier.parameters()}\n",
    "    ], lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    dataset = CodeCloneDataset(train_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_stats = []\n",
    "    start_time = time.time()\n",
    "    initial_memory = get_memory_usage()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(model.prefix_encoder.parameters()) + list(model.classifier.parameters()), 1.0\n",
    "                )\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                    correct += (predictions == labels.to(predictions.device)).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            \n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        current_memory = get_memory_usage()\n",
    "        \n",
    "        epoch_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'acc': accuracy,\n",
    "            'time': epoch_time,\n",
    "            'memory_mb': current_memory\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}, Time: {epoch_time:.2f}s, Memory: {current_memory:.2f}MB\")\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "    \n",
    "    total_training_time = time.time() - start_time\n",
    "    peak_memory = max([stat['memory_mb'] for stat in epoch_stats])\n",
    "    memory_increase = peak_memory - initial_memory\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Peak Memory Usage: {peak_memory:.2f}MB\")\n",
    "    print(f\"Memory Increase: {memory_increase:.2f}MB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return epoch_stats\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, batch_size=2):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_embeddings = []\n",
    "    \n",
    "    dataset = CodeCloneDataset(test_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            pooled_embeddings = hidden_states.mean(dim=1)\n",
    "            pooled_embeddings = F.normalize(pooled_embeddings, p=2, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "            all_embeddings.append(pooled_embeddings.cpu())\n",
    "            \n",
    "            if len(all_predictions) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    samples_per_sec = len(test_data) / inference_time\n",
    "    \n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    metrics = calculate_comprehensive_metrics(\n",
    "        all_labels, \n",
    "        all_predictions, \n",
    "        all_probabilities,\n",
    "        all_embeddings\n",
    "    )\n",
    "    \n",
    "    metrics['inference_time'] = inference_time\n",
    "    metrics['samples_per_sec'] = samples_per_sec\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_results(metrics, dataset_name):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"{metrics['cm']}\")\n",
    "    print(f\"\\nTrue Negatives (TN): {metrics['tn']}\")\n",
    "    print(f\"False Positives (FP): {metrics['fp']}\")\n",
    "    print(f\"False Negatives (FN): {metrics['fn']}\")\n",
    "    print(f\"True Positives (TP): {metrics['tp']}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLASSIFICATION METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Accuracy: {metrics['acc']:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {metrics['bal_acc']:.4f}\")\n",
    "    print(f\"Precision: {metrics['prec']:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {metrics['rec']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "    print(f\"Negative Predictive Value (NPV): {metrics['npv']:.4f}\")\n",
    "    print(f\"False Positive Rate (FPR): {metrics['fpr']:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {metrics['fnr']:.4f}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ADVANCED METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Jaccard Score: {metrics['jacc']:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {metrics['mcc']:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "    \n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PROBABILISTIC METRICS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "    if 'pr_auc' in metrics:\n",
    "        print(f\"Precision-Recall AUC: {metrics['pr_auc']:.4f}\")\n",
    "    if 'log_loss' in metrics:\n",
    "        print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n",
    "    if 'brier' in metrics:\n",
    "        print(f\"Brier Score: {metrics['brier']:.4f}\")\n",
    "    \n",
    "    if 'recall@1' in metrics:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"RETRIEVAL METRICS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Recall@1: {metrics['recall@1']:.4f}\")\n",
    "        print(f\"Recall@3: {metrics['recall@3']:.4f}\")\n",
    "        print(f\"Recall@5: {metrics['recall@5']:.4f}\")\n",
    "        print(f\"Recall@10: {metrics['recall@10']:.4f}\")\n",
    "        print(f\"Mean Reciprocal Rank (MRR): {metrics['mrr']:.4f}\")\n",
    "    \n",
    "    if 'inference_time' in metrics:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PERFORMANCE METRICS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Inference Time: {metrics['inference_time']:.2f}s\")\n",
    "        print(f\"Throughput: {metrics['samples_per_sec']:.2f} samples/sec\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    cache_dir = '/hf_cache'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\", cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    prefix_config = PrefixTuningConfig(\n",
    "        prefix_length=20,\n",
    "        num_layers=24,\n",
    "        hidden_size=1024,\n",
    "        num_heads=16,\n",
    "        head_dim=64,\n",
    "        reparam_dim=512\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLoading model with pipeline parallelism across GPUs...\")\n",
    "    model = PrefixTuningCodeT5(\"Salesforce/codet5p-770m\", prefix_config, num_classes=2, cache_dir=cache_dir)\n",
    "    print_parameter_statistics(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING PHASE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    train_csv = '/train.csv'\n",
    "    train_data = create_training_data(\n",
    "        train_csv, \n",
    "        tokenizer,\n",
    "        label_col='label', \n",
    "        func1_col='func1', \n",
    "        func2_col='func2'\n",
    "    )\n",
    "    print(f\"Created {len(train_data)} training examples\\n\")\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        print(\"No training data available. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    epoch_stats = train_model(model, train_data, num_epochs=5, learning_rate=1e-4, batch_size=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TESTING PHASE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_csv = '/test.csv'\n",
    "    test_data = create_training_data(\n",
    "        test_csv,\n",
    "        tokenizer,\n",
    "        label_col='label',\n",
    "        func1_col='func1',\n",
    "        func2_col='func2'\n",
    "    )\n",
    "    print(f\"Created {len(test_data)} test examples\\n\")\n",
    "    \n",
    "    if len(test_data) > 0:\n",
    "        test_metrics = evaluate_model(model, test_data, batch_size=2)\n",
    "        print_results(test_metrics, \"TEST SET\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7e3a2-7d05-4d05-8272-c2937a3cbf93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# adpater clone det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a00e7-d82e-4d44-9cfb-a2b773810d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, balanced_accuracy_score, \n",
    "    matthews_corrcoef, roc_auc_score, average_precision_score, \n",
    "    f1_score, jaccard_score, cohen_kappa_score, log_loss, brier_score_loss\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class AdapterConfig:\n",
    "    def __init__(self, hidden_size: int = 1024, reduction_factor: int = 16, non_linearity: str = \"gelu\", dropout: float = 0.1):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.adapter_size = hidden_size // reduction_factor\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.non_linearity = non_linearity\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "class AdapterModule(nn.Module):\n",
    "    def __init__(self, config: AdapterConfig):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(config.hidden_size, config.adapter_size)\n",
    "        self.up_project = nn.Linear(config.adapter_size, config.hidden_size)\n",
    "        if config.non_linearity == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif config.non_linearity == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif config.non_linearity == \"swish\":\n",
    "            self.activation = nn.SiLU()\n",
    "        else:\n",
    "            self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        nn.init.normal_(self.down_project.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.down_project.bias)\n",
    "        nn.init.normal_(self.up_project.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.up_project.bias)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.down_project(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.up_project(hidden_states)\n",
    "        return hidden_states + residual\n",
    "\n",
    "\n",
    "class CodeCloneDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class AdapterCodeT5(nn.Module):\n",
    "    def __init__(self, model_name: str, adapter_config: AdapterConfig, num_classes: int = 2, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Initializing model across {num_gpus} GPUs\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(\"Loading base LLM model...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float32, cache_dir=cache_dir, low_cpu_mem_usage=True, device_map='auto')\n",
    "        self.llm = base_model\n",
    "        self.adapter_config = adapter_config\n",
    "        encoder_layers = len(base_model.encoder.block)\n",
    "        decoder_layers = len(base_model.decoder.block)\n",
    "        total_layers = encoder_layers + decoder_layers\n",
    "        print(f\"Model has {encoder_layers} encoder layers and {decoder_layers} decoder layers (total: {total_layers})\")\n",
    "        self.layer_devices = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer_device = next(base_model.encoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Encoder Layer {i} -> {layer_device}\")\n",
    "        for i in range(decoder_layers):\n",
    "            layer_device = next(base_model.decoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Decoder Layer {i} -> {layer_device}\")\n",
    "        embed_device = next(base_model.encoder.embed_tokens.parameters()).device\n",
    "        final_device = next(base_model.decoder.final_layer_norm.parameters()).device\n",
    "        print(f\"Embeddings -> {embed_device}\")\n",
    "        print(f\"Final Layer -> {final_device}\")\n",
    "        self.embed_device = str(embed_device)\n",
    "        self.final_device = str(final_device)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.encoder_adapters_attn = nn.ModuleList()\n",
    "        self.encoder_adapters_ffn = nn.ModuleList()\n",
    "        self.decoder_adapters_attn = nn.ModuleList()\n",
    "        self.decoder_adapters_ffn = nn.ModuleList()\n",
    "        self._inject_adapter_modules()\n",
    "        self.num_classes = num_classes\n",
    "        hidden_size = base_model.config.d_model\n",
    "        self.classifier = nn.Sequential(nn.Linear(hidden_size, hidden_size // 4), nn.ReLU(), nn.Dropout(0.1), nn.Linear(hidden_size // 4, num_classes)).to(self.primary_device).float()\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Model initialization complete!\")\n",
    "    \n",
    "    def _inject_adapter_modules(self):\n",
    "        encoder_layers = len(self.llm.encoder.block)\n",
    "        decoder_layers = len(self.llm.decoder.block)\n",
    "        for layer_idx in range(encoder_layers):\n",
    "            layer = self.llm.encoder.block[layer_idx]\n",
    "            device = next(layer.parameters()).device\n",
    "            adapter_after_attn = AdapterModule(self.adapter_config).to(device)\n",
    "            self.encoder_adapters_attn.append(adapter_after_attn)\n",
    "            adapter_after_ffn = AdapterModule(self.adapter_config).to(device)\n",
    "            self.encoder_adapters_ffn.append(adapter_after_ffn)\n",
    "        for layer_idx in range(decoder_layers):\n",
    "            layer = self.llm.decoder.block[layer_idx]\n",
    "            device = next(layer.parameters()).device\n",
    "            adapter_after_attn = AdapterModule(self.adapter_config).to(device)\n",
    "            self.decoder_adapters_attn.append(adapter_after_attn)\n",
    "            adapter_after_ffn = AdapterModule(self.adapter_config).to(device)\n",
    "            self.decoder_adapters_ffn.append(adapter_after_ffn)\n",
    "        self._register_forward_hooks()\n",
    "    \n",
    "    def _register_forward_hooks(self):\n",
    "        encoder_layers = len(self.llm.encoder.block)\n",
    "        decoder_layers = len(self.llm.decoder.block)\n",
    "        for layer_idx in range(encoder_layers):\n",
    "            layer = self.llm.encoder.block[layer_idx]\n",
    "            attn_adapter = self.encoder_adapters_attn[layer_idx]\n",
    "            ffn_adapter = self.encoder_adapters_ffn[layer_idx]\n",
    "            original_forward = layer.forward\n",
    "            def make_forward(orig_fwd, attn_adp, ffn_adp):\n",
    "                def new_forward(*args, **kwargs):\n",
    "                    outputs = orig_fwd(*args, **kwargs)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        hidden_states = outputs[0]\n",
    "                        hidden_states = attn_adp(hidden_states)\n",
    "                        hidden_states = ffn_adp(hidden_states)\n",
    "                        outputs = (hidden_states,) + outputs[1:]\n",
    "                    return outputs\n",
    "                return new_forward\n",
    "            layer.forward = make_forward(original_forward, attn_adapter, ffn_adapter)\n",
    "        for layer_idx in range(decoder_layers):\n",
    "            layer = self.llm.decoder.block[layer_idx]\n",
    "            attn_adapter = self.decoder_adapters_attn[layer_idx]\n",
    "            ffn_adapter = self.decoder_adapters_ffn[layer_idx]\n",
    "            original_forward = layer.forward\n",
    "            def make_forward(orig_fwd, attn_adp, ffn_adp):\n",
    "                def new_forward(*args, **kwargs):\n",
    "                    outputs = orig_fwd(*args, **kwargs)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        hidden_states = outputs[0]\n",
    "                        hidden_states = attn_adp(hidden_states)\n",
    "                        hidden_states = ffn_adp(hidden_states)\n",
    "                        outputs = (hidden_states,) + outputs[1:]\n",
    "                    return outputs\n",
    "                return new_forward\n",
    "            layer.forward = make_forward(original_forward, attn_adapter, ffn_adapter)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        input_ids = input_ids.to(self.embed_device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.embed_device)\n",
    "        outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=input_ids, output_hidden_states=True, return_dict=True)\n",
    "        hidden_states = outputs.decoder_hidden_states[-1]\n",
    "        if str(hidden_states.device) != self.primary_device:\n",
    "            hidden_states = hidden_states.to(self.primary_device)\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.device != logits.device:\n",
    "                labels = labels.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {'logits': logits, 'loss': loss, 'hidden_states': hidden_states}\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model: AdapterCodeT5):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    adapter_params = sum(p.numel() for p in model.encoder_adapters_attn.parameters()) + sum(p.numel() for p in model.encoder_adapters_ffn.parameters()) + sum(p.numel() for p in model.decoder_adapters_attn.parameters()) + sum(p.numel() for p in model.decoder_adapters_ffn.parameters())\n",
    "    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "    trainable_params = adapter_params + classifier_params\n",
    "    print(f\"\\n{'='*80}\\nPARAMETER STATISTICS\\n{'='*80}\")\n",
    "    print(f\"Total Frozen LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Adapter Parameters: {adapter_params:,}\")\n",
    "    print(f\"Trainable Classifier Parameters: {classifier_params:,}\")\n",
    "    print(f\"Total Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\\n{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\\nGPU MEMORY USAGE\\n{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"GPU {i}: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(csv_path, label_col, func1_col, func2_col):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded dataset: {len(df)} samples\")\n",
    "        print(f\"Label distribution: {df[label_col].value_counts().to_dict()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_training_data(csv_path, tokenizer, max_length=512, label_col='label', func1_col='func1', func2_col='func2'):\n",
    "    df = load_dataset(csv_path, label_col, func1_col, func2_col)\n",
    "    if df is None:\n",
    "        return []\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        func1 = str(row[func1_col])\n",
    "        func2 = str(row[func2_col])\n",
    "        label = int(row[label_col])\n",
    "        combined_text = f\"Code1: {func1}\\nCode2: {func2}\\nAre these code clones?\"\n",
    "        encoding = tokenizer(combined_text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "        train_data.append({'input_ids': encoding['input_ids'].squeeze(0), 'attention_mask': encoding['attention_mask'].squeeze(0), 'labels': torch.tensor(label, dtype=torch.long)})\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(embeddings, labels, k_values=[1, 3, 5, 10]):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(labels)):\n",
    "            true_label = labels[i]\n",
    "            sims = similarities[i].clone()\n",
    "            sims[i] = -float('inf')\n",
    "            top_k_indices = torch.topk(sims, min(k, len(labels)-1)).indices\n",
    "            top_k_labels = labels[top_k_indices]\n",
    "            if true_label in top_k_labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        results[f'recall@{k}'] = correct / total if total > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_mrr(embeddings, labels):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    mrr_sum = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        true_label = labels[i]\n",
    "        sims = similarities[i].clone()\n",
    "        sims[i] = -float('inf')\n",
    "        sorted_indices = torch.argsort(sims, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        for rank, label in enumerate(sorted_labels, 1):\n",
    "            if label == true_label:\n",
    "                mrr_sum += 1.0 / rank\n",
    "                break\n",
    "        count += 1\n",
    "    return mrr_sum / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba=None, embeddings=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape != (2, 2):\n",
    "        cm_2x2 = np.zeros((2, 2))\n",
    "        for i in range(min(cm.shape[0], 2)):\n",
    "            for j in range(min(cm.shape[1], 2)):\n",
    "                cm_2x2[i, j] = cm[i, j]\n",
    "        cm = cm_2x2\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    jacc = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_proba)\n",
    "            pr_auc = average_precision_score(y_true, y_proba)\n",
    "            logloss = log_loss(y_true, np.column_stack([1-y_proba, y_proba]))\n",
    "            brier = brier_score_loss(y_true, y_proba)\n",
    "        except:\n",
    "            roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    else:\n",
    "        roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    results = {'cm': cm, 'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp), 'acc': acc, 'bal_acc': bal_acc, 'prec': prec, 'rec': rec, 'specificity': specificity, 'npv': npv, 'fpr': fpr, 'fnr': fnr, 'f1': f1, 'mcc': mcc, 'jacc': jacc, 'kappa': kappa, 'roc_auc': roc_auc, 'pr_auc': pr_auc, 'log_loss': logloss, 'brier': brier}\n",
    "    if embeddings is not None:\n",
    "        recall_metrics = calculate_recall_at_k(embeddings, torch.tensor(y_true))\n",
    "        mrr = calculate_mrr(embeddings, torch.tensor(y_true))\n",
    "        results.update(recall_metrics)\n",
    "        results['mrr'] = mrr\n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=1e-4, batch_size=2):\n",
    "    adapter_params = list(model.encoder_adapters_attn.parameters()) + list(model.encoder_adapters_ffn.parameters()) + list(model.decoder_adapters_attn.parameters()) + list(model.decoder_adapters_ffn.parameters())\n",
    "    optimizer = torch.optim.AdamW([{'params': adapter_params}, {'params': model.classifier.parameters()}], lr=learning_rate, weight_decay=0.01)\n",
    "    dataset = CodeCloneDataset(train_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model.train()\n",
    "    epoch_stats = []\n",
    "    start_time = time.time()\n",
    "    initial_memory = get_memory_usage()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_count = 0\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(adapter_params + list(model.classifier.parameters()), 1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                with torch.no_grad():\n",
    "                    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                    correct += (predictions == labels.to(predictions.device)).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        current_memory = get_memory_usage()\n",
    "        epoch_stats.append({'epoch': epoch + 1, 'loss': avg_loss, 'acc': accuracy, 'time': epoch_time, 'memory_mb': current_memory})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}, Time: {epoch_time:.2f}s, Memory: {current_memory:.2f}MB\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "    total_training_time = time.time() - start_time\n",
    "    peak_memory = max([stat['memory_mb'] for stat in epoch_stats])\n",
    "    memory_increase = peak_memory - initial_memory\n",
    "    print(f\"\\n{'='*80}\\nTRAINING SUMMARY\\n{'='*80}\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Peak Memory Usage: {peak_memory:.2f}MB\")\n",
    "    print(f\"Memory Increase: {memory_increase:.2f}MB\\n{'='*80}\\n\")\n",
    "    return epoch_stats\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, batch_size=2):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_embeddings = []\n",
    "    dataset = CodeCloneDataset(test_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            pooled_embeddings = hidden_states.mean(dim=1)\n",
    "            pooled_embeddings = F.normalize(pooled_embeddings, p=2, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "            all_embeddings.append(pooled_embeddings.cpu())\n",
    "            if len(all_predictions) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    inference_time = time.time() - start_time\n",
    "    samples_per_sec = len(test_data) / inference_time\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    metrics = calculate_comprehensive_metrics(all_labels, all_predictions, all_probabilities, all_embeddings)\n",
    "    metrics['inference_time'] = inference_time\n",
    "    metrics['samples_per_sec'] = samples_per_sec\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_results(metrics, dataset_name):\n",
    "    print(f\"\\n{'='*80}\\nRESULTS: {dataset_name}\\n{'='*80}\\n\\nConfusion Matrix:\\n{metrics['cm']}\\n\\nTrue Negatives (TN): {metrics['tn']}\\nFalse Positives (FP): {metrics['fp']}\\nFalse Negatives (FN): {metrics['fn']}\\nTrue Positives (TP): {metrics['tp']}\\n\\n{'='*80}\\nCLASSIFICATION METRICS\\n{'='*80}\")\n",
    "    print(f\"Accuracy: {metrics['acc']:.4f}\\nBalanced Accuracy: {metrics['bal_acc']:.4f}\\nPrecision: {metrics['prec']:.4f}\\nRecall (Sensitivity): {metrics['rec']:.4f}\\nF1 Score: {metrics['f1']:.4f}\\nSpecificity: {metrics['specificity']:.4f}\\nNegative Predictive Value (NPV): {metrics['npv']:.4f}\\nFalse Positive Rate (FPR): {metrics['fpr']:.4f}\\nFalse Negative Rate (FNR): {metrics['fnr']:.4f}\")\n",
    "    print(f\"\\n{'='*80}\\nADVANCED METRICS\\n{'='*80}\\nJaccard Score: {metrics['jacc']:.4f}\\nMatthews Correlation Coefficient (MCC): {metrics['mcc']:.4f}\\nCohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPROBABILISTIC METRICS\\n{'='*80}\\nROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "    if 'pr_auc' in metrics:\n",
    "        print(f\"Precision-Recall AUC: {metrics['pr_auc']:.4f}\")\n",
    "    if 'log_loss' in metrics:\n",
    "        print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n",
    "    if 'brier' in metrics:\n",
    "        print(f\"Brier Score: {metrics['brier']:.4f}\")\n",
    "    if 'recall@1' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nRETRIEVAL METRICS\\n{'='*80}\\nRecall@1: {metrics['recall@1']:.4f}\\nRecall@3: {metrics['recall@3']:.4f}\\nRecall@5: {metrics['recall@5']:.4f}\\nRecall@10: {metrics['recall@10']:.4f}\\nMean Reciprocal Rank (MRR): {metrics['mrr']:.4f}\")\n",
    "    if 'inference_time' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPERFORMANCE METRICS\\n{'='*80}\\nInference Time: {metrics['inference_time']:.2f}s\\nThroughput: {metrics['samples_per_sec']:.2f} samples/sec\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    cache_dir = '/hf_cache'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\", cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    adapter_config = AdapterConfig(hidden_size=1024, reduction_factor=16, non_linearity=\"gelu\", dropout=0.1)\n",
    "    print(\"\\nLoading model with Adapter Tuning and pipeline parallelism across GPUs...\")\n",
    "    model = AdapterCodeT5(\"Salesforce/codet5p-770m\", adapter_config, num_classes=2, cache_dir=cache_dir)\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\\nTRAINING PHASE\\n{'='*80}\\n\")\n",
    "    train_csv = 'train.csv'\n",
    "    train_data = create_training_data(train_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(train_data)} training examples\\n\")\n",
    "    if len(train_data) == 0:\n",
    "        print(\"No training data available. Exiting.\")\n",
    "        return\n",
    "    epoch_stats = train_model(model, train_data, num_epochs=5, learning_rate=1e-4, batch_size=2)\n",
    "    print(f\"\\n{'='*80}\\nTESTING PHASE\\n{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = create_training_data(test_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(test_data)} test examples\\n\")\n",
    "    if len(test_data) > 0:\n",
    "        test_metrics = evaluate_model(model, test_data, batch_size=2)\n",
    "        print_results(test_metrics, \"TEST SET\")\n",
    "    print(f\"\\n{'='*80}\\nTRAINING COMPLETE\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177e228-bebe-4776-97a1-9233ebcfa523",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf047b72-f0b7-4a89-95ce-ef8b9c377a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, balanced_accuracy_score, \n",
    "    matthews_corrcoef, roc_auc_score, average_precision_score, \n",
    "    f1_score, jaccard_score, cohen_kappa_score, log_loss, brier_score_loss\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class LoRAConfig:\n",
    "    def __init__(self, r: int = 8, lora_alpha: int = 16, lora_dropout: float = 0.1):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.scaling = lora_alpha / r\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, config: LoRAConfig):\n",
    "        super().__init__()\n",
    "        self.r = config.r\n",
    "        self.lora_alpha = config.lora_alpha\n",
    "        self.scaling = config.scaling\n",
    "        self.lora_dropout = nn.Dropout(p=config.lora_dropout)\n",
    "        self.lora_A = nn.Parameter(torch.zeros((in_features, config.r)))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((config.r, out_features)))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x, original_weight):\n",
    "        result = F.linear(x, original_weight)\n",
    "        lora_result = F.linear(F.linear(self.lora_dropout(x), self.lora_A.T), self.lora_B.T)\n",
    "        return result + lora_result * self.scaling\n",
    "\n",
    "\n",
    "class CodeCloneDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class LoRACodeT5(nn.Module):\n",
    "    def __init__(self, model_name: str, lora_config: LoRAConfig, num_classes: int = 2, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Initializing model across {num_gpus} GPUs\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(\"Loading base LLM model...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float32, \n",
    "            cache_dir=cache_dir, \n",
    "            low_cpu_mem_usage=True, \n",
    "            device_map='auto'\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        self.lora_config = lora_config\n",
    "        encoder_layers = len(base_model.encoder.block)\n",
    "        decoder_layers = len(base_model.decoder.block)\n",
    "        total_layers = encoder_layers + decoder_layers\n",
    "        print(f\"Model has {encoder_layers} encoder layers and {decoder_layers} decoder layers (total: {total_layers})\")\n",
    "        self.layer_devices = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer_device = next(base_model.encoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Encoder Layer {i} -> {layer_device}\")\n",
    "        for i in range(decoder_layers):\n",
    "            layer_device = next(base_model.decoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Decoder Layer {i} -> {layer_device}\")\n",
    "        embed_device = next(base_model.encoder.embed_tokens.parameters()).device\n",
    "        final_device = next(base_model.decoder.final_layer_norm.parameters()).device\n",
    "        print(f\"Embeddings -> {embed_device}\")\n",
    "        print(f\"Final Layer -> {final_device}\")\n",
    "        self.embed_device = str(embed_device)\n",
    "        self.final_device = str(final_device)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        self._inject_lora_layers()\n",
    "        self.num_classes = num_classes\n",
    "        hidden_size = base_model.config.d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 4, num_classes)\n",
    "        ).to(self.primary_device).float()\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Model initialization complete!\")\n",
    "    \n",
    "    def _inject_lora_layers(self):\n",
    "        encoder_layers = len(self.llm.encoder.block)\n",
    "        decoder_layers = len(self.llm.decoder.block)\n",
    "        d_model = self.llm.config.d_model\n",
    "        for layer_idx in range(encoder_layers):\n",
    "            layer = self.llm.encoder.block[layer_idx]\n",
    "            device = next(layer.parameters()).device\n",
    "            self_attn = layer.layer[0].SelfAttention\n",
    "            q_proj = self_attn.q\n",
    "            k_proj = self_attn.k\n",
    "            v_proj = self_attn.v\n",
    "            o_proj = self_attn.o\n",
    "            self.lora_layers[f'encoder_{layer_idx}_q'] = LoRALayer(d_model, d_model, self.lora_config).to(device)\n",
    "            self.lora_layers[f'encoder_{layer_idx}_v'] = LoRALayer(d_model, d_model, self.lora_config).to(device)\n",
    "            original_q_forward = q_proj.forward\n",
    "            original_v_forward = v_proj.forward\n",
    "            def make_lora_forward(orig_forward, lora_layer, orig_weight):\n",
    "                def new_forward(x):\n",
    "                    return lora_layer(x, orig_weight)\n",
    "                return new_forward\n",
    "            q_proj.forward = make_lora_forward(original_q_forward, self.lora_layers[f'encoder_{layer_idx}_q'], q_proj.weight)\n",
    "            v_proj.forward = make_lora_forward(original_v_forward, self.lora_layers[f'encoder_{layer_idx}_v'], v_proj.weight)\n",
    "        for layer_idx in range(decoder_layers):\n",
    "            layer = self.llm.decoder.block[layer_idx]\n",
    "            device = next(layer.parameters()).device\n",
    "            self_attn = layer.layer[0].SelfAttention\n",
    "            q_proj = self_attn.q\n",
    "            k_proj = self_attn.k\n",
    "            v_proj = self_attn.v\n",
    "            o_proj = self_attn.o\n",
    "            self.lora_layers[f'decoder_{layer_idx}_q'] = LoRALayer(d_model, d_model, self.lora_config).to(device)\n",
    "            self.lora_layers[f'decoder_{layer_idx}_v'] = LoRALayer(d_model, d_model, self.lora_config).to(device)\n",
    "            original_q_forward = q_proj.forward\n",
    "            original_v_forward = v_proj.forward\n",
    "            def make_lora_forward(orig_forward, lora_layer, orig_weight):\n",
    "                def new_forward(x):\n",
    "                    return lora_layer(x, orig_weight)\n",
    "                return new_forward\n",
    "            q_proj.forward = make_lora_forward(original_q_forward, self.lora_layers[f'decoder_{layer_idx}_q'], q_proj.weight)\n",
    "            v_proj.forward = make_lora_forward(original_v_forward, self.lora_layers[f'decoder_{layer_idx}_v'], v_proj.weight)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        input_ids = input_ids.to(self.embed_device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.embed_device)\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            decoder_input_ids=input_ids, \n",
    "            output_hidden_states=True, \n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden_states = outputs.decoder_hidden_states[-1]\n",
    "        if str(hidden_states.device) != self.primary_device:\n",
    "            hidden_states = hidden_states.to(self.primary_device)\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.device != logits.device:\n",
    "                labels = labels.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {'logits': logits, 'loss': loss, 'hidden_states': hidden_states}\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model: LoRACodeT5):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    lora_params = sum(p.numel() for p in model.lora_layers.parameters())\n",
    "    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "    trainable_params = lora_params + classifier_params\n",
    "    print(f\"\\n{'='*80}\\nPARAMETER STATISTICS\\n{'='*80}\")\n",
    "    print(f\"Total Frozen LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable LoRA Parameters: {lora_params:,}\")\n",
    "    print(f\"Trainable Classifier Parameters: {classifier_params:,}\")\n",
    "    print(f\"Total Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    print(f\"LoRA Rank: {model.lora_config.r}\")\n",
    "    print(f\"LoRA Alpha: {model.lora_config.lora_alpha}\")\n",
    "    print(f\"LoRA Scaling: {model.lora_config.scaling}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\\nGPU MEMORY USAGE\\n{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"GPU {i}: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(csv_path, label_col, func1_col, func2_col):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded dataset: {len(df)} samples\")\n",
    "        print(f\"Label distribution: {df[label_col].value_counts().to_dict()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_training_data(csv_path, tokenizer, max_length=512, label_col='label', func1_col='func1', func2_col='func2'):\n",
    "    df = load_dataset(csv_path, label_col, func1_col, func2_col)\n",
    "    if df is None:\n",
    "        return []\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        func1 = str(row[func1_col])\n",
    "        func2 = str(row[func2_col])\n",
    "        label = int(row[label_col])\n",
    "        combined_text = f\"Code1: {func1}\\nCode2: {func2}\\nAre these code clones?\"\n",
    "        encoding = tokenizer(\n",
    "            combined_text, \n",
    "            return_tensors='pt', \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        )\n",
    "        train_data.append({\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        })\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(embeddings, labels, k_values=[1, 3, 5, 10]):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(labels)):\n",
    "            true_label = labels[i]\n",
    "            sims = similarities[i].clone()\n",
    "            sims[i] = -float('inf')\n",
    "            top_k_indices = torch.topk(sims, min(k, len(labels)-1)).indices\n",
    "            top_k_labels = labels[top_k_indices]\n",
    "            if true_label in top_k_labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        results[f'recall@{k}'] = correct / total if total > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_mrr(embeddings, labels):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    mrr_sum = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        true_label = labels[i]\n",
    "        sims = similarities[i].clone()\n",
    "        sims[i] = -float('inf')\n",
    "        sorted_indices = torch.argsort(sims, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        for rank, label in enumerate(sorted_labels, 1):\n",
    "            if label == true_label:\n",
    "                mrr_sum += 1.0 / rank\n",
    "                break\n",
    "        count += 1\n",
    "    return mrr_sum / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba=None, embeddings=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape != (2, 2):\n",
    "        cm_2x2 = np.zeros((2, 2))\n",
    "        for i in range(min(cm.shape[0], 2)):\n",
    "            for j in range(min(cm.shape[1], 2)):\n",
    "                cm_2x2[i, j] = cm[i, j]\n",
    "        cm = cm_2x2\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    jacc = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_proba)\n",
    "            pr_auc = average_precision_score(y_true, y_proba)\n",
    "            logloss = log_loss(y_true, np.column_stack([1-y_proba, y_proba]))\n",
    "            brier = brier_score_loss(y_true, y_proba)\n",
    "        except:\n",
    "            roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    else:\n",
    "        roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    results = {\n",
    "        'cm': cm, 'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
    "        'acc': acc, 'bal_acc': bal_acc, 'prec': prec, 'rec': rec,\n",
    "        'specificity': specificity, 'npv': npv, 'fpr': fpr, 'fnr': fnr,\n",
    "        'f1': f1, 'mcc': mcc, 'jacc': jacc, 'kappa': kappa,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc, 'log_loss': logloss, 'brier': brier\n",
    "    }\n",
    "    if embeddings is not None:\n",
    "        recall_metrics = calculate_recall_at_k(embeddings, torch.tensor(y_true))\n",
    "        mrr = calculate_mrr(embeddings, torch.tensor(y_true))\n",
    "        results.update(recall_metrics)\n",
    "        results['mrr'] = mrr\n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=15, learning_rate=1e-4, batch_size=2):\n",
    "    lora_params = list(model.lora_layers.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [{'params': lora_params}, {'params': model.classifier.parameters()}], \n",
    "        lr=learning_rate, \n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    dataset = CodeCloneDataset(train_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model.train()\n",
    "    epoch_stats = []\n",
    "    start_time = time.time()\n",
    "    initial_memory = get_memory_usage()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_count = 0\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(lora_params + list(model.classifier.parameters()), 1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                with torch.no_grad():\n",
    "                    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                    correct += (predictions == labels.to(predictions.device)).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        current_memory = get_memory_usage()\n",
    "        epoch_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'acc': accuracy,\n",
    "            'time': epoch_time,\n",
    "            'memory_mb': current_memory\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}, Time: {epoch_time:.2f}s, Memory: {current_memory:.2f}MB\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "    total_training_time = time.time() - start_time\n",
    "    peak_memory = max([stat['memory_mb'] for stat in epoch_stats])\n",
    "    memory_increase = peak_memory - initial_memory\n",
    "    print(f\"\\n{'='*80}\\nTRAINING SUMMARY\\n{'='*80}\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Peak Memory Usage: {peak_memory:.2f}MB\")\n",
    "    print(f\"Memory Increase: {memory_increase:.2f}MB\\n{'='*80}\\n\")\n",
    "    return epoch_stats\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, batch_size=2):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_embeddings = []\n",
    "    dataset = CodeCloneDataset(test_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            pooled_embeddings = hidden_states.mean(dim=1)\n",
    "            pooled_embeddings = F.normalize(pooled_embeddings, p=2, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "            all_embeddings.append(pooled_embeddings.cpu())\n",
    "            if len(all_predictions) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    inference_time = time.time() - start_time\n",
    "    samples_per_sec = len(test_data) / inference_time\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    metrics = calculate_comprehensive_metrics(all_labels, all_predictions, all_probabilities, all_embeddings)\n",
    "    metrics['inference_time'] = inference_time\n",
    "    metrics['samples_per_sec'] = samples_per_sec\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_results(metrics, dataset_name):\n",
    "    print(f\"\\n{'='*80}\\nRESULTS: {dataset_name}\\n{'='*80}\\n\\nConfusion Matrix:\\n{metrics['cm']}\\n\\nTrue Negatives (TN): {metrics['tn']}\\nFalse Positives (FP): {metrics['fp']}\\nFalse Negatives (FN): {metrics['fn']}\\nTrue Positives (TP): {metrics['tp']}\\n\\n{'='*80}\\nCLASSIFICATION METRICS\\n{'='*80}\")\n",
    "    print(f\"Accuracy: {metrics['acc']:.4f}\\nBalanced Accuracy: {metrics['bal_acc']:.4f}\\nPrecision: {metrics['prec']:.4f}\\nRecall (Sensitivity): {metrics['rec']:.4f}\\nF1 Score: {metrics['f1']:.4f}\\nSpecificity: {metrics['specificity']:.4f}\\nNegative Predictive Value (NPV): {metrics['npv']:.4f}\\nFalse Positive Rate (FPR): {metrics['fpr']:.4f}\\nFalse Negative Rate (FNR): {metrics['fnr']:.4f}\")\n",
    "    print(f\"\\n{'='*80}\\nADVANCED METRICS\\n{'='*80}\\nJaccard Score: {metrics['jacc']:.4f}\\nMatthews Correlation Coefficient (MCC): {metrics['mcc']:.4f}\\nCohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPROBABILISTIC METRICS\\n{'='*80}\\nROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "    if 'pr_auc' in metrics:\n",
    "        print(f\"Precision-Recall AUC: {metrics['pr_auc']:.4f}\")\n",
    "    if 'log_loss' in metrics:\n",
    "        print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n",
    "    if 'brier' in metrics:\n",
    "        print(f\"Brier Score: {metrics['brier']:.4f}\")\n",
    "    if 'recall@1' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nRETRIEVAL METRICS\\n{'='*80}\\nRecall@1: {metrics['recall@1']:.4f}\\nRecall@3: {metrics['recall@3']:.4f}\\nRecall@5: {metrics['recall@5']:.4f}\\nRecall@10: {metrics['recall@10']:.4f}\\nMean Reciprocal Rank (MRR): {metrics['mrr']:.4f}\")\n",
    "    if 'inference_time' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPERFORMANCE METRICS\\n{'='*80}\\nInference Time: {metrics['inference_time']:.2f}s\\nThroughput: {metrics['samples_per_sec']:.2f} samples/sec\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    cache_dir = '/hf_cache'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\", cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    lora_config = LoRAConfig(r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "    print(\"\\nLoading model with LoRA (rank=8) and pipeline parallelism across GPUs...\")\n",
    "    model = LoRACodeT5(\"Salesforce/codet5p-770m\", lora_config, num_classes=2, cache_dir=cache_dir)\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\\nTRAINING PHASE\\n{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = create_training_data(train_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(train_data)} training examples\\n\")\n",
    "    if len(train_data) == 0:\n",
    "        print(\"No training data available. Exiting.\")\n",
    "        return\n",
    "    epoch_stats = train_model(model, train_data, num_epochs=5, learning_rate=1e-4, batch_size=2)\n",
    "    print(f\"\\n{'='*80}\\nTESTING PHASE\\n{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = create_training_data(test_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(test_data)} test examples\\n\")\n",
    "    if len(test_data) > 0:\n",
    "        test_metrics = evaluate_model(model, test_data, batch_size=2)\n",
    "        print_results(test_metrics, \"TEST SET\")\n",
    "    print(f\"\\n{'='*80}\\nTRAINING COMPLETE\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53a58b-9103-41ff-aa29-13e498b9b667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# bitfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c866f88-346b-4775-b2b8-157013bd5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, balanced_accuracy_score, \n",
    "    matthews_corrcoef, roc_auc_score, average_precision_score, \n",
    "    f1_score, jaccard_score, cohen_kappa_score, log_loss, brier_score_loss\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BitFitConfig:\n",
    "    def __init__(self, bias_type: str = 'all'):\n",
    "        self.bias_type = bias_type\n",
    "\n",
    "\n",
    "class CodeCloneDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class BitFitCodeT5(nn.Module):\n",
    "    def __init__(self, model_name: str, bitfit_config: BitFitConfig, num_classes: int = 2, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Initializing model across {num_gpus} GPUs\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(\"Loading base LLM model...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.float32, \n",
    "            cache_dir=cache_dir, \n",
    "            low_cpu_mem_usage=True, \n",
    "            device_map='auto'\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        self.bitfit_config = bitfit_config\n",
    "        encoder_layers = len(base_model.encoder.block)\n",
    "        decoder_layers = len(base_model.decoder.block)\n",
    "        total_layers = encoder_layers + decoder_layers\n",
    "        print(f\"Model has {encoder_layers} encoder layers and {decoder_layers} decoder layers (total: {total_layers})\")\n",
    "        self.layer_devices = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer_device = next(base_model.encoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Encoder Layer {i} -> {layer_device}\")\n",
    "        for i in range(decoder_layers):\n",
    "            layer_device = next(base_model.decoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Decoder Layer {i} -> {layer_device}\")\n",
    "        embed_device = next(base_model.encoder.embed_tokens.parameters()).device\n",
    "        final_device = next(base_model.decoder.final_layer_norm.parameters()).device\n",
    "        print(f\"Embeddings -> {embed_device}\")\n",
    "        print(f\"Final Layer -> {final_device}\")\n",
    "        self.embed_device = str(embed_device)\n",
    "        self.final_device = str(final_device)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._enable_bias_training()\n",
    "        self.num_classes = num_classes\n",
    "        hidden_size = base_model.config.d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 4, num_classes)\n",
    "        ).to(self.primary_device).float()\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Model initialization complete!\")\n",
    "    \n",
    "    def _enable_bias_training(self):\n",
    "        trainable_bias_count = 0\n",
    "        for name, param in self.llm.named_parameters():\n",
    "            if 'bias' in name.lower():\n",
    "                if self.bitfit_config.bias_type == 'all':\n",
    "                    param.requires_grad = True\n",
    "                    trainable_bias_count += param.numel()\n",
    "                elif self.bitfit_config.bias_type == 'query_mlp':\n",
    "                    if any(x in name.lower() for x in ['q.bias', 'query.bias', 'dense.bias']):\n",
    "                        param.requires_grad = True\n",
    "                        trainable_bias_count += param.numel()\n",
    "        print(f\"BitFit enabled with {trainable_bias_count:,} trainable bias parameters\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        input_ids = input_ids.to(self.embed_device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.embed_device)\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            decoder_input_ids=input_ids, \n",
    "            output_hidden_states=True, \n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden_states = outputs.decoder_hidden_states[-1]\n",
    "        if str(hidden_states.device) != self.primary_device:\n",
    "            hidden_states = hidden_states.to(self.primary_device)\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.device != logits.device:\n",
    "                labels = labels.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {'logits': logits, 'loss': loss, 'hidden_states': hidden_states}\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model: BitFitCodeT5):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    bias_params = sum(p.numel() for p in model.llm.parameters() if p.requires_grad)\n",
    "    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "    trainable_params = bias_params + classifier_params\n",
    "    print(f\"\\n{'='*80}\\nPARAMETER STATISTICS\\n{'='*80}\")\n",
    "    print(f\"Total Frozen LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Bias Parameters: {bias_params:,}\")\n",
    "    print(f\"Trainable Classifier Parameters: {classifier_params:,}\")\n",
    "    print(f\"Total Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    print(f\"BitFit Bias Type: {model.bitfit_config.bias_type}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\\nGPU MEMORY USAGE\\n{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"GPU {i}: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(csv_path, label_col, func1_col, func2_col):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded dataset: {len(df)} samples\")\n",
    "        print(f\"Label distribution: {df[label_col].value_counts().to_dict()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_training_data(csv_path, tokenizer, max_length=512, label_col='label', func1_col='func1', func2_col='func2'):\n",
    "    df = load_dataset(csv_path, label_col, func1_col, func2_col)\n",
    "    if df is None:\n",
    "        return []\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        func1 = str(row[func1_col])\n",
    "        func2 = str(row[func2_col])\n",
    "        label = int(row[label_col])\n",
    "        combined_text = f\"Code1: {func1}\\nCode2: {func2}\\nAre these code clones?\"\n",
    "        encoding = tokenizer(\n",
    "            combined_text, \n",
    "            return_tensors='pt', \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        )\n",
    "        train_data.append({\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        })\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(embeddings, labels, k_values=[1, 3, 5, 10]):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(labels)):\n",
    "            true_label = labels[i]\n",
    "            sims = similarities[i].clone()\n",
    "            sims[i] = -float('inf')\n",
    "            top_k_indices = torch.topk(sims, min(k, len(labels)-1)).indices\n",
    "            top_k_labels = labels[top_k_indices]\n",
    "            if true_label in top_k_labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        results[f'recall@{k}'] = correct / total if total > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_mrr(embeddings, labels):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    mrr_sum = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        true_label = labels[i]\n",
    "        sims = similarities[i].clone()\n",
    "        sims[i] = -float('inf')\n",
    "        sorted_indices = torch.argsort(sims, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        for rank, label in enumerate(sorted_labels, 1):\n",
    "            if label == true_label:\n",
    "                mrr_sum += 1.0 / rank\n",
    "                break\n",
    "        count += 1\n",
    "    return mrr_sum / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba=None, embeddings=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape != (2, 2):\n",
    "        cm_2x2 = np.zeros((2, 2))\n",
    "        for i in range(min(cm.shape[0], 2)):\n",
    "            for j in range(min(cm.shape[1], 2)):\n",
    "                cm_2x2[i, j] = cm[i, j]\n",
    "        cm = cm_2x2\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    jacc = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_proba)\n",
    "            pr_auc = average_precision_score(y_true, y_proba)\n",
    "            logloss = log_loss(y_true, np.column_stack([1-y_proba, y_proba]))\n",
    "            brier = brier_score_loss(y_true, y_proba)\n",
    "        except:\n",
    "            roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    else:\n",
    "        roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    results = {\n",
    "        'cm': cm, 'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
    "        'acc': acc, 'bal_acc': bal_acc, 'prec': prec, 'rec': rec,\n",
    "        'specificity': specificity, 'npv': npv, 'fpr': fpr, 'fnr': fnr,\n",
    "        'f1': f1, 'mcc': mcc, 'jacc': jacc, 'kappa': kappa,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc, 'log_loss': logloss, 'brier': brier\n",
    "    }\n",
    "    if embeddings is not None:\n",
    "        recall_metrics = calculate_recall_at_k(embeddings, torch.tensor(y_true))\n",
    "        mrr = calculate_mrr(embeddings, torch.tensor(y_true))\n",
    "        results.update(recall_metrics)\n",
    "        results['mrr'] = mrr\n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=1e-3, batch_size=2):\n",
    "    bias_params = [p for p in model.llm.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [{'params': bias_params}, {'params': model.classifier.parameters()}], \n",
    "        lr=learning_rate, \n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    dataset = CodeCloneDataset(train_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model.train()\n",
    "    epoch_stats = []\n",
    "    start_time = time.time()\n",
    "    initial_memory = get_memory_usage()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_count = 0\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(bias_params + list(model.classifier.parameters()), 1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                with torch.no_grad():\n",
    "                    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                    correct += (predictions == labels.to(predictions.device)).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            batch_count += 1\n",
    "            if batch_count % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        current_memory = get_memory_usage()\n",
    "        epoch_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'acc': accuracy,\n",
    "            'time': epoch_time,\n",
    "            'memory_mb': current_memory\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}, Time: {epoch_time:.2f}s, Memory: {current_memory:.2f}MB\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "    total_training_time = time.time() - start_time\n",
    "    peak_memory = max([stat['memory_mb'] for stat in epoch_stats])\n",
    "    memory_increase = peak_memory - initial_memory\n",
    "    print(f\"\\n{'='*80}\\nTRAINING SUMMARY\\n{'='*80}\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Peak Memory Usage: {peak_memory:.2f}MB\")\n",
    "    print(f\"Memory Increase: {memory_increase:.2f}MB\\n{'='*80}\\n\")\n",
    "    return epoch_stats\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, batch_size=2):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_embeddings = []\n",
    "    dataset = CodeCloneDataset(test_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            pooled_embeddings = hidden_states.mean(dim=1)\n",
    "            pooled_embeddings = F.normalize(pooled_embeddings, p=2, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "            all_embeddings.append(pooled_embeddings.cpu())\n",
    "            if len(all_predictions) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    inference_time = time.time() - start_time\n",
    "    samples_per_sec = len(test_data) / inference_time\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    metrics = calculate_comprehensive_metrics(all_labels, all_predictions, all_probabilities, all_embeddings)\n",
    "    metrics['inference_time'] = inference_time\n",
    "    metrics['samples_per_sec'] = samples_per_sec\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_results(metrics, dataset_name):\n",
    "    print(f\"\\n{'='*80}\\nRESULTS: {dataset_name}\\n{'='*80}\\n\\nConfusion Matrix:\\n{metrics['cm']}\\n\\nTrue Negatives (TN): {metrics['tn']}\\nFalse Positives (FP): {metrics['fp']}\\nFalse Negatives (FN): {metrics['fn']}\\nTrue Positives (TP): {metrics['tp']}\\n\\n{'='*80}\\nCLASSIFICATION METRICS\\n{'='*80}\")\n",
    "    print(f\"Accuracy: {metrics['acc']:.4f}\\nBalanced Accuracy: {metrics['bal_acc']:.4f}\\nPrecision: {metrics['prec']:.4f}\\nRecall (Sensitivity): {metrics['rec']:.4f}\\nF1 Score: {metrics['f1']:.4f}\\nSpecificity: {metrics['specificity']:.4f}\\nNegative Predictive Value (NPV): {metrics['npv']:.4f}\\nFalse Positive Rate (FPR): {metrics['fpr']:.4f}\\nFalse Negative Rate (FNR): {metrics['fnr']:.4f}\")\n",
    "    print(f\"\\n{'='*80}\\nADVANCED METRICS\\n{'='*80}\\nJaccard Score: {metrics['jacc']:.4f}\\nMatthews Correlation Coefficient (MCC): {metrics['mcc']:.4f}\\nCohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPROBABILISTIC METRICS\\n{'='*80}\\nROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "    if 'pr_auc' in metrics:\n",
    "        print(f\"Precision-Recall AUC: {metrics['pr_auc']:.4f}\")\n",
    "    if 'log_loss' in metrics:\n",
    "        print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n",
    "    if 'brier' in metrics:\n",
    "        print(f\"Brier Score: {metrics['brier']:.4f}\")\n",
    "    if 'recall@1' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nRETRIEVAL METRICS\\n{'='*80}\\nRecall@1: {metrics['recall@1']:.4f}\\nRecall@3: {metrics['recall@3']:.4f}\\nRecall@5: {metrics['recall@5']:.4f}\\nRecall@10: {metrics['recall@10']:.4f}\\nMean Reciprocal Rank (MRR): {metrics['mrr']:.4f}\")\n",
    "    if 'inference_time' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPERFORMANCE METRICS\\n{'='*80}\\nInference Time: {metrics['inference_time']:.2f}s\\nThroughput: {metrics['samples_per_sec']:.2f} samples/sec\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    cache_dir = '/hf_cache'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\", cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    bitfit_config = BitFitConfig(bias_type='all')\n",
    "    print(\"\\nLoading model with BitFit (bias-only fine-tuning) and pipeline parallelism across GPUs...\")\n",
    "    model = BitFitCodeT5(\"Salesforce/codet5p-770m\", bitfit_config, num_classes=2, cache_dir=cache_dir)\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\\nTRAINING PHASE\\n{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = create_training_data(train_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(train_data)} training examples\\n\")\n",
    "    if len(train_data) == 0:\n",
    "        print(\"No training data available. Exiting.\")\n",
    "        return\n",
    "    epoch_stats = train_model(model, train_data, num_epochs=5, learning_rate=1e-3, batch_size=8)\n",
    "    print(f\"\\n{'='*80}\\nTESTING PHASE\\n{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = create_training_data(test_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(test_data)} test examples\\n\")\n",
    "    if len(test_data) > 0:\n",
    "        test_metrics = evaluate_model(model, test_data, batch_size=8)\n",
    "        print_results(test_metrics, \"TEST SET\")\n",
    "    print(f\"\\n{'='*80}\\nTRAINING COMPLETE\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4133ce-6b00-4da8-a880-1a8961b5d8fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TS PEft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6968d78-7549-4ea5-9980-d1c1ebc87c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, balanced_accuracy_score, \n",
    "    matthews_corrcoef, roc_auc_score, average_precision_score, \n",
    "    f1_score, jaccard_score, cohen_kappa_score, log_loss, brier_score_loss\n",
    ")\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TSPEFTConfig:\n",
    "    rank: int = 16\n",
    "    alpha: float = 0.5\n",
    "    dropout: float = 0.05\n",
    "    s: float = 4e-5\n",
    "    lambda_reg: float = 4.5e-5\n",
    "    target_modules: List[str] = None\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.98\n",
    "    eps: float = 1e-8\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"]\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 4, alpha: float = 1.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return result * self.scaling\n",
    "\n",
    "\n",
    "class TSPEFTLayer(nn.Module):\n",
    "    def __init__(self, base_layer: nn.Linear, rank: int = 4, alpha: float = 1.0, dropout: float = 0.0, \n",
    "                 s: float = 4e-5, lambda_reg: float = 1e-5, beta1: float = 0.9, beta2: float = 0.98, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.base_layer.requires_grad_(False)\n",
    "        self.lora = LoRALayer(base_layer.in_features, base_layer.out_features, rank=rank, alpha=alpha, dropout=dropout)\n",
    "        self.s = s\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.register_buffer('tau', torch.tensor(0.0))\n",
    "        self.register_buffer('m', torch.tensor(0.0))\n",
    "        self.register_buffer('v', torch.tensor(0.0))\n",
    "        self.register_buffer('step', torch.tensor(0))\n",
    "        \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.base_layer.weight\n",
    "    \n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self.base_layer.bias if hasattr(self.base_layer, 'bias') else None\n",
    "    \n",
    "    @property\n",
    "    def in_features(self):\n",
    "        return self.base_layer.in_features\n",
    "    \n",
    "    @property\n",
    "    def out_features(self):\n",
    "        return self.base_layer.out_features\n",
    "        \n",
    "    def compute_relative_magnitude(self, base_output: torch.Tensor, lora_output: torch.Tensor) -> torch.Tensor:\n",
    "        base_norm = torch.norm(base_output, p=2, dim=-1, keepdim=True)\n",
    "        lora_norm = torch.norm(lora_output, p=2, dim=-1, keepdim=True)\n",
    "        r_i = lora_norm / (base_norm + self.eps)\n",
    "        return r_i.squeeze(-1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base_output = self.base_layer(x)\n",
    "        lora_output = self.lora(x)\n",
    "        if not self.training:\n",
    "            r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "            gate = (r_i >= self.tau).float().unsqueeze(-1)\n",
    "            return base_output + gate * lora_output\n",
    "        r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "        gate = (r_i >= self.tau).float()\n",
    "        gated_output = base_output + gate.unsqueeze(-1) * lora_output\n",
    "        self._cache_for_backward = {'r_i': r_i, 'gate': gate, 'lora_output': lora_output, 'base_output': base_output}\n",
    "        return gated_output\n",
    "    \n",
    "    def compute_threshold_gradient(self, grad_output: torch.Tensor) -> float:\n",
    "        if not hasattr(self, '_cache_for_backward'):\n",
    "            return 0.0\n",
    "        cache = self._cache_for_backward\n",
    "        r_i = cache['r_i']\n",
    "        gate = cache['gate']\n",
    "        lora_output = cache['lora_output']\n",
    "        mu_i = (grad_output * lora_output).sum(dim=-1)\n",
    "        consistency_mask = ((mu_i >= 0).float() == gate).float()\n",
    "        sparsity_mask = gate\n",
    "        grad_loss = -self.s * (consistency_mask * mu_i).sum()\n",
    "        grad_sparsity = -self.s * (sparsity_mask * self.lambda_reg).sum()\n",
    "        g_k = grad_loss + grad_sparsity\n",
    "        return g_k.item()\n",
    "    \n",
    "    def update_threshold(self, grad_output: torch.Tensor, lr: float = 1.0):\n",
    "        if not self.training:\n",
    "            return\n",
    "        g_k = self.compute_threshold_gradient(grad_output)\n",
    "        self.step += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g_k\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g_k ** 2)\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.step.item())\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.step.item())\n",
    "        tau_update = lr * self.s * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "        self.tau = torch.clamp(self.tau + tau_update, min=0.0)\n",
    "        if hasattr(self, '_cache_for_backward'):\n",
    "            delattr(self, '_cache_for_backward')\n",
    "\n",
    "\n",
    "class CodeCloneDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class TSPEFTCodeT5(nn.Module):\n",
    "    def __init__(self, model_name: str, tspeft_config: TSPEFTConfig, num_classes: int = 2, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Initializing model across {num_gpus} GPUs\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(\"Loading base LLM model...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, torch_dtype=torch.float32, cache_dir=cache_dir, \n",
    "            low_cpu_mem_usage=True, device_map='auto'\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        if hasattr(self.llm, 'gradient_checkpointing_enable'):\n",
    "            self.llm.gradient_checkpointing_enable()\n",
    "        self.tspeft_config = tspeft_config\n",
    "        encoder_layers = len(base_model.encoder.block)\n",
    "        decoder_layers = len(base_model.decoder.block)\n",
    "        total_layers = encoder_layers + decoder_layers\n",
    "        print(f\"Model has {encoder_layers} encoder layers and {decoder_layers} decoder layers (total: {total_layers})\")\n",
    "        self.layer_devices = []\n",
    "        for i in range(encoder_layers):\n",
    "            layer_device = next(base_model.encoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Encoder Layer {i} -> {layer_device}\")\n",
    "        for i in range(decoder_layers):\n",
    "            layer_device = next(base_model.decoder.block[i].parameters()).device\n",
    "            self.layer_devices.append(str(layer_device))\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Decoder Layer {i} -> {layer_device}\")\n",
    "        embed_device = next(base_model.encoder.embed_tokens.parameters()).device\n",
    "        final_device = next(base_model.decoder.final_layer_norm.parameters()).device\n",
    "        print(f\"Embeddings -> {embed_device}\")\n",
    "        print(f\"Final Layer -> {final_device}\")\n",
    "        self.embed_device = str(embed_device)\n",
    "        self.final_device = str(final_device)\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.tspeft_layers = nn.ModuleDict()\n",
    "        self._inject_tspeft_layers()\n",
    "        self.num_classes = num_classes\n",
    "        hidden_size = base_model.config.d_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 4, num_classes)\n",
    "        ).to(self.primary_device).float()\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Model initialization complete!\")\n",
    "    \n",
    "    def _inject_tspeft_layers(self):\n",
    "        trainable_params = 0\n",
    "        layer_counter = 0\n",
    "        for name, module in self.llm.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                layer_name = name.split('.')[-1]\n",
    "                should_inject = any(target in layer_name for target in self.tspeft_config.target_modules)\n",
    "                if should_inject:\n",
    "                    tspeft_layer = TSPEFTLayer(\n",
    "                        base_layer=module,\n",
    "                        rank=self.tspeft_config.rank,\n",
    "                        alpha=self.tspeft_config.alpha,\n",
    "                        dropout=self.tspeft_config.dropout,\n",
    "                        s=self.tspeft_config.s,\n",
    "                        lambda_reg=self.tspeft_config.lambda_reg,\n",
    "                        beta1=self.tspeft_config.beta1,\n",
    "                        beta2=self.tspeft_config.beta2,\n",
    "                        eps=self.tspeft_config.eps\n",
    "                    )\n",
    "                    layer_device = next(module.parameters()).device\n",
    "                    tspeft_layer = tspeft_layer.to(layer_device)\n",
    "                    safe_key = f\"tspeft_layer_{layer_counter}\"\n",
    "                    self.tspeft_layers[safe_key] = tspeft_layer\n",
    "                    layer_counter += 1\n",
    "                    parent = self.llm\n",
    "                    if parent_name:\n",
    "                        for part in parent_name.split('.'):\n",
    "                            parent = getattr(parent, part)\n",
    "                    setattr(parent, layer_name, tspeft_layer)\n",
    "                    trainable_params += tspeft_layer.lora.lora_A.numel() + tspeft_layer.lora.lora_B.numel()\n",
    "        print(f\"TS-PEFT enabled with {trainable_params:,} trainable parameters across {len(self.tspeft_layers)} layers\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        input_ids = input_ids.to(self.embed_device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.embed_device)\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, \n",
    "            decoder_input_ids=input_ids, output_hidden_states=True, return_dict=True\n",
    "        )\n",
    "        hidden_states = outputs.decoder_hidden_states[-1]\n",
    "        if str(hidden_states.device) != self.primary_device:\n",
    "            hidden_states = hidden_states.to(self.primary_device)\n",
    "        pooled_output = hidden_states.mean(dim=1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.device != logits.device:\n",
    "                labels = labels.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {'logits': logits, 'loss': loss, 'hidden_states': hidden_states}\n",
    "    \n",
    "    def update_thresholds(self, lr: float = 1.0):\n",
    "        for layer in self.tspeft_layers.values():\n",
    "            if hasattr(layer, '_cache_for_backward') and layer.training:\n",
    "                grad_output = torch.ones_like(layer._cache_for_backward['base_output'])\n",
    "                layer.update_threshold(grad_output, lr)\n",
    "    \n",
    "    def get_sparsity_stats(self) -> Dict[str, float]:\n",
    "        stats = {}\n",
    "        for name, layer in self.tspeft_layers.items():\n",
    "            if hasattr(layer, '_cache_for_backward'):\n",
    "                gate = layer._cache_for_backward['gate']\n",
    "                sparsity = (1 - gate.float().mean()).item() * 100\n",
    "                stats[name] = sparsity\n",
    "        return stats\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model: TSPEFTCodeT5):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    tspeft_params = sum(p.numel() for p in model.tspeft_layers.parameters() if p.requires_grad)\n",
    "    classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
    "    trainable_params = tspeft_params + classifier_params\n",
    "    print(f\"\\n{'='*80}\\nPARAMETER STATISTICS\\n{'='*80}\")\n",
    "    print(f\"Total Frozen LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable TS-PEFT Parameters: {tspeft_params:,}\")\n",
    "    print(f\"Trainable Classifier Parameters: {classifier_params:,}\")\n",
    "    print(f\"Total Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    print(f\"TS-PEFT Rank: {model.tspeft_config.rank}\")\n",
    "    print(f\"TS-PEFT Alpha: {model.tspeft_config.alpha}\")\n",
    "    print(f\"TS-PEFT s: {model.tspeft_config.s}\")\n",
    "    print(f\"TS-PEFT lambda_reg: {model.tspeft_config.lambda_reg}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\\nGPU MEMORY USAGE\\n{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"GPU {i}: Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(csv_path, label_col, func1_col, func2_col):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded dataset: {len(df)} samples\")\n",
    "        print(f\"Label distribution: {df[label_col].value_counts().to_dict()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_training_data(csv_path, tokenizer, max_length=512, label_col='label', func1_col='func1', func2_col='func2'):\n",
    "    df = load_dataset(csv_path, label_col, func1_col, func2_col)\n",
    "    if df is None:\n",
    "        return []\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        func1 = str(row[func1_col])\n",
    "        func2 = str(row[func2_col])\n",
    "        label = int(row[label_col])\n",
    "        combined_text = f\"Code1: {func1}\\nCode2: {func2}\\nAre these code clones?\"\n",
    "        encoding = tokenizer(\n",
    "            combined_text, return_tensors='pt', padding='max_length', \n",
    "            truncation=True, max_length=max_length\n",
    "        )\n",
    "        train_data.append({\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        })\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(embeddings, labels, k_values=[1, 3, 5, 10]):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(labels)):\n",
    "            true_label = labels[i]\n",
    "            sims = similarities[i].clone()\n",
    "            sims[i] = -float('inf')\n",
    "            top_k_indices = torch.topk(sims, min(k, len(labels)-1)).indices\n",
    "            top_k_labels = labels[top_k_indices]\n",
    "            if true_label in top_k_labels:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        results[f'recall@{k}'] = correct / total if total > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_mrr(embeddings, labels):\n",
    "    similarities = torch.mm(embeddings, embeddings.t())\n",
    "    mrr_sum = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        true_label = labels[i]\n",
    "        sims = similarities[i].clone()\n",
    "        sims[i] = -float('inf')\n",
    "        sorted_indices = torch.argsort(sims, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        for rank, label in enumerate(sorted_labels, 1):\n",
    "            if label == true_label:\n",
    "                mrr_sum += 1.0 / rank\n",
    "                break\n",
    "        count += 1\n",
    "    return mrr_sum / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba=None, embeddings=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape != (2, 2):\n",
    "        cm_2x2 = np.zeros((2, 2))\n",
    "        for i in range(min(cm.shape[0], 2)):\n",
    "            for j in range(min(cm.shape[1], 2)):\n",
    "                cm_2x2[i, j] = cm[i, j]\n",
    "        cm = cm_2x2\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "    jacc = jaccard_score(y_true, y_pred, zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_proba)\n",
    "            pr_auc = average_precision_score(y_true, y_proba)\n",
    "            logloss = log_loss(y_true, np.column_stack([1-y_proba, y_proba]))\n",
    "            brier = brier_score_loss(y_true, y_proba)\n",
    "        except:\n",
    "            roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    else:\n",
    "        roc_auc = pr_auc = logloss = brier = 0.0\n",
    "    results = {\n",
    "        'cm': cm, 'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp),\n",
    "        'acc': acc, 'bal_acc': bal_acc, 'prec': prec, 'rec': rec,\n",
    "        'specificity': specificity, 'npv': npv, 'fpr': fpr, 'fnr': fnr,\n",
    "        'f1': f1, 'mcc': mcc, 'jacc': jacc, 'kappa': kappa,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc, 'log_loss': logloss, 'brier': brier\n",
    "    }\n",
    "    if embeddings is not None:\n",
    "        recall_metrics = calculate_recall_at_k(embeddings, torch.tensor(y_true))\n",
    "        mrr = calculate_mrr(embeddings, torch.tensor(y_true))\n",
    "        results.update(recall_metrics)\n",
    "        results['mrr'] = mrr\n",
    "    return results\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=1e-3, batch_size=2):\n",
    "    tspeft_params = [p for p in model.tspeft_layers.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(\n",
    "        [{'params': tspeft_params}, {'params': model.classifier.parameters()}], \n",
    "        lr=learning_rate, weight_decay=0.01\n",
    "    )\n",
    "    dataset = CodeCloneDataset(train_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model.train()\n",
    "    epoch_stats = []\n",
    "    start_time = time.time()\n",
    "    initial_memory = get_memory_usage()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_count = 0\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                model.update_thresholds(lr=1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(tspeft_params + list(model.classifier.parameters()), 1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                with torch.no_grad():\n",
    "                    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                    correct += (predictions == labels.to(predictions.device)).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                del outputs, loss, predictions\n",
    "                torch.cuda.empty_cache()\n",
    "            batch_count += 1\n",
    "            if batch_count % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        current_memory = get_memory_usage()\n",
    "        sparsity_stats = model.get_sparsity_stats()\n",
    "        avg_sparsity = sum(sparsity_stats.values()) / len(sparsity_stats) if sparsity_stats else 0.0\n",
    "        epoch_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': avg_loss,\n",
    "            'acc': accuracy,\n",
    "            'sparsity': avg_sparsity,\n",
    "            'time': epoch_time,\n",
    "            'memory_mb': current_memory\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}, Sparsity: {avg_sparsity:.2f}%, Time: {epoch_time:.2f}s, Memory: {current_memory:.2f}MB\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "    total_training_time = time.time() - start_time\n",
    "    peak_memory = max([stat['memory_mb'] for stat in epoch_stats])\n",
    "    memory_increase = peak_memory - initial_memory\n",
    "    print(f\"\\n{'='*80}\\nTRAINING SUMMARY\\n{'='*80}\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Peak Memory Usage: {peak_memory:.2f}MB\")\n",
    "    print(f\"Memory Increase: {memory_increase:.2f}MB\\n{'='*80}\\n\")\n",
    "    return epoch_stats\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, batch_size=2):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_embeddings = []\n",
    "    dataset = CodeCloneDataset(test_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids']\n",
    "            attention_mask = batch_data['attention_mask']\n",
    "            labels = batch_data['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            pooled_embeddings = hidden_states.mean(dim=1)\n",
    "            pooled_embeddings = F.normalize(pooled_embeddings, p=2, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
    "            all_embeddings.append(pooled_embeddings.cpu())\n",
    "            if len(all_predictions) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    inference_time = time.time() - start_time\n",
    "    samples_per_sec = len(test_data) / inference_time\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    metrics = calculate_comprehensive_metrics(all_labels, all_predictions, all_probabilities, all_embeddings)\n",
    "    metrics['inference_time'] = inference_time\n",
    "    metrics['samples_per_sec'] = samples_per_sec\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_results(metrics, dataset_name):\n",
    "    print(f\"\\n{'='*80}\\nRESULTS: {dataset_name}\\n{'='*80}\\n\\nConfusion Matrix:\\n{metrics['cm']}\\n\\nTrue Negatives (TN): {metrics['tn']}\\nFalse Positives (FP): {metrics['fp']}\\nFalse Negatives (FN): {metrics['fn']}\\nTrue Positives (TP): {metrics['tp']}\\n\\n{'='*80}\\nCLASSIFICATION METRICS\\n{'='*80}\")\n",
    "    print(f\"Accuracy: {metrics['acc']:.4f}\\nBalanced Accuracy: {metrics['bal_acc']:.4f}\\nPrecision: {metrics['prec']:.4f}\\nRecall (Sensitivity): {metrics['rec']:.4f}\\nF1 Score: {metrics['f1']:.4f}\\nSpecificity: {metrics['specificity']:.4f}\\nNegative Predictive Value (NPV): {metrics['npv']:.4f}\\nFalse Positive Rate (FPR): {metrics['fpr']:.4f}\\nFalse Negative Rate (FNR): {metrics['fnr']:.4f}\")\n",
    "    print(f\"\\n{'='*80}\\nADVANCED METRICS\\n{'='*80}\\nJaccard Score: {metrics['jacc']:.4f}\\nMatthews Correlation Coefficient (MCC): {metrics['mcc']:.4f}\\nCohen's Kappa: {metrics['kappa']:.4f}\")\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPROBABILISTIC METRICS\\n{'='*80}\\nROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "    if 'pr_auc' in metrics:\n",
    "        print(f\"Precision-Recall AUC: {metrics['pr_auc']:.4f}\")\n",
    "    if 'log_loss' in metrics:\n",
    "        print(f\"Log Loss: {metrics['log_loss']:.4f}\")\n",
    "    if 'brier' in metrics:\n",
    "        print(f\"Brier Score: {metrics['brier']:.4f}\")\n",
    "    if 'recall@1' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nRETRIEVAL METRICS\\n{'='*80}\\nRecall@1: {metrics['recall@1']:.4f}\\nRecall@3: {metrics['recall@3']:.4f}\\nRecall@5: {metrics['recall@5']:.4f}\\nRecall@10: {metrics['recall@10']:.4f}\\nMean Reciprocal Rank (MRR): {metrics['mrr']:.4f}\")\n",
    "    if 'inference_time' in metrics:\n",
    "        print(f\"\\n{'='*80}\\nPERFORMANCE METRICS\\n{'='*80}\\nInference Time: {metrics['inference_time']:.2f}s\\nThroughput: {metrics['samples_per_sec']:.2f} samples/sec\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    cache_dir = '/hf_cache'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\", cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tspeft_config = TSPEFTConfig(rank=16, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5)\n",
    "    print(\"\\nLoading model with TS-PEFT and pipeline parallelism across GPUs...\")\n",
    "    model = TSPEFTCodeT5(\"Salesforce/codet5p-770m\", tspeft_config, num_classes=2, cache_dir=cache_dir)\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\\nTRAINING PHASE\\n{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = create_training_data(train_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(train_data)} training examples\\n\")\n",
    "    if len(train_data) == 0:\n",
    "        print(\"No training data available. Exiting.\")\n",
    "        return\n",
    "    epoch_stats = train_model(model, train_data, num_epochs=5, learning_rate=1e-3, batch_size=4)\n",
    "    print(f\"\\n{'='*80}\\nTESTING PHASE\\n{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = create_training_data(test_csv, tokenizer, label_col='label', func1_col='func1', func2_col='func2')\n",
    "    print(f\"Created {len(test_data)} test examples\\n\")\n",
    "    if len(test_data) > 0:\n",
    "        test_metrics = evaluate_model(model, test_data, batch_size=4)\n",
    "        print_results(test_metrics, \"TEST SET\")\n",
    "    print(f\"\\n{'='*80}\\nTRAINING COMPLETE\\n{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
