{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa080c25-b2d1-4b2d-ab58-0ee917115c65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GateRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4e563-f368-405b-85eb-cf7b4d663a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeSearchDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=256):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        comment = str(row['comment'])\n",
    "        code = str(row['code'])\n",
    "        \n",
    "        comment_encoding = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'comment_input_ids': comment_encoding['input_ids'].squeeze(0),\n",
    "            'comment_attention_mask': comment_encoding['attention_mask'].squeeze(0),\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(0),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class GatingModule(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.gate_linear = nn.Linear(input_dim, 1, bias=True)\n",
    "        nn.init.zeros_(self.gate_linear.weight)\n",
    "        nn.init.zeros_(self.gate_linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate_linear(x)\n",
    "        gate_values = torch.sigmoid(gate_logits)\n",
    "        return gate_values\n",
    "\n",
    "\n",
    "class GateRALayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(input_dim, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, output_dim))\n",
    "        \n",
    "        self.gating_module = GatingModule(input_dim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.base_layer(x)\n",
    "        \n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, hidden_dim = x.shape\n",
    "            x_2d = x.reshape(-1, hidden_dim)\n",
    "        else:\n",
    "            x_2d = x\n",
    "            batch_size = None\n",
    "            seq_len = None\n",
    "        \n",
    "        gate_values = self.gating_module(x_2d)\n",
    "        \n",
    "        lora_output = x_2d @ self.lora_A @ self.lora_B\n",
    "        lora_output = self.dropout_layer(lora_output)\n",
    "        \n",
    "        gated_lora_output = gate_values * lora_output * self.scaling\n",
    "        \n",
    "        if batch_size is not None and seq_len is not None:\n",
    "            gated_lora_output = gated_lora_output.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        final_output = base_output + gated_lora_output\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "\n",
    "class GateRACodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model_name, rank=16, alpha=16.0, dropout=0.0, entropy_reg_weight=0.01):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        self.d_kv = config.d_kv\n",
    "        self.num_heads = config.num_heads\n",
    "        self.entropy_reg_weight = entropy_reg_weight\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        self.gatera_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, block in enumerate(encoder.block):\n",
    "            q_proj = block.layer[0].SelfAttention.q\n",
    "            v_proj = block.layer[0].SelfAttention.v\n",
    "            \n",
    "            q_gatera = GateRALayer(\n",
    "                q_proj, rank=rank, alpha=alpha, dropout=dropout, \n",
    "                input_dim=self.d_model, output_dim=self.d_kv * self.num_heads\n",
    "            )\n",
    "            v_gatera = GateRALayer(\n",
    "                v_proj, rank=rank, alpha=alpha, dropout=dropout,\n",
    "                input_dim=self.d_model, output_dim=self.d_kv * self.num_heads\n",
    "            )\n",
    "            \n",
    "            self.gatera_layers[f'encoder_q_{i}'] = q_gatera\n",
    "            self.gatera_layers[f'encoder_v_{i}'] = v_gatera\n",
    "            \n",
    "            block.layer[0].SelfAttention.q = q_gatera\n",
    "            block.layer[0].SelfAttention.v = v_gatera\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def compute_entropy_loss(self, gate_values):\n",
    "        eps = 1e-8\n",
    "        gate_values = torch.clamp(gate_values, eps, 1.0 - eps)\n",
    "        entropy = -gate_values * torch.log(gate_values) - (1 - gate_values) * torch.log(1 - gate_values)\n",
    "        return entropy.mean()\n",
    "    \n",
    "    def encode_with_gatera(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        projected = self.projection(pooled)\n",
    "        \n",
    "        return projected\n",
    "    \n",
    "    def forward(self, comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask):\n",
    "        comment_emb = self.encode_with_gatera(comment_input_ids, comment_attention_mask)\n",
    "        code_emb = self.encode_with_gatera(code_input_ids, code_attention_mask)\n",
    "        \n",
    "        comment_emb = F.normalize(comment_emb, p=2, dim=1)\n",
    "        code_emb = F.normalize(code_emb, p=2, dim=1)\n",
    "        \n",
    "        batch_size = comment_emb.shape[0]\n",
    "        \n",
    "        similarities = torch.matmul(comment_emb, code_emb.T)\n",
    "        \n",
    "        labels = torch.arange(batch_size).to(comment_emb.device)\n",
    "        \n",
    "        temperature = 0.07\n",
    "        similarities = similarities / temperature\n",
    "        \n",
    "        contrastive_loss = F.cross_entropy(similarities, labels)\n",
    "        \n",
    "        if self.training and self.entropy_reg_weight > 0:\n",
    "            total_entropy_loss = 0.0\n",
    "            gate_count = 0\n",
    "            \n",
    "            for name, layer in self.gatera_layers.items():\n",
    "                if hasattr(layer, 'gating_module'):\n",
    "                    try:\n",
    "                        dummy_input = torch.randn(\n",
    "                            256,\n",
    "                            layer.lora_A.shape[0],\n",
    "                            device=comment_input_ids.device\n",
    "                        )\n",
    "                        gate_vals = layer.gating_module(dummy_input)\n",
    "                        entropy_loss = self.compute_entropy_loss(gate_vals)\n",
    "                        total_entropy_loss += entropy_loss\n",
    "                        gate_count += 1\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            \n",
    "            if gate_count > 0:\n",
    "                avg_entropy_loss = total_entropy_loss / gate_count\n",
    "                total_loss = contrastive_loss + self.entropy_reg_weight * avg_entropy_loss\n",
    "            else:\n",
    "                total_loss = contrastive_loss\n",
    "        else:\n",
    "            total_loss = contrastive_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "        comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_code_search(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_code_embeddings = []\n",
    "    all_comment_embeddings = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "            comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "            \n",
    "            code_emb = model.encode_with_gatera(code_input_ids, code_attention_mask)\n",
    "            comment_emb = model.encode_with_gatera(comment_input_ids, comment_attention_mask)\n",
    "            \n",
    "            all_code_embeddings.append(code_emb)\n",
    "            all_comment_embeddings.append(comment_emb)\n",
    "        \n",
    "        all_code_embeddings = torch.cat(all_code_embeddings, dim=0)\n",
    "        all_comment_embeddings = torch.cat(all_comment_embeddings, dim=0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    num_samples = all_code_embeddings.shape[0]\n",
    "    total_tokens = num_samples * 256 * 2\n",
    "    tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    return all_comment_embeddings, all_code_embeddings, tokens_per_second\n",
    "\n",
    "\n",
    "def compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10]):\n",
    "    num_queries = comment_embeddings.shape[0]\n",
    "    \n",
    "    comment_embeddings = F.normalize(comment_embeddings, p=2, dim=1)\n",
    "    code_embeddings = F.normalize(code_embeddings, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(comment_embeddings, code_embeddings.T)\n",
    "    \n",
    "    recall_at_k = {k: 0.0 for k in top_k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sim_scores = similarities[i]\n",
    "        \n",
    "        sorted_indices = torch.argsort(sim_scores, descending=True)\n",
    "        \n",
    "        true_index = i\n",
    "        rank_position = (sorted_indices == true_index).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(rank_position) > 0:\n",
    "            rank = rank_position[0].item() + 1\n",
    "        else:\n",
    "            rank = num_queries + 1\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        recall_at_k[k] = recall_at_k[k] / num_queries\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    \n",
    "    metrics = {\n",
    "        'MRR': mrr,\n",
    "        'Recall@1': recall_at_k[1],\n",
    "        'Recall@5': recall_at_k[5],\n",
    "        'Recall@10': recall_at_k[10]\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_code_search_metrics(metrics, tokens_per_second, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} CODE SEARCH METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Ranking Metrics:\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank):  {metrics['MRR']:.4f}\")\n",
    "    print(f\"  Recall@1:                    {metrics['Recall@1']:.4f}\")\n",
    "    print(f\"  Recall@5:                    {metrics['Recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:                   {metrics['Recall@10']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nGeneration Speed:\")\n",
    "    print(f\"  Tokens per Second:           {tokens_per_second:.2f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeSearchDataset('/train (1).csv', tokenizer)\n",
    "    test_dataset = CodeSearchDataset('/test (3).csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing GateRA Model for Code Search...\")\n",
    "    model = GateRACodeSearchModel(model_name, rank=16, alpha=16.0, dropout=0.0, entropy_reg_weight=0.01).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING GateRA FOR CODE SEARCH\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_mrr = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "            model, test_loader, device\n",
    "        )\n",
    "        \n",
    "        metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val MRR:    {metrics['MRR']:.4f} | Recall@1: {metrics['Recall@1']:.4f} | Recall@5: {metrics['Recall@5']:.4f} | Recall@10: {metrics['Recall@10']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['MRR'] > best_mrr:\n",
    "            best_mrr = metrics['MRR']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "        model, test_loader, device\n",
    "    )\n",
    "    \n",
    "    final_metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "    \n",
    "    print_code_search_metrics(final_metrics, tokens_per_second, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GateRA CODE SEARCH COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271163d-bc1d-4858-a1ae-8669a8047949",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TS EPFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce79990-5d32-45d9-af2c-54a5bee0e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeSearchDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=256):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        comment = str(row['comment'])\n",
    "        code = str(row['code'])\n",
    "        \n",
    "        comment_encoding = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'comment_input_ids': comment_encoding['input_ids'].squeeze(0),\n",
    "            'comment_attention_mask': comment_encoding['attention_mask'].squeeze(0),\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(0),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4, alpha=1.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return result * self.scaling\n",
    "\n",
    "\n",
    "class TSPEFTLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank=4, alpha=1.0, dropout=0.0, s=4e-5, lambda_reg=1e-5, beta1=0.9, beta2=0.98, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.base_layer.requires_grad_(False)\n",
    "        \n",
    "        self.lora = LoRALayer(\n",
    "            base_layer.in_features,\n",
    "            base_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        self.s = s\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.register_buffer('tau', torch.tensor(0.0))\n",
    "        self.register_buffer('m', torch.tensor(0.0))\n",
    "        self.register_buffer('v', torch.tensor(0.0))\n",
    "        self.register_buffer('step', torch.tensor(0))\n",
    "        \n",
    "    def compute_relative_magnitude(self, base_output, lora_output):\n",
    "        base_norm = torch.norm(base_output, p=2, dim=-1, keepdim=True)\n",
    "        lora_norm = torch.norm(lora_output, p=2, dim=-1, keepdim=True)\n",
    "        r_i = lora_norm / (base_norm + self.eps)\n",
    "        return r_i.squeeze(-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        base_output = self.base_layer(x)\n",
    "        lora_output = self.lora(x)\n",
    "        \n",
    "        if not self.training:\n",
    "            r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "            gate = (r_i >= self.tau).float().unsqueeze(-1)\n",
    "            return base_output + gate * lora_output\n",
    "        \n",
    "        r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "        gate = (r_i >= self.tau).float()\n",
    "        \n",
    "        gated_output = base_output + gate.unsqueeze(-1) * lora_output\n",
    "        \n",
    "        self._cache_for_backward = {\n",
    "            'r_i': r_i,\n",
    "            'gate': gate,\n",
    "            'lora_output': lora_output,\n",
    "            'base_output': base_output,\n",
    "        }\n",
    "        \n",
    "        return gated_output\n",
    "    \n",
    "    def compute_threshold_gradient(self, grad_output):\n",
    "        if not hasattr(self, '_cache_for_backward'):\n",
    "            return 0.0\n",
    "            \n",
    "        cache = self._cache_for_backward\n",
    "        r_i = cache['r_i']\n",
    "        gate = cache['gate']\n",
    "        lora_output = cache['lora_output']\n",
    "        \n",
    "        mu_i = (grad_output * lora_output).sum(dim=-1)\n",
    "        \n",
    "        consistency_mask = ((mu_i >= 0).float() == gate).float()\n",
    "        sparsity_mask = gate\n",
    "        \n",
    "        grad_loss = -self.s * (consistency_mask * mu_i).sum()\n",
    "        grad_sparsity = -self.s * (sparsity_mask * self.lambda_reg).sum()\n",
    "        \n",
    "        g_k = grad_loss + grad_sparsity\n",
    "        \n",
    "        return g_k.item()\n",
    "    \n",
    "    def update_threshold(self, grad_output, lr=1.0):\n",
    "        if not self.training:\n",
    "            return\n",
    "            \n",
    "        g_k = self.compute_threshold_gradient(grad_output)\n",
    "        \n",
    "        self.step += 1\n",
    "        \n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g_k\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g_k ** 2)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1 ** self.step.item())\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.step.item())\n",
    "        \n",
    "        tau_update = lr * self.s * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "        self.tau = torch.clamp(self.tau + tau_update, min=0.0)\n",
    "        \n",
    "        if hasattr(self, '_cache_for_backward'):\n",
    "            delattr(self, '_cache_for_backward')\n",
    "\n",
    "\n",
    "class TSPEFTCodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model_name, rank=32, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        self.d_kv = config.d_kv\n",
    "        self.num_heads = config.num_heads\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        self.ts_peft_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, block in enumerate(encoder.block):\n",
    "            q_proj = block.layer[0].SelfAttention.q\n",
    "            v_proj = block.layer[0].SelfAttention.v\n",
    "            \n",
    "            self.ts_peft_layers[f'encoder_q_{i}'] = TSPEFTLayer(\n",
    "                q_proj, rank=rank, alpha=alpha, dropout=dropout, s=s, lambda_reg=lambda_reg\n",
    "            )\n",
    "            self.ts_peft_layers[f'encoder_v_{i}'] = TSPEFTLayer(\n",
    "                v_proj, rank=rank, alpha=alpha, dropout=dropout, s=s, lambda_reg=lambda_reg\n",
    "            )\n",
    "            \n",
    "            block.layer[0].SelfAttention.q = self.ts_peft_layers[f'encoder_q_{i}']\n",
    "            block.layer[0].SelfAttention.v = self.ts_peft_layers[f'encoder_v_{i}']\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def encode_with_ts_peft(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        projected = self.projection(pooled)\n",
    "        \n",
    "        return projected\n",
    "    \n",
    "    def update_thresholds(self, lr=1.0):\n",
    "        for layer in self.ts_peft_layers.values():\n",
    "            if hasattr(layer, '_cache_for_backward') and layer.training:\n",
    "                grad_output = torch.ones_like(layer._cache_for_backward['base_output'])\n",
    "                layer.update_threshold(grad_output, lr)\n",
    "    \n",
    "    def forward(self, comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask):\n",
    "        comment_emb = self.encode_with_ts_peft(comment_input_ids, comment_attention_mask)\n",
    "        code_emb = self.encode_with_ts_peft(code_input_ids, code_attention_mask)\n",
    "        \n",
    "        comment_emb = F.normalize(comment_emb, p=2, dim=1)\n",
    "        code_emb = F.normalize(code_emb, p=2, dim=1)\n",
    "        \n",
    "        batch_size = comment_emb.shape[0]\n",
    "        \n",
    "        similarities = torch.matmul(comment_emb, code_emb.T)\n",
    "        \n",
    "        labels = torch.arange(batch_size).to(comment_emb.device)\n",
    "        \n",
    "        temperature = 0.07\n",
    "        similarities = similarities / temperature\n",
    "        \n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "        comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        model.update_thresholds(lr=1.0)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_code_search(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_code_embeddings = []\n",
    "    all_comment_embeddings = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "            comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "            \n",
    "            code_emb = model.encode_with_ts_peft(code_input_ids, code_attention_mask)\n",
    "            comment_emb = model.encode_with_ts_peft(comment_input_ids, comment_attention_mask)\n",
    "            \n",
    "            all_code_embeddings.append(code_emb)\n",
    "            all_comment_embeddings.append(comment_emb)\n",
    "        \n",
    "        all_code_embeddings = torch.cat(all_code_embeddings, dim=0)\n",
    "        all_comment_embeddings = torch.cat(all_comment_embeddings, dim=0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    num_samples = all_code_embeddings.shape[0]\n",
    "    total_tokens = num_samples * 256 * 2\n",
    "    tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    return all_comment_embeddings, all_code_embeddings, tokens_per_second\n",
    "\n",
    "\n",
    "def compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10]):\n",
    "    num_queries = comment_embeddings.shape[0]\n",
    "    \n",
    "    comment_embeddings = F.normalize(comment_embeddings, p=2, dim=1)\n",
    "    code_embeddings = F.normalize(code_embeddings, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(comment_embeddings, code_embeddings.T)\n",
    "    \n",
    "    recall_at_k = {k: 0.0 for k in top_k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sim_scores = similarities[i]\n",
    "        \n",
    "        sorted_indices = torch.argsort(sim_scores, descending=True)\n",
    "        \n",
    "        true_index = i\n",
    "        rank_position = (sorted_indices == true_index).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(rank_position) > 0:\n",
    "            rank = rank_position[0].item() + 1\n",
    "        else:\n",
    "            rank = num_queries + 1\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        recall_at_k[k] = recall_at_k[k] / num_queries\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    \n",
    "    metrics = {\n",
    "        'MRR': mrr,\n",
    "        'Recall@1': recall_at_k[1],\n",
    "        'Recall@5': recall_at_k[5],\n",
    "        'Recall@10': recall_at_k[10]\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_code_search_metrics(metrics, tokens_per_second, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} CODE SEARCH METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Ranking Metrics:\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank):  {metrics['MRR']:.4f}\")\n",
    "    print(f\"  Recall@1:                    {metrics['Recall@1']:.4f}\")\n",
    "    print(f\"  Recall@5:                    {metrics['Recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:                   {metrics['Recall@10']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nGeneration Speed:\")\n",
    "    print(f\"  Tokens per Second:           {tokens_per_second:.2f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeSearchDataset('/train (1).csv', tokenizer)\n",
    "    test_dataset = CodeSearchDataset('/test (3).csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing TS-PEFT Model for Code Search...\")\n",
    "    model = TSPEFTCodeSearchModel(model_name, rank=32, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING TS-PEFT FOR CODE SEARCH\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_mrr = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "            model, test_loader, device\n",
    "        )\n",
    "        \n",
    "        metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val MRR:    {metrics['MRR']:.4f} | Recall@1: {metrics['Recall@1']:.4f} | Recall@5: {metrics['Recall@5']:.4f} | Recall@10: {metrics['Recall@10']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['MRR'] > best_mrr:\n",
    "            best_mrr = metrics['MRR']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "        model, test_loader, device\n",
    "    )\n",
    "    \n",
    "    final_metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "    \n",
    "    print_code_search_metrics(final_metrics, tokens_per_second, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TS-PEFT CODE SEARCH COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022d26d-2084-4679-9e8d-8ca90a56be4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ADpater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8896369-b686-4721-8ade-af95822f424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeSearchDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=256):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        comment = str(row['comment'])\n",
    "        code = str(row['code'])\n",
    "        \n",
    "        comment_encoding = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'comment_input_ids': comment_encoding['input_ids'].squeeze(0),\n",
    "            'comment_attention_mask': comment_encoding['attention_mask'].squeeze(0),\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(0),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, input_size, bottleneck_size=64):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(input_size, bottleneck_size)\n",
    "        self.up_project = nn.Linear(bottleneck_size, input_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.down_project(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.up_project(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class AdapterCodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model_name, bottleneck_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        self.adapters = nn.ModuleList()\n",
    "        for i in range(config.num_layers):\n",
    "            self.adapters.append(Adapter(self.d_model, bottleneck_size))\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.hidden_states\n",
    "        \n",
    "        for i, adapter in enumerate(self.adapters):\n",
    "            if i + 1 < len(hidden_states):\n",
    "                hidden_states_list = list(hidden_states)\n",
    "                hidden_states_list[i + 1] = adapter(hidden_states_list[i + 1])\n",
    "                hidden_states = tuple(hidden_states_list)\n",
    "        \n",
    "        final_hidden = hidden_states[-1]\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(final_hidden.size()).float()\n",
    "        sum_hidden = torch.sum(final_hidden * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        projected = self.projection(pooled)\n",
    "        \n",
    "        return projected\n",
    "    \n",
    "    def forward(self, comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask):\n",
    "        comment_emb = self.encode(comment_input_ids, comment_attention_mask)\n",
    "        code_emb = self.encode(code_input_ids, code_attention_mask)\n",
    "        \n",
    "        comment_emb = F.normalize(comment_emb, p=2, dim=1)\n",
    "        code_emb = F.normalize(code_emb, p=2, dim=1)\n",
    "        \n",
    "        batch_size = comment_emb.shape[0]\n",
    "        \n",
    "        similarities = torch.matmul(comment_emb, code_emb.T)\n",
    "        \n",
    "        labels = torch.arange(batch_size).to(comment_emb.device)\n",
    "        \n",
    "        temperature = 0.07\n",
    "        similarities = similarities / temperature\n",
    "        \n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "        comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_code_search(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_code_embeddings = []\n",
    "    all_comment_embeddings = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "            comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "            \n",
    "            code_emb = model.encode(code_input_ids, code_attention_mask)\n",
    "            comment_emb = model.encode(comment_input_ids, comment_attention_mask)\n",
    "            \n",
    "            all_code_embeddings.append(code_emb)\n",
    "            all_comment_embeddings.append(comment_emb)\n",
    "        \n",
    "        all_code_embeddings = torch.cat(all_code_embeddings, dim=0)\n",
    "        all_comment_embeddings = torch.cat(all_comment_embeddings, dim=0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    num_samples = all_code_embeddings.shape[0]\n",
    "    total_tokens = num_samples * 256 * 2\n",
    "    tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    return all_comment_embeddings, all_code_embeddings, tokens_per_second\n",
    "\n",
    "\n",
    "def compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10]):\n",
    "    num_queries = comment_embeddings.shape[0]\n",
    "    \n",
    "    comment_embeddings = F.normalize(comment_embeddings, p=2, dim=1)\n",
    "    code_embeddings = F.normalize(code_embeddings, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(comment_embeddings, code_embeddings.T)\n",
    "    \n",
    "    recall_at_k = {k: 0.0 for k in top_k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sim_scores = similarities[i]\n",
    "        \n",
    "        sorted_indices = torch.argsort(sim_scores, descending=True)\n",
    "        \n",
    "        true_index = i\n",
    "        rank_position = (sorted_indices == true_index).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(rank_position) > 0:\n",
    "            rank = rank_position[0].item() + 1\n",
    "        else:\n",
    "            rank = num_queries + 1\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        recall_at_k[k] = recall_at_k[k] / num_queries\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    \n",
    "    metrics = {\n",
    "        'MRR': mrr,\n",
    "        'Recall@1': recall_at_k[1],\n",
    "        'Recall@5': recall_at_k[5],\n",
    "        'Recall@10': recall_at_k[10]\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_code_search_metrics(metrics, tokens_per_second, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} CODE SEARCH METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Ranking Metrics:\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank):  {metrics['MRR']:.4f}\")\n",
    "    print(f\"  Recall@1:                    {metrics['Recall@1']:.4f}\")\n",
    "    print(f\"  Recall@5:                    {metrics['Recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:                   {metrics['Recall@10']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nGeneration Speed:\")\n",
    "    print(f\"  Tokens per Second:           {tokens_per_second:.2f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeSearchDataset('/train (1).csv', tokenizer)\n",
    "    test_dataset = CodeSearchDataset('//test (3).csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing Adapter Model for Code Search...\")\n",
    "    model = AdapterCodeSearchModel(model_name, bottleneck_size=64).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING ADAPTER FOR CODE SEARCH\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_mrr = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "            model, test_loader, device\n",
    "        )\n",
    "        \n",
    "        metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val MRR:    {metrics['MRR']:.4f} | Recall@1: {metrics['Recall@1']:.4f} | Recall@5: {metrics['Recall@5']:.4f} | Recall@10: {metrics['Recall@10']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['MRR'] > best_mrr:\n",
    "            best_mrr = metrics['MRR']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "        model, test_loader, device\n",
    "    )\n",
    "    \n",
    "    final_metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "    \n",
    "    print_code_search_metrics(final_metrics, tokens_per_second, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ADAPTER CODE SEARCH COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3000bc0-1fbf-47e5-a215-113835574574",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8715c0a-c6b4-4901-9ff7-471ebdeabb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeSearchDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=256):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        comment = str(row['comment'])\n",
    "        code = str(row['code'])\n",
    "        \n",
    "        comment_encoding = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'comment_input_ids': comment_encoding['input_ids'].squeeze(0),\n",
    "            'comment_attention_mask': comment_encoding['attention_mask'].squeeze(0),\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(0),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=8, lora_alpha=16, lora_dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.scaling = lora_alpha / r\n",
    "        \n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, r) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(r, out_features))\n",
    "        \n",
    "    def forward(self, x, original_output):\n",
    "        lora_out = self.lora_dropout(x) @ self.lora_A @ self.lora_B\n",
    "        return original_output + lora_out * self.scaling\n",
    "\n",
    "\n",
    "class LoRACodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model_name, r=8, lora_alpha=16, lora_dropout=0.05):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        self.d_kv = config.d_kv\n",
    "        self.num_heads = config.num_heads\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, block in enumerate(encoder.block):\n",
    "            q_proj = block.layer[0].SelfAttention.q\n",
    "            v_proj = block.layer[0].SelfAttention.v\n",
    "            \n",
    "            self.lora_layers[f'encoder_q_{i}'] = LoRALayer(\n",
    "                self.d_model, self.d_kv * self.num_heads, r, lora_alpha, lora_dropout\n",
    "            )\n",
    "            self.lora_layers[f'encoder_v_{i}'] = LoRALayer(\n",
    "                self.d_model, self.d_kv * self.num_heads, r, lora_alpha, lora_dropout\n",
    "            )\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def encode_with_lora(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        batch_size = input_ids.shape[0]\n",
    "        device = input_ids.device\n",
    "        \n",
    "        hidden_states = encoder.embed_tokens(input_ids)\n",
    "        \n",
    "        for i, block in enumerate(encoder.block):\n",
    "            attention = block.layer[0].SelfAttention\n",
    "            \n",
    "            original_q = attention.q(hidden_states)\n",
    "            original_v = attention.v(hidden_states)\n",
    "            \n",
    "            q = self.lora_layers[f'encoder_q_{i}'](hidden_states, original_q)\n",
    "            k = attention.k(hidden_states)\n",
    "            v = self.lora_layers[f'encoder_v_{i}'](hidden_states, original_v)\n",
    "            \n",
    "            batch_size, seq_length, _ = hidden_states.shape\n",
    "            \n",
    "            q = q.view(batch_size, seq_length, self.num_heads, self.d_kv).transpose(1, 2)\n",
    "            k = k.view(batch_size, seq_length, self.num_heads, self.d_kv).transpose(1, 2)\n",
    "            v = v.view(batch_size, seq_length, self.num_heads, self.d_kv).transpose(1, 2)\n",
    "            \n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_kv ** 0.5)\n",
    "            \n",
    "            attention_mask_expanded = attention_mask[:, None, None, :].to(dtype=scores.dtype)\n",
    "            attention_mask_expanded = (1.0 - attention_mask_expanded) * -10000.0\n",
    "            scores = scores + attention_mask_expanded\n",
    "            \n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            attn_output = torch.matmul(attn_weights, v)\n",
    "            \n",
    "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "            attn_output = attention.o(attn_output)\n",
    "            \n",
    "            hidden_states = hidden_states + block.layer[0].layer_norm(attn_output)\n",
    "            \n",
    "            ff_output = block.layer[1](hidden_states)\n",
    "            hidden_states = hidden_states + ff_output\n",
    "        \n",
    "        hidden_states = encoder.final_layer_norm(hidden_states)\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        projected = self.projection(pooled)\n",
    "        \n",
    "        return projected\n",
    "    \n",
    "    def forward(self, comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask):\n",
    "        comment_emb = self.encode_with_lora(comment_input_ids, comment_attention_mask)\n",
    "        code_emb = self.encode_with_lora(code_input_ids, code_attention_mask)\n",
    "        \n",
    "        comment_emb = F.normalize(comment_emb, p=2, dim=1)\n",
    "        code_emb = F.normalize(code_emb, p=2, dim=1)\n",
    "        \n",
    "        batch_size = comment_emb.shape[0]\n",
    "        \n",
    "        similarities = torch.matmul(comment_emb, code_emb.T)\n",
    "        \n",
    "        labels = torch.arange(batch_size).to(comment_emb.device)\n",
    "        \n",
    "        temperature = 0.07\n",
    "        similarities = similarities / temperature\n",
    "        \n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "        comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_code_search(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_code_embeddings = []\n",
    "    all_comment_embeddings = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "            comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "            \n",
    "            code_emb = model.encode_with_lora(code_input_ids, code_attention_mask)\n",
    "            comment_emb = model.encode_with_lora(comment_input_ids, comment_attention_mask)\n",
    "            \n",
    "            all_code_embeddings.append(code_emb)\n",
    "            all_comment_embeddings.append(comment_emb)\n",
    "        \n",
    "        all_code_embeddings = torch.cat(all_code_embeddings, dim=0)\n",
    "        all_comment_embeddings = torch.cat(all_comment_embeddings, dim=0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    num_samples = all_code_embeddings.shape[0]\n",
    "    total_tokens = num_samples * 256 * 2\n",
    "    tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    return all_comment_embeddings, all_code_embeddings, tokens_per_second\n",
    "\n",
    "\n",
    "def compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10]):\n",
    "    num_queries = comment_embeddings.shape[0]\n",
    "    \n",
    "    comment_embeddings = F.normalize(comment_embeddings, p=2, dim=1)\n",
    "    code_embeddings = F.normalize(code_embeddings, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(comment_embeddings, code_embeddings.T)\n",
    "    \n",
    "    recall_at_k = {k: 0.0 for k in top_k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sim_scores = similarities[i]\n",
    "        \n",
    "        sorted_indices = torch.argsort(sim_scores, descending=True)\n",
    "        \n",
    "        true_index = i\n",
    "        rank_position = (sorted_indices == true_index).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(rank_position) > 0:\n",
    "            rank = rank_position[0].item() + 1\n",
    "        else:\n",
    "            rank = num_queries + 1\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        recall_at_k[k] = recall_at_k[k] / num_queries\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    \n",
    "    metrics = {\n",
    "        'MRR': mrr,\n",
    "        'Recall@1': recall_at_k[1],\n",
    "        'Recall@5': recall_at_k[5],\n",
    "        'Recall@10': recall_at_k[10]\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_code_search_metrics(metrics, tokens_per_second, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} CODE SEARCH METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Ranking Metrics:\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank):  {metrics['MRR']:.4f}\")\n",
    "    print(f\"  Recall@1:                    {metrics['Recall@1']:.4f}\")\n",
    "    print(f\"  Recall@5:                    {metrics['Recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:                   {metrics['Recall@10']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nGeneration Speed:\")\n",
    "    print(f\"  Tokens per Second:           {tokens_per_second:.2f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeSearchDataset('/train (1).csv', tokenizer)\n",
    "    test_dataset = CodeSearchDataset('/test (3).csv', tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing LoRA Model for Code Search...\")\n",
    "    model = LoRACodeSearchModel(model_name, r=8, lora_alpha=16, lora_dropout=0.05).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING LORA FOR CODE SEARCH\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_mrr = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "            model, test_loader, device\n",
    "        )\n",
    "        \n",
    "        metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val MRR:    {metrics['MRR']:.4f} | Recall@1: {metrics['Recall@1']:.4f} | Recall@5: {metrics['Recall@5']:.4f} | Recall@10: {metrics['Recall@10']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['MRR'] > best_mrr:\n",
    "            best_mrr = metrics['MRR']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "        model, test_loader, device\n",
    "    )\n",
    "    \n",
    "    final_metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "    \n",
    "    print_code_search_metrics(final_metrics, tokens_per_second, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LORA CODE SEARCH COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366cee8-4592-4dd1-b281-bb114b61d060",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bitfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8729c-df5e-4711-897d-43fd29641c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeSearchDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=256):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        comment = str(row['comment'])\n",
    "        code = str(row['code'])\n",
    "        \n",
    "        comment_encoding = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'comment_input_ids': comment_encoding['input_ids'].squeeze(0),\n",
    "            'comment_attention_mask': comment_encoding['attention_mask'].squeeze(0),\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(0),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class BitFitCodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        projected = self.projection(pooled)\n",
    "        \n",
    "        return projected\n",
    "    \n",
    "    def forward(self, comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask):\n",
    "        comment_emb = self.encode(comment_input_ids, comment_attention_mask)\n",
    "        code_emb = self.encode(code_input_ids, code_attention_mask)\n",
    "        \n",
    "        comment_emb = F.normalize(comment_emb, p=2, dim=1)\n",
    "        code_emb = F.normalize(code_emb, p=2, dim=1)\n",
    "        \n",
    "        batch_size = comment_emb.shape[0]\n",
    "        \n",
    "        similarities = torch.matmul(comment_emb, code_emb.T)\n",
    "        \n",
    "        labels = torch.arange(batch_size).to(comment_emb.device)\n",
    "        \n",
    "        temperature = 0.07\n",
    "        similarities = similarities / temperature\n",
    "        \n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "        comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_code_search(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_code_embeddings = []\n",
    "    all_comment_embeddings = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "            comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "            \n",
    "            code_emb = model.encode(code_input_ids, code_attention_mask)\n",
    "            comment_emb = model.encode(comment_input_ids, comment_attention_mask)\n",
    "            \n",
    "            all_code_embeddings.append(code_emb)\n",
    "            all_comment_embeddings.append(comment_emb)\n",
    "        \n",
    "        all_code_embeddings = torch.cat(all_code_embeddings, dim=0)\n",
    "        all_comment_embeddings = torch.cat(all_comment_embeddings, dim=0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    num_samples = all_code_embeddings.shape[0]\n",
    "    total_tokens = num_samples * 256 * 2\n",
    "    tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    return all_comment_embeddings, all_code_embeddings, tokens_per_second\n",
    "\n",
    "\n",
    "def compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10]):\n",
    "    num_queries = comment_embeddings.shape[0]\n",
    "    \n",
    "    comment_embeddings = F.normalize(comment_embeddings, p=2, dim=1)\n",
    "    code_embeddings = F.normalize(code_embeddings, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(comment_embeddings, code_embeddings.T)\n",
    "    \n",
    "    recall_at_k = {k: 0.0 for k in top_k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sim_scores = similarities[i]\n",
    "        \n",
    "        sorted_indices = torch.argsort(sim_scores, descending=True)\n",
    "        \n",
    "        true_index = i\n",
    "        rank_position = (sorted_indices == true_index).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(rank_position) > 0:\n",
    "            rank = rank_position[0].item() + 1\n",
    "        else:\n",
    "            rank = num_queries + 1\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        recall_at_k[k] = recall_at_k[k] / num_queries\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    \n",
    "    metrics = {\n",
    "        'MRR': mrr,\n",
    "        'Recall@1': recall_at_k[1],\n",
    "        'Recall@5': recall_at_k[5],\n",
    "        'Recall@10': recall_at_k[10]\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_code_search_metrics(metrics, tokens_per_second, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} CODE SEARCH METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Ranking Metrics:\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank):  {metrics['MRR']:.4f}\")\n",
    "    print(f\"  Recall@1:                    {metrics['Recall@1']:.4f}\")\n",
    "    print(f\"  Recall@5:                    {metrics['Recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:                   {metrics['Recall@10']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nGeneration Speed:\")\n",
    "    print(f\"  Tokens per Second:           {tokens_per_second:.2f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeSearchDataset('/train (1).csv', tokenizer)\n",
    "    test_dataset = CodeSearchDataset('/test (3).csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing BitFit Model for Code Search...\")\n",
    "    model = BitFitCodeSearchModel(model_name).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING BITFIT FOR CODE SEARCH\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_mrr = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "            model, test_loader, device\n",
    "        )\n",
    "        \n",
    "        metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val MRR:    {metrics['MRR']:.4f} | Recall@1: {metrics['Recall@1']:.4f} | Recall@5: {metrics['Recall@5']:.4f} | Recall@10: {metrics['Recall@10']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['MRR'] > best_mrr:\n",
    "            best_mrr = metrics['MRR']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "        model, test_loader, device\n",
    "    )\n",
    "    \n",
    "    final_metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "    \n",
    "    print_code_search_metrics(final_metrics, tokens_per_second, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BITFIT CODE SEARCH COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9dee39-e9d7-4ac5-8184-75d115ac21c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c2bb3-d7bd-4f95-ad5c-a5ed0858fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeSearchDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=256):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        comment = str(row['comment'])\n",
    "        code = str(row['code'])\n",
    "        \n",
    "        comment_encoding = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'comment_input_ids': comment_encoding['input_ids'].squeeze(0),\n",
    "            'comment_attention_mask': comment_encoding['attention_mask'].squeeze(0),\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(0),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class PrefixEncoder(nn.Module):\n",
    "    def __init__(self, prefix_length, num_layers, num_heads, head_dim, prefix_hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(prefix_length, prefix_hidden_size)\n",
    "        \n",
    "        self.trans = nn.Sequential(\n",
    "            nn.Linear(prefix_hidden_size, prefix_hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(prefix_hidden_size, num_layers * 2 * num_heads * head_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_size):\n",
    "        prefix_tokens = torch.arange(self.prefix_length).to(self.embedding.weight.device)\n",
    "        prefix_tokens = prefix_tokens.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        past_key_values = self.trans(self.embedding(prefix_tokens))\n",
    "        \n",
    "        past_key_values = past_key_values.view(\n",
    "            batch_size,\n",
    "            self.prefix_length,\n",
    "            self.num_layers * 2,\n",
    "            self.num_heads,\n",
    "            self.head_dim\n",
    "        )\n",
    "        \n",
    "        past_key_values = past_key_values.permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        past_key_values_list = past_key_values.split(2)\n",
    "        \n",
    "        return past_key_values_list\n",
    "\n",
    "\n",
    "class PrefixCodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model_name, prefix_length=10, prefix_hidden_size=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        self.num_encoder_layers = config.num_layers\n",
    "        self.num_decoder_layers = config.num_decoder_layers\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.d_kv\n",
    "        \n",
    "        self.prefix_length = prefix_length\n",
    "        \n",
    "        self.encoder_prefix = PrefixEncoder(\n",
    "            prefix_length=prefix_length,\n",
    "            num_layers=self.num_encoder_layers,\n",
    "            num_heads=self.num_heads,\n",
    "            head_dim=self.head_dim,\n",
    "            prefix_hidden_size=prefix_hidden_size\n",
    "        )\n",
    "        \n",
    "        self.encoder_prefix_embeds = nn.Parameter(torch.randn(1, prefix_length, self.d_model) * 0.02)\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def encode_with_prefix(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        device = input_ids.device\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        embeddings = encoder.embed_tokens(input_ids)\n",
    "        \n",
    "        prefix_embeds = self.encoder_prefix_embeds.expand(batch_size, -1, -1)\n",
    "        \n",
    "        inputs_embeds = torch.cat([prefix_embeds, embeddings], dim=1)\n",
    "        \n",
    "        prefix_attention_mask = torch.ones(batch_size, self.prefix_length).to(device)\n",
    "        \n",
    "        extended_attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        \n",
    "        outputs = encoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state[:, self.prefix_length:, :]\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        projected = self.projection(pooled)\n",
    "        \n",
    "        return projected\n",
    "    \n",
    "    def forward(self, comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask):\n",
    "        comment_emb = self.encode_with_prefix(comment_input_ids, comment_attention_mask)\n",
    "        code_emb = self.encode_with_prefix(code_input_ids, code_attention_mask)\n",
    "        \n",
    "        comment_emb = F.normalize(comment_emb, p=2, dim=1)\n",
    "        code_emb = F.normalize(code_emb, p=2, dim=1)\n",
    "        \n",
    "        batch_size = comment_emb.shape[0]\n",
    "        \n",
    "        similarities = torch.matmul(comment_emb, code_emb.T)\n",
    "        \n",
    "        labels = torch.arange(batch_size).to(comment_emb.device)\n",
    "        \n",
    "        temperature = 0.07\n",
    "        similarities = similarities / temperature\n",
    "        \n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "        comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(comment_input_ids, comment_attention_mask, code_input_ids, code_attention_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_code_search(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_code_embeddings = []\n",
    "    all_comment_embeddings = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            comment_input_ids = batch['comment_input_ids'].to(device)\n",
    "            comment_attention_mask = batch['comment_attention_mask'].to(device)\n",
    "            \n",
    "            code_emb = model.encode_with_prefix(code_input_ids, code_attention_mask)\n",
    "            comment_emb = model.encode_with_prefix(comment_input_ids, comment_attention_mask)\n",
    "            \n",
    "            all_code_embeddings.append(code_emb)\n",
    "            all_comment_embeddings.append(comment_emb)\n",
    "        \n",
    "        all_code_embeddings = torch.cat(all_code_embeddings, dim=0)\n",
    "        all_comment_embeddings = torch.cat(all_comment_embeddings, dim=0)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    num_samples = all_code_embeddings.shape[0]\n",
    "    total_tokens = num_samples * 256 * 2\n",
    "    tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    return all_comment_embeddings, all_code_embeddings, tokens_per_second\n",
    "\n",
    "\n",
    "def compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10]):\n",
    "    num_queries = comment_embeddings.shape[0]\n",
    "    \n",
    "    comment_embeddings = F.normalize(comment_embeddings, p=2, dim=1)\n",
    "    code_embeddings = F.normalize(code_embeddings, p=2, dim=1)\n",
    "    \n",
    "    similarities = torch.matmul(comment_embeddings, code_embeddings.T)\n",
    "    \n",
    "    recall_at_k = {k: 0.0 for k in top_k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sim_scores = similarities[i]\n",
    "        \n",
    "        sorted_indices = torch.argsort(sim_scores, descending=True)\n",
    "        \n",
    "        true_index = i\n",
    "        rank_position = (sorted_indices == true_index).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(rank_position) > 0:\n",
    "            rank = rank_position[0].item() + 1\n",
    "        else:\n",
    "            rank = num_queries + 1\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        for k in top_k_values:\n",
    "            if rank <= k:\n",
    "                recall_at_k[k] += 1\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        recall_at_k[k] = recall_at_k[k] / num_queries\n",
    "    \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    \n",
    "    metrics = {\n",
    "        'MRR': mrr,\n",
    "        'Recall@1': recall_at_k[1],\n",
    "        'Recall@5': recall_at_k[5],\n",
    "        'Recall@10': recall_at_k[10]\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_code_search_metrics(metrics, tokens_per_second, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} CODE SEARCH METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Ranking Metrics:\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank):  {metrics['MRR']:.4f}\")\n",
    "    print(f\"  Recall@1:                    {metrics['Recall@1']:.4f}\")\n",
    "    print(f\"  Recall@5:                    {metrics['Recall@5']:.4f}\")\n",
    "    print(f\"  Recall@10:                   {metrics['Recall@10']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nGeneration Speed:\")\n",
    "    print(f\"  Tokens per Second:           {tokens_per_second:.2f}\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeSearchDataset('/train (1).csv', tokenizer)\n",
    "    test_dataset = CodeSearchDataset('/test (3).csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing Prefix Tuning Model for Code Search...\")\n",
    "    model = PrefixCodeSearchModel(model_name, prefix_length=10, prefix_hidden_size=512).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING PREFIX TUNING FOR CODE SEARCH\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_mrr = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "            model, test_loader, device\n",
    "        )\n",
    "        \n",
    "        metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val MRR:    {metrics['MRR']:.4f} | Recall@1: {metrics['Recall@1']:.4f} | Recall@5: {metrics['Recall@5']:.4f} | Recall@10: {metrics['Recall@10']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['MRR'] > best_mrr:\n",
    "            best_mrr = metrics['MRR']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    comment_embeddings, code_embeddings, tokens_per_second = evaluate_code_search(\n",
    "        model, test_loader, device\n",
    "    )\n",
    "    \n",
    "    final_metrics = compute_code_search_metrics_fast(comment_embeddings, code_embeddings, top_k_values=[1, 5, 10])\n",
    "    \n",
    "    print_code_search_metrics(final_metrics, tokens_per_second, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREFIX TUNING CODE SEARCH COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
