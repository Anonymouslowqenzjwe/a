{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65efc8b9-7800-4ae0-b928-501d1958c929",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TS PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f2e59-f740-43ea-8c75-4983a80170f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import math\n",
    "from typing import Optional, List, Dict\n",
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TSPEFTConfig:\n",
    "    rank: int = 32\n",
    "    alpha: float = 0.5\n",
    "    dropout: float = 0.05\n",
    "    s: float = 4e-5\n",
    "    lambda_reg: float = 4.5e-5\n",
    "    target_modules: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 4, alpha: float = 1.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features, dtype=torch.bfloat16))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank, dtype=torch.bfloat16))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        with torch.no_grad():\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_dropped = self.dropout(x)\n",
    "        result = (x_dropped @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "        return result\n",
    "\n",
    "\n",
    "class TSPEFTLayer(nn.Module):\n",
    "    def __init__(self, base_layer: nn.Linear, rank: int = 4, alpha: float = 1.0, dropout: float = 0.0, s: float = 4e-5, lambda_reg: float = 1e-5, beta1: float = 0.9, beta2: float = 0.98, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        for param in self.base_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.lora = LoRALayer(base_layer.in_features, base_layer.out_features, rank=rank, alpha=alpha, dropout=dropout)\n",
    "        self.s = s\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.tau = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.m = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.v = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.step = nn.Parameter(torch.tensor(0, dtype=torch.long), requires_grad=False)\n",
    "        \n",
    "    def compute_relative_magnitude(self, base_output: torch.Tensor, lora_output: torch.Tensor) -> torch.Tensor:\n",
    "        base_norm = torch.norm(base_output, p=2, dim=-1, keepdim=True)\n",
    "        lora_norm = torch.norm(lora_output, p=2, dim=-1, keepdim=True)\n",
    "        r_i = lora_norm / (base_norm + self.eps)\n",
    "        return r_i.squeeze(-1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base_output = self.base_layer(x)\n",
    "        lora_output = self.lora(x)\n",
    "        if not self.training:\n",
    "            r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "            gate = (r_i >= self.tau).float().unsqueeze(-1)\n",
    "            return base_output + gate * lora_output\n",
    "        r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "        gate = (r_i >= self.tau).float()\n",
    "        gated_output = base_output + gate.unsqueeze(-1) * lora_output\n",
    "        self._cache_for_backward = {'r_i': r_i, 'gate': gate, 'lora_output': lora_output, 'base_output': base_output}\n",
    "        return gated_output\n",
    "    \n",
    "    def compute_threshold_gradient(self, grad_output: torch.Tensor) -> float:\n",
    "        if not hasattr(self, '_cache_for_backward'):\n",
    "            return 0.0\n",
    "        cache = self._cache_for_backward\n",
    "        r_i = cache['r_i']\n",
    "        gate = cache['gate']\n",
    "        lora_output = cache['lora_output']\n",
    "        mu_i = (grad_output * lora_output).sum(dim=-1)\n",
    "        consistency_mask = ((mu_i >= 0).float() == gate).float()\n",
    "        sparsity_mask = gate\n",
    "        grad_loss = -self.s * (consistency_mask * mu_i).sum()\n",
    "        grad_sparsity = -self.s * (sparsity_mask * self.lambda_reg).sum()\n",
    "        g_k = grad_loss + grad_sparsity\n",
    "        return g_k.item()\n",
    "    \n",
    "    def update_threshold(self, grad_output: torch.Tensor, lr: float = 1.0):\n",
    "        if not self.training:\n",
    "            return\n",
    "        g_k = self.compute_threshold_gradient(grad_output)\n",
    "        self.step.data += 1\n",
    "        self.m.data = self.beta1 * self.m.data + (1 - self.beta1) * g_k\n",
    "        self.v.data = self.beta2 * self.v.data + (1 - self.beta2) * (g_k ** 2)\n",
    "        m_hat = self.m.data / (1 - self.beta1 ** self.step.item())\n",
    "        v_hat = self.v.data / (1 - self.beta2 ** self.step.item())\n",
    "        tau_update = lr * self.s * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "        self.tau.data = torch.clamp(self.tau.data + tau_update, min=0.0)\n",
    "        if hasattr(self, '_cache_for_backward'):\n",
    "            delattr(self, '_cache_for_backward')\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class TSPEFTMistral(nn.Module):\n",
    "    def __init__(self, model_name: str, tspeft_config: TSPEFTConfig, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nInitializing model across {num_gpus} GPUs\")\n",
    "        for i in range(num_gpus):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name} - {props.total_memory / 1024**3:.2f} GB\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(f\"\\nLoading base model: {model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, cache_dir=cache_dir, low_cpu_mem_usage=True, device_map='auto', trust_remote_code=True)\n",
    "        self.llm = base_model\n",
    "        self.tspeft_config = tspeft_config\n",
    "        self.tspeft_layers = nn.ModuleDict()\n",
    "        num_layers = len(base_model.model.layers)\n",
    "        print(f\"Model has {num_layers} transformer layers\")\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._inject_tspeft_layers()\n",
    "        print(\"Model initialization complete!\\n\")\n",
    "    \n",
    "    def _inject_tspeft_layers(self):\n",
    "        num_layers = len(self.llm.model.layers)\n",
    "        print(f\"Injecting TS-PEFT layers for {num_layers} transformer layers...\")\n",
    "        trainable_params = 0\n",
    "        for layer_idx in range(num_layers):\n",
    "            layer = self.llm.model.layers[layer_idx]\n",
    "            layer_device = next(layer.parameters()).device\n",
    "            if hasattr(layer.self_attn, 'q_proj'):\n",
    "                module = layer.self_attn.q_proj\n",
    "                tspeft_layer = TSPEFTLayer(base_layer=module, rank=self.tspeft_config.rank, alpha=self.tspeft_config.alpha, dropout=self.tspeft_config.dropout, s=self.tspeft_config.s, lambda_reg=self.tspeft_config.lambda_reg)\n",
    "                tspeft_layer = tspeft_layer.to(device=layer_device)\n",
    "                layer.self_attn.q_proj = tspeft_layer\n",
    "                self.tspeft_layers[f\"layer_{layer_idx}_q_proj\"] = tspeft_layer\n",
    "                trainable_params += sum(p.numel() for p in tspeft_layer.lora.parameters())\n",
    "            if hasattr(layer.self_attn, 'k_proj'):\n",
    "                module = layer.self_attn.k_proj\n",
    "                tspeft_layer = TSPEFTLayer(base_layer=module, rank=self.tspeft_config.rank, alpha=self.tspeft_config.alpha, dropout=self.tspeft_config.dropout, s=self.tspeft_config.s, lambda_reg=self.tspeft_config.lambda_reg)\n",
    "                tspeft_layer = tspeft_layer.to(device=layer_device)\n",
    "                layer.self_attn.k_proj = tspeft_layer\n",
    "                self.tspeft_layers[f\"layer_{layer_idx}_k_proj\"] = tspeft_layer\n",
    "                trainable_params += sum(p.numel() for p in tspeft_layer.lora.parameters())\n",
    "            if hasattr(layer.self_attn, 'v_proj'):\n",
    "                module = layer.self_attn.v_proj\n",
    "                tspeft_layer = TSPEFTLayer(base_layer=module, rank=self.tspeft_config.rank, alpha=self.tspeft_config.alpha, dropout=self.tspeft_config.dropout, s=self.tspeft_config.s, lambda_reg=self.tspeft_config.lambda_reg)\n",
    "                tspeft_layer = tspeft_layer.to(device=layer_device)\n",
    "                layer.self_attn.v_proj = tspeft_layer\n",
    "                self.tspeft_layers[f\"layer_{layer_idx}_v_proj\"] = tspeft_layer\n",
    "                trainable_params += sum(p.numel() for p in tspeft_layer.lora.parameters())\n",
    "            if hasattr(layer.self_attn, 'o_proj'):\n",
    "                module = layer.self_attn.o_proj\n",
    "                tspeft_layer = TSPEFTLayer(base_layer=module, rank=self.tspeft_config.rank, alpha=self.tspeft_config.alpha, dropout=self.tspeft_config.dropout, s=self.tspeft_config.s, lambda_reg=self.tspeft_config.lambda_reg)\n",
    "                tspeft_layer = tspeft_layer.to(device=layer_device)\n",
    "                layer.self_attn.o_proj = tspeft_layer\n",
    "                self.tspeft_layers[f\"layer_{layer_idx}_o_proj\"] = tspeft_layer\n",
    "                trainable_params += sum(p.numel() for p in tspeft_layer.lora.parameters())\n",
    "            if hasattr(layer, 'mlp'):\n",
    "                if hasattr(layer.mlp, 'gate_proj'):\n",
    "                    module = layer.mlp.gate_proj\n",
    "                    tspeft_layer = TSPEFTLayer(base_layer=module, rank=self.tspeft_config.rank, alpha=self.tspeft_config.alpha, dropout=self.tspeft_config.dropout, s=self.tspeft_config.s, lambda_reg=self.tspeft_config.lambda_reg)\n",
    "                    tspeft_layer = tspeft_layer.to(device=layer_device)\n",
    "                    layer.mlp.gate_proj = tspeft_layer\n",
    "                    self.tspeft_layers[f\"layer_{layer_idx}_gate_proj\"] = tspeft_layer\n",
    "                    trainable_params += sum(p.numel() for p in tspeft_layer.lora.parameters())\n",
    "                if hasattr(layer.mlp, 'up_proj'):\n",
    "                    module = layer.mlp.up_proj\n",
    "                    tspeft_layer = TSPEFTLayer(base_layer=module, rank=self.tspeft_config.rank, alpha=self.tspeft_config.alpha, dropout=self.tspeft_config.dropout, s=self.tspeft_config.s, lambda_reg=self.tspeft_config.lambda_reg)\n",
    "                    tspeft_layer = tspeft_layer.to(device=layer_device)\n",
    "                    layer.mlp.up_proj = tspeft_layer\n",
    "                    self.tspeft_layers[f\"layer_{layer_idx}_up_proj\"] = tspeft_layer\n",
    "                    trainable_params += sum(p.numel() for p in tspeft_layer.lora.parameters())\n",
    "                if hasattr(layer.mlp, 'down_proj'):\n",
    "                    module = layer.mlp.down_proj\n",
    "                    tspeft_layer = TSPEFTLayer(base_layer=module, rank=self.tspeft_config.rank, alpha=self.tspeft_config.alpha, dropout=self.tspeft_config.dropout, s=self.tspeft_config.s, lambda_reg=self.tspeft_config.lambda_reg)\n",
    "                    tspeft_layer = tspeft_layer.to(device=layer_device)\n",
    "                    layer.mlp.down_proj = tspeft_layer\n",
    "                    self.tspeft_layers[f\"layer_{layer_idx}_down_proj\"] = tspeft_layer\n",
    "                    trainable_params += sum(p.numel() for p in tspeft_layer.lora.parameters())\n",
    "        print(f\"TS-PEFT training enabled: {trainable_params} trainable parameters\")\n",
    "            \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                loss = None\n",
    "        return {'logits': outputs.logits, 'loss': loss, 'hidden_states': outputs.hidden_states}\n",
    "    \n",
    "    def update_thresholds(self, lr: float = 1.0):\n",
    "        for layer in self.tspeft_layers.values():\n",
    "            if hasattr(layer, '_cache_for_backward') and layer.training:\n",
    "                grad_output = torch.ones_like(layer._cache_for_backward['base_output'])\n",
    "                layer.update_threshold(grad_output, lr)\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Initial RAM Usage: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    virtual_mem = psutil.virtual_memory()\n",
    "    print(f\"Total System RAM: {virtual_mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {virtual_mem.available / 1024**3:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PARAMETER STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable TS-PEFT Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    if trainable_params > 0:\n",
    "        print(f\"Parameter Efficiency: {total_params / trainable_params:.2f}x reduction\")\n",
    "    print(f\"TS-PEFT Rank: {model.tspeft_config.rank}\")\n",
    "    print(f\"TS-PEFT Alpha: {model.tspeft_config.alpha}\")\n",
    "    print(f\"TS-PEFT s: {model.tspeft_config.s}\")\n",
    "    print(f\"TS-PEFT lambda_reg: {model.tspeft_config.lambda_reg}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"GPU MEMORY ALLOCATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.2f} GB total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_and_process_data(csv_path, tokenizer, max_length=384, java_col='java', csharp_col='C#'):\n",
    "    print(f\"Loading dataset from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset loaded: {len(df)} samples\\n\")\n",
    "    data_list = []\n",
    "    skipped = 0\n",
    "    print(\"Processing dataset...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            java_code = str(row[java_col]).strip()\n",
    "            csharp_code = str(row[csharp_col]).strip()\n",
    "            if len(java_code) < 5 or len(csharp_code) < 5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "            full_text = f\"{prompt} {csharp_code}\"\n",
    "            inputs = tokenizer(full_text, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "            labels = inputs['input_ids'].clone()\n",
    "            prompt_tokens = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "            prompt_length = prompt_tokens.shape[1]\n",
    "            labels[0, :prompt_length] = -100\n",
    "            data_list.append({'input_ids': inputs['input_ids'].squeeze(0), 'attention_mask': inputs['attention_mask'].squeeze(0), 'labels': labels.squeeze(0), 'java_code': java_code, 'csharp_code': csharp_code})\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    print(f\"Processing complete: {len(data_list)} valid samples, {skipped} skipped\\n\")\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis, max_n=4):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)])\n",
    "        hyp_ngrams = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens) - n + 1)])\n",
    "        matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "        total = sum(hyp_ngrams.values())\n",
    "        precision = matches / total if total > 0 else 0.0\n",
    "        precisions.append(precision)\n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0.0\n",
    "    weights = [1.0 / max_n] * max_n\n",
    "    log_precisions = [w * math.log(p) if p > 0 else float('-inf') for w, p in zip(weights, precisions)]\n",
    "    if any(lp == float('-inf') for lp in log_precisions):\n",
    "        return 0.0\n",
    "    geometric_mean = math.exp(sum(log_precisions))\n",
    "    brevity_penalty = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0\n",
    "    return brevity_penalty * geometric_mean\n",
    "\n",
    "\n",
    "def calculate_meteor_score(reference, hypothesis):\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    hyp_tokens = set(hypothesis.lower().split())\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    matches = len(ref_tokens & hyp_tokens)\n",
    "    precision = matches / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f_mean = (10 * precision * recall) / (9 * precision + recall)\n",
    "    return f_mean\n",
    "\n",
    "\n",
    "def calculate_rouge_l(reference, hypothesis):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(ref_tokens) == 0 or len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    lcs_length = 0\n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs_length = dp[m][n]\n",
    "    precision = lcs_length / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = lcs_length / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_exact_match(reference, hypothesis):\n",
    "    return 1.0 if reference.strip() == hypothesis.strip() else 0.0\n",
    "\n",
    "\n",
    "def calculate_chrf_score(reference, hypothesis, n=6):\n",
    "    def get_char_ngrams(text, n):\n",
    "        chars = list(text)\n",
    "        ngrams = []\n",
    "        for i in range(len(chars) - n + 1):\n",
    "            ngrams.append(''.join(chars[i:i+n]))\n",
    "        return ngrams\n",
    "    ref_ngrams = Counter(get_char_ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(get_char_ngrams(hypothesis, n))\n",
    "    if len(hyp_ngrams) == 0:\n",
    "        return 0.0\n",
    "    matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    precision = matches / sum(hyp_ngrams.values()) if sum(hyp_ngrams.values()) > 0 else 0.0\n",
    "    recall = matches / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def extract_syntax_features(code):\n",
    "    features = {'methods': len(re.findall(r'\\b(public|private|protected|static)\\s+\\w+\\s+\\w+\\s*\\(', code)), 'classes': len(re.findall(r'\\bclass\\s+\\w+', code)), 'if_statements': len(re.findall(r'\\bif\\s*\\(', code)), 'for_loops': len(re.findall(r'\\bfor\\s*\\(', code)), 'while_loops': len(re.findall(r'\\bwhile\\s*\\(', code)), 'return_statements': len(re.findall(r'\\breturn\\b', code)), 'variables': len(re.findall(r'\\b\\w+\\s*=\\s*', code)), 'try_catch': len(re.findall(r'\\btry\\s*\\{', code))}\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_syntax_similarity(ref_features, hyp_features):\n",
    "    if not ref_features or not hyp_features:\n",
    "        return 0.0\n",
    "    total_diff = 0\n",
    "    total_count = 0\n",
    "    for key in ref_features:\n",
    "        ref_val = ref_features[key]\n",
    "        hyp_val = hyp_features.get(key, 0)\n",
    "        max_val = max(ref_val, hyp_val)\n",
    "        if max_val > 0:\n",
    "            similarity = 1.0 - abs(ref_val - hyp_val) / max_val\n",
    "            total_diff += similarity\n",
    "            total_count += 1\n",
    "    return total_diff / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_code_similarity_metrics(reference, hypothesis):\n",
    "    metrics = {}\n",
    "    metrics['bleu_1'] = calculate_bleu_score(reference, hypothesis, max_n=1)\n",
    "    metrics['bleu_2'] = calculate_bleu_score(reference, hypothesis, max_n=2)\n",
    "    metrics['bleu_4'] = calculate_bleu_score(reference, hypothesis, max_n=4)\n",
    "    metrics['meteor'] = calculate_meteor_score(reference, hypothesis)\n",
    "    metrics['rouge_l'] = calculate_rouge_l(reference, hypothesis)\n",
    "    metrics['exact_match'] = calculate_exact_match(reference, hypothesis)\n",
    "    metrics['chrf'] = calculate_chrf_score(reference, hypothesis)\n",
    "    ref_features = extract_syntax_features(reference)\n",
    "    hyp_features = extract_syntax_features(hypothesis)\n",
    "    metrics['syntax_similarity'] = calculate_syntax_similarity(ref_features, hyp_features)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def check_compilation(code, language='csharp'):\n",
    "    try:\n",
    "        if language == 'csharp':\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.cs', delete=False) as f:\n",
    "                wrapper = f\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\n\\nnamespace TempCompile {{\\n    public class TempClass {{\\n        {code}\\n    }}\\n}}\"\n",
    "                f.write(wrapper)\n",
    "                temp_file = f.name\n",
    "            result = subprocess.run(['csc', '/nologo', '/t:library', '/noconfig', temp_file], capture_output=True, timeout=5, text=True)\n",
    "            success = result.returncode == 0\n",
    "            try:\n",
    "                os.unlink(temp_file)\n",
    "                dll_file = temp_file.replace('.cs', '.dll')\n",
    "                if os.path.exists(dll_file):\n",
    "                    os.unlink(dll_file)\n",
    "            except:\n",
    "                pass\n",
    "            return success\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {'input_ids': torch.stack([item['input_ids'] for item in batch]), 'attention_mask': torch.stack([item['attention_mask'] for item in batch]), 'labels': torch.stack([item['labels'] for item in batch]), 'java_code': [item['java_code'] for item in batch], 'csharp_code': [item['csharp_code'] for item in batch]}\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2, warmup_steps=100):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Number of Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Warmup Steps: {warmup_steps}\")\n",
    "    print(f\"Total Training Samples: {len(train_data)}\")\n",
    "    print(f\"Total Batches per Epoch: {len(train_data) // batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(trainable_params, lr=learning_rate, weight_decay=0.01, eps=1e-8)\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return 1.0\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    dataset = CodeTranslationDataset(train_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=False)\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 80)\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(model.primary_device)\n",
    "                attention_mask = batch['attention_mask'].to(model.primary_device)\n",
    "                labels = batch['labels'].to(model.primary_device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    model.update_thresholds(lr=1.0)\n",
    "                    scheduler.step()\n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                    global_step += 1\n",
    "                    if (batch_idx + 1) % 100 == 0:\n",
    "                        avg_loss = total_loss / valid_batches\n",
    "                        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "                if (batch_idx + 1) % 500 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Batch {batch_idx + 1} failed - {str(e)}\")\n",
    "                continue\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0\n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Valid Batches: {valid_batches}/{len(dataloader)}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average Time per Epoch: {total_time/num_epochs:.2f}s\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, batch_size=1, max_new_tokens=256):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PHASE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test Samples: {len(test_data)}\")\n",
    "    print(f\"Max Generation Length: {max_new_tokens}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    model.eval()\n",
    "    all_metrics = {'bleu_1': [], 'bleu_2': [], 'bleu_4': [], 'meteor': [], 'rouge_l': [], 'exact_match': [], 'chrf': [], 'syntax_similarity': []}\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    compilation_success = 0\n",
    "    start_time = time.time()\n",
    "    total_tokens = 0\n",
    "    print(\"Generating translations...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(test_data):\n",
    "            try:\n",
    "                java_code = item['java_code']\n",
    "                csharp_code = item['csharp_code']\n",
    "                prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "                inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=384)\n",
    "                inputs = {k: v.to(model.primary_device) for k, v in inputs.items()}\n",
    "                outputs = model.llm.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=4, early_stopping=True, do_sample=False, temperature=1.0, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                hypothesis = generated_text[len(prompt):].strip()\n",
    "                references.append(csharp_code)\n",
    "                hypotheses.append(hypothesis)\n",
    "                metrics = calculate_code_similarity_metrics(csharp_code, hypothesis)\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(metrics[key])\n",
    "                if check_compilation(hypothesis):\n",
    "                    compilation_success += 1\n",
    "                total_tokens += len(outputs[0])\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    print(f\"  Processed {idx + 1}/{len(test_data)} samples\")\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Sample {idx + 1} failed - {str(e)}\")\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(0.0)\n",
    "                references.append(\"\")\n",
    "                hypotheses.append(\"\")\n",
    "                continue\n",
    "    inference_time = time.time() - start_time\n",
    "    results = {'bleu_1': np.mean(all_metrics['bleu_1']), 'bleu_1_std': np.std(all_metrics['bleu_1']), 'bleu_2': np.mean(all_metrics['bleu_2']), 'bleu_2_std': np.std(all_metrics['bleu_2']), 'bleu_4': np.mean(all_metrics['bleu_4']), 'bleu_4_std': np.std(all_metrics['bleu_4']), 'meteor': np.mean(all_metrics['meteor']), 'meteor_std': np.std(all_metrics['meteor']), 'rouge_l': np.mean(all_metrics['rouge_l']), 'rouge_l_std': np.std(all_metrics['rouge_l']), 'exact_match': np.mean(all_metrics['exact_match']), 'chrf': np.mean(all_metrics['chrf']), 'chrf_std': np.std(all_metrics['chrf']), 'syntax_similarity': np.mean(all_metrics['syntax_similarity']), 'syntax_similarity_std': np.std(all_metrics['syntax_similarity']), 'compilation_rate': (compilation_success / len(test_data)) * 100, 'inference_time': inference_time, 'throughput': len(test_data) / inference_time, 'tokens_per_sec': total_tokens / inference_time, 'total_tokens': total_tokens, 'avg_tokens_per_sample': total_tokens / len(test_data)}\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"N-GRAM BASED METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"BLEU-1: {results['bleu_1']:.4f} (±{results['bleu_1_std']:.4f})\")\n",
    "    print(f\"BLEU-2: {results['bleu_2']:.4f} (±{results['bleu_2_std']:.4f})\")\n",
    "    print(f\"BLEU-4: {results['bleu_4']:.4f} (±{results['bleu_4_std']:.4f})\")\n",
    "    print(f\"chrF: {results['chrf']:.4f} (±{results['chrf_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SEMANTIC SIMILARITY METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f} (±{results['meteor_std']:.4f})\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f} (±{results['rouge_l_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CODE-SPECIFIC METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Syntax Similarity: {results['syntax_similarity']:.4f} (±{results['syntax_similarity_std']:.4f})\")\n",
    "    print(f\"Exact Match: {results['exact_match']:.4f}\")\n",
    "    print(f\"Compilation Success Rate: {results['compilation_rate']:.2f}%\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Inference Time: {results['inference_time']:.2f}s ({results['inference_time']/60:.2f} min)\")\n",
    "    print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
    "    print(f\"Token Generation Rate: {results['tokens_per_sec']:.2f} tokens/sec\")\n",
    "    print(f\"Total Tokens Generated: {results['total_tokens']}\")\n",
    "    print(f\"Average Tokens per Sample: {results['avg_tokens_per_sample']:.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"JAVA TO C# CODE TRANSLATION\")\n",
    "    print(\"Using TS-PEFT-Enhanced Mistral-7B\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print_system_info()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    cache_dir = '/hf_cache'\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=cache_dir)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded\\n\")\n",
    "    tspeft_config = TSPEFTConfig(rank=32, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
    "    print(\"Initializing model...\")\n",
    "    model = TSPEFTMistral(\"mistralai/Mistral-7B-Instruct-v0.2\", tspeft_config, cache_dir=cache_dir)\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    train_csv = 'train.csv'\n",
    "    train_data = load_and_process_data(train_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data loaded. Exiting.\")\n",
    "        return\n",
    "    train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2, warmup_steps=100)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TEST DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = load_and_process_data(test_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(test_data) == 0:\n",
    "        print(\"ERROR: No test data loaded. Exiting.\")\n",
    "        return\n",
    "    results = evaluate_model(model, tokenizer, test_data, batch_size=1)\n",
    "    print_evaluation_results(results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e8c07c-8419-4f71-8731-fc819a5db212",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GateRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f61233-d699-417c-afbc-157022af230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import math\n",
    "from typing import Optional, List, Dict\n",
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GateRAConfig:\n",
    "    rank: int = 16\n",
    "    alpha: float = 16.0\n",
    "    dropout: float = 0.0\n",
    "    target_modules: List[str] = None\n",
    "    entropy_reg_weight: float = 0.01\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "\n",
    "class GatingModule(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.gate_linear = nn.Linear(input_dim, 1, bias=True)\n",
    "        nn.init.zeros_(self.gate_linear.weight)\n",
    "        nn.init.zeros_(self.gate_linear.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate_logits = self.gate_linear(x)\n",
    "        gate_values = torch.sigmoid(gate_logits)\n",
    "        return gate_values\n",
    "\n",
    "\n",
    "class GateRALayer(nn.Module):\n",
    "    def __init__(self, base_layer: nn.Module, rank: int, alpha: float, dropout: float, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        self.dropout = dropout\n",
    "        self.lora_A = nn.Parameter(torch.zeros(input_dim, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, output_dim))\n",
    "        self.gating_module = GatingModule(input_dim)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base_output = self.base_layer(x)\n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, hidden_dim = x.shape\n",
    "            x_flat = x.view(-1, hidden_dim)\n",
    "        else:\n",
    "            x_flat = x\n",
    "            batch_size, seq_len = None, None\n",
    "        gate_values = self.gating_module(x_flat)\n",
    "        lora_output = x_flat @ self.lora_A @ self.lora_B\n",
    "        lora_output = self.dropout_layer(lora_output)\n",
    "        modulated_output = gate_values * lora_output * self.scaling\n",
    "        if batch_size is not None and seq_len is not None:\n",
    "            modulated_output = modulated_output.view(batch_size, seq_len, -1)\n",
    "        final_output = base_output + modulated_output\n",
    "        return final_output\n",
    "\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class GateRAMistral(nn.Module):\n",
    "    def __init__(self, model_name: str, gatera_config: GateRAConfig, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nInitializing model across {num_gpus} GPUs\")\n",
    "        for i in range(num_gpus):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name} - {props.total_memory / 1024**3:.2f} GB\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(f\"\\nLoading base model: {model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            cache_dir=cache_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        self.gatera_config = gatera_config\n",
    "        self.gatera_layers = nn.ModuleDict()\n",
    "        num_layers = len(base_model.model.layers)\n",
    "        print(f\"Model has {num_layers} transformer layers\")\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._inject_gatera_layers()\n",
    "        print(\"Model initialization complete!\\n\")\n",
    "    \n",
    "    def _inject_gatera_layers(self):\n",
    "        num_layers = len(self.llm.model.layers)\n",
    "        print(f\"Injecting GateRA layers for {num_layers} transformer layers...\")\n",
    "        trainable_params = 0\n",
    "        for layer_idx in range(num_layers):\n",
    "            layer = self.llm.model.layers[layer_idx]\n",
    "            if hasattr(layer.self_attn, 'q_proj'):\n",
    "                module = layer.self_attn.q_proj\n",
    "                input_dim = module.in_features\n",
    "                output_dim = module.out_features\n",
    "                gatera_layer = GateRALayer(\n",
    "                    base_layer=module,\n",
    "                    rank=self.gatera_config.rank,\n",
    "                    alpha=self.gatera_config.alpha,\n",
    "                    dropout=self.gatera_config.dropout,\n",
    "                    input_dim=input_dim,\n",
    "                    output_dim=output_dim\n",
    "                )\n",
    "                layer.self_attn.q_proj = gatera_layer\n",
    "                self.gatera_layers[f\"layer_{layer_idx}_q_proj\"] = gatera_layer\n",
    "                trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "            if hasattr(layer.self_attn, 'k_proj'):\n",
    "                module = layer.self_attn.k_proj\n",
    "                input_dim = module.in_features\n",
    "                output_dim = module.out_features\n",
    "                gatera_layer = GateRALayer(\n",
    "                    base_layer=module,\n",
    "                    rank=self.gatera_config.rank,\n",
    "                    alpha=self.gatera_config.alpha,\n",
    "                    dropout=self.gatera_config.dropout,\n",
    "                    input_dim=input_dim,\n",
    "                    output_dim=output_dim\n",
    "                )\n",
    "                layer.self_attn.k_proj = gatera_layer\n",
    "                self.gatera_layers[f\"layer_{layer_idx}_k_proj\"] = gatera_layer\n",
    "                trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "            if hasattr(layer.self_attn, 'v_proj'):\n",
    "                module = layer.self_attn.v_proj\n",
    "                input_dim = module.in_features\n",
    "                output_dim = module.out_features\n",
    "                gatera_layer = GateRALayer(\n",
    "                    base_layer=module,\n",
    "                    rank=self.gatera_config.rank,\n",
    "                    alpha=self.gatera_config.alpha,\n",
    "                    dropout=self.gatera_config.dropout,\n",
    "                    input_dim=input_dim,\n",
    "                    output_dim=output_dim\n",
    "                )\n",
    "                layer.self_attn.v_proj = gatera_layer\n",
    "                self.gatera_layers[f\"layer_{layer_idx}_v_proj\"] = gatera_layer\n",
    "                trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "            if hasattr(layer.self_attn, 'o_proj'):\n",
    "                module = layer.self_attn.o_proj\n",
    "                input_dim = module.in_features\n",
    "                output_dim = module.out_features\n",
    "                gatera_layer = GateRALayer(\n",
    "                    base_layer=module,\n",
    "                    rank=self.gatera_config.rank,\n",
    "                    alpha=self.gatera_config.alpha,\n",
    "                    dropout=self.gatera_config.dropout,\n",
    "                    input_dim=input_dim,\n",
    "                    output_dim=output_dim\n",
    "                )\n",
    "                layer.self_attn.o_proj = gatera_layer\n",
    "                self.gatera_layers[f\"layer_{layer_idx}_o_proj\"] = gatera_layer\n",
    "                trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "            if hasattr(layer, 'mlp'):\n",
    "                if hasattr(layer.mlp, 'gate_proj'):\n",
    "                    module = layer.mlp.gate_proj\n",
    "                    input_dim = module.in_features\n",
    "                    output_dim = module.out_features\n",
    "                    gatera_layer = GateRALayer(\n",
    "                        base_layer=module,\n",
    "                        rank=self.gatera_config.rank,\n",
    "                        alpha=self.gatera_config.alpha,\n",
    "                        dropout=self.gatera_config.dropout,\n",
    "                        input_dim=input_dim,\n",
    "                        output_dim=output_dim\n",
    "                    )\n",
    "                    layer.mlp.gate_proj = gatera_layer\n",
    "                    self.gatera_layers[f\"layer_{layer_idx}_gate_proj\"] = gatera_layer\n",
    "                    trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                    trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "                if hasattr(layer.mlp, 'up_proj'):\n",
    "                    module = layer.mlp.up_proj\n",
    "                    input_dim = module.in_features\n",
    "                    output_dim = module.out_features\n",
    "                    gatera_layer = GateRALayer(\n",
    "                        base_layer=module,\n",
    "                        rank=self.gatera_config.rank,\n",
    "                        alpha=self.gatera_config.alpha,\n",
    "                        dropout=self.gatera_config.dropout,\n",
    "                        input_dim=input_dim,\n",
    "                        output_dim=output_dim\n",
    "                    )\n",
    "                    layer.mlp.up_proj = gatera_layer\n",
    "                    self.gatera_layers[f\"layer_{layer_idx}_up_proj\"] = gatera_layer\n",
    "                    trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                    trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "                if hasattr(layer.mlp, 'down_proj'):\n",
    "                    module = layer.mlp.down_proj\n",
    "                    input_dim = module.in_features\n",
    "                    output_dim = module.out_features\n",
    "                    gatera_layer = GateRALayer(\n",
    "                        base_layer=module,\n",
    "                        rank=self.gatera_config.rank,\n",
    "                        alpha=self.gatera_config.alpha,\n",
    "                        dropout=self.gatera_config.dropout,\n",
    "                        input_dim=input_dim,\n",
    "                        output_dim=output_dim\n",
    "                    )\n",
    "                    layer.mlp.down_proj = gatera_layer\n",
    "                    self.gatera_layers[f\"layer_{layer_idx}_down_proj\"] = gatera_layer\n",
    "                    trainable_params += gatera_layer.lora_A.numel() + gatera_layer.lora_B.numel()\n",
    "                    trainable_params += sum(p.numel() for p in gatera_layer.gating_module.parameters())\n",
    "        print(f\"GateRA training enabled: {trainable_params} trainable parameters\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "            task_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            total_entropy_loss = 0.0\n",
    "            gate_count = 0\n",
    "            for name, layer in self.gatera_layers.items():\n",
    "                if hasattr(layer, 'gating_module'):\n",
    "                    dummy_input = torch.randn(\n",
    "                        input_ids.shape[0] * input_ids.shape[1],\n",
    "                        layer.lora_A.shape[0],\n",
    "                        device=input_ids.device,\n",
    "                        dtype=torch.bfloat16\n",
    "                    )\n",
    "                    gate_vals = layer.gating_module(dummy_input)\n",
    "                    eps = 1e-8\n",
    "                    gate_vals = torch.clamp(gate_vals, eps, 1.0 - eps)\n",
    "                    entropy = -gate_vals * torch.log(gate_vals) - (1 - gate_vals) * torch.log(1 - gate_vals)\n",
    "                    total_entropy_loss += entropy.mean()\n",
    "                    gate_count += 1\n",
    "            if gate_count > 0:\n",
    "                avg_entropy_loss = total_entropy_loss / gate_count\n",
    "            else:\n",
    "                avg_entropy_loss = torch.tensor(0.0, device=input_ids.device)\n",
    "            loss = task_loss + self.gatera_config.entropy_reg_weight * avg_entropy_loss\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                loss = None\n",
    "        return {'logits': outputs.logits, 'loss': loss, 'hidden_states': outputs.hidden_states}\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Initial RAM Usage: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    virtual_mem = psutil.virtual_memory()\n",
    "    print(f\"Total System RAM: {virtual_mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {virtual_mem.available / 1024**3:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PARAMETER STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable GateRA Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    if trainable_params > 0:\n",
    "        print(f\"Parameter Efficiency: {total_params / trainable_params:.2f}x reduction\")\n",
    "    print(f\"GateRA Rank: {model.gatera_config.rank}\")\n",
    "    print(f\"GateRA Alpha: {model.gatera_config.alpha}\")\n",
    "    print(f\"Entropy Regularization Weight: {model.gatera_config.entropy_reg_weight}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"GPU MEMORY ALLOCATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.2f} GB total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_and_process_data(csv_path, tokenizer, max_length=384, java_col='java', csharp_col='C#'):\n",
    "    print(f\"Loading dataset from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset loaded: {len(df)} samples\\n\")\n",
    "    data_list = []\n",
    "    skipped = 0\n",
    "    print(\"Processing dataset...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            java_code = str(row[java_col]).strip()\n",
    "            csharp_code = str(row[csharp_col]).strip()\n",
    "            if len(java_code) < 5 or len(csharp_code) < 5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "            full_text = f\"{prompt} {csharp_code}\"\n",
    "            inputs = tokenizer(\n",
    "                full_text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            labels = inputs['input_ids'].clone()\n",
    "            prompt_tokens = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "            prompt_length = prompt_tokens.shape[1]\n",
    "            labels[0, :prompt_length] = -100\n",
    "            data_list.append({\n",
    "                'input_ids': inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "                'labels': labels.squeeze(0),\n",
    "                'java_code': java_code,\n",
    "                'csharp_code': csharp_code\n",
    "            })\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    print(f\"Processing complete: {len(data_list)} valid samples, {skipped} skipped\\n\")\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis, max_n=4):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)])\n",
    "        hyp_ngrams = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens) - n + 1)])\n",
    "        matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "        total = sum(hyp_ngrams.values())\n",
    "        precision = matches / total if total > 0 else 0.0\n",
    "        precisions.append(precision)\n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0.0\n",
    "    weights = [1.0 / max_n] * max_n\n",
    "    log_precisions = [w * math.log(p) if p > 0 else float('-inf') for w, p in zip(weights, precisions)]\n",
    "    if any(lp == float('-inf') for lp in log_precisions):\n",
    "        return 0.0\n",
    "    geometric_mean = math.exp(sum(log_precisions))\n",
    "    brevity_penalty = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0\n",
    "    return brevity_penalty * geometric_mean\n",
    "\n",
    "\n",
    "def calculate_meteor_score(reference, hypothesis):\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    hyp_tokens = set(hypothesis.lower().split())\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    matches = len(ref_tokens & hyp_tokens)\n",
    "    precision = matches / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f_mean = (10 * precision * recall) / (9 * precision + recall)\n",
    "    return f_mean\n",
    "\n",
    "\n",
    "def calculate_rouge_l(reference, hypothesis):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(ref_tokens) == 0 or len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    lcs_length = 0\n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs_length = dp[m][n]\n",
    "    precision = lcs_length / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = lcs_length / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_exact_match(reference, hypothesis):\n",
    "    return 1.0 if reference.strip() == hypothesis.strip() else 0.0\n",
    "\n",
    "\n",
    "def calculate_chrf_score(reference, hypothesis, n=6):\n",
    "    def get_char_ngrams(text, n):\n",
    "        chars = list(text)\n",
    "        ngrams = []\n",
    "        for i in range(len(chars) - n + 1):\n",
    "            ngrams.append(''.join(chars[i:i+n]))\n",
    "        return ngrams\n",
    "    ref_ngrams = Counter(get_char_ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(get_char_ngrams(hypothesis, n))\n",
    "    if len(hyp_ngrams) == 0:\n",
    "        return 0.0\n",
    "    matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    precision = matches / sum(hyp_ngrams.values()) if sum(hyp_ngrams.values()) > 0 else 0.0\n",
    "    recall = matches / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def extract_syntax_features(code):\n",
    "    features = {\n",
    "        'methods': len(re.findall(r'\\b(public|private|protected|static)\\s+\\w+\\s+\\w+\\s*\\(', code)),\n",
    "        'classes': len(re.findall(r'\\bclass\\s+\\w+', code)),\n",
    "        'if_statements': len(re.findall(r'\\bif\\s*\\(', code)),\n",
    "        'for_loops': len(re.findall(r'\\bfor\\s*\\(', code)),\n",
    "        'while_loops': len(re.findall(r'\\bwhile\\s*\\(', code)),\n",
    "        'return_statements': len(re.findall(r'\\breturn\\b', code)),\n",
    "        'variables': len(re.findall(r'\\b\\w+\\s*=\\s*', code)),\n",
    "        'try_catch': len(re.findall(r'\\btry\\s*\\{', code)),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_syntax_similarity(ref_features, hyp_features):\n",
    "    if not ref_features or not hyp_features:\n",
    "        return 0.0\n",
    "    total_diff = 0\n",
    "    total_count = 0\n",
    "    for key in ref_features:\n",
    "        ref_val = ref_features[key]\n",
    "        hyp_val = hyp_features.get(key, 0)\n",
    "        max_val = max(ref_val, hyp_val)\n",
    "        if max_val > 0:\n",
    "            similarity = 1.0 - abs(ref_val - hyp_val) / max_val\n",
    "            total_diff += similarity\n",
    "            total_count += 1\n",
    "    return total_diff / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_code_similarity_metrics(reference, hypothesis):\n",
    "    metrics = {}\n",
    "    metrics['bleu_1'] = calculate_bleu_score(reference, hypothesis, max_n=1)\n",
    "    metrics['bleu_2'] = calculate_bleu_score(reference, hypothesis, max_n=2)\n",
    "    metrics['bleu_4'] = calculate_bleu_score(reference, hypothesis, max_n=4)\n",
    "    metrics['meteor'] = calculate_meteor_score(reference, hypothesis)\n",
    "    metrics['rouge_l'] = calculate_rouge_l(reference, hypothesis)\n",
    "    metrics['exact_match'] = calculate_exact_match(reference, hypothesis)\n",
    "    metrics['chrf'] = calculate_chrf_score(reference, hypothesis)\n",
    "    ref_features = extract_syntax_features(reference)\n",
    "    hyp_features = extract_syntax_features(hypothesis)\n",
    "    metrics['syntax_similarity'] = calculate_syntax_similarity(ref_features, hyp_features)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def check_compilation(code, language='csharp'):\n",
    "    try:\n",
    "        if language == 'csharp':\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.cs', delete=False) as f:\n",
    "                wrapper = f\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\n\\nnamespace TempCompile {{\\n    public class TempClass {{\\n        {code}\\n    }}\\n}}\"\n",
    "                f.write(wrapper)\n",
    "                temp_file = f.name\n",
    "            result = subprocess.run(\n",
    "                ['csc', '/nologo', '/t:library', '/noconfig', temp_file],\n",
    "                capture_output=True,\n",
    "                timeout=5,\n",
    "                text=True\n",
    "            )\n",
    "            success = result.returncode == 0\n",
    "            try:\n",
    "                os.unlink(temp_file)\n",
    "                dll_file = temp_file.replace('.cs', '.dll')\n",
    "                if os.path.exists(dll_file):\n",
    "                    os.unlink(dll_file)\n",
    "            except:\n",
    "                pass\n",
    "            return success\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch]),\n",
    "        'java_code': [item['java_code'] for item in batch],\n",
    "        'csharp_code': [item['csharp_code'] for item in batch]\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2, warmup_steps=100):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Number of Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Warmup Steps: {warmup_steps}\")\n",
    "    print(f\"Total Training Samples: {len(train_data)}\")\n",
    "    print(f\"Total Batches per Epoch: {len(train_data) // batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(trainable_params, lr=learning_rate, weight_decay=0.01, eps=1e-8)\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return 1.0\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    dataset = CodeTranslationDataset(train_data)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 80)\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].cuda()\n",
    "                attention_mask = batch['attention_mask'].cuda()\n",
    "                labels = batch['labels'].cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                    global_step += 1\n",
    "                    if (batch_idx + 1) % 100 == 0:\n",
    "                        avg_loss = total_loss / valid_batches\n",
    "                        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "                if (batch_idx + 1) % 500 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Batch {batch_idx + 1} failed - {str(e)}\")\n",
    "                continue\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0\n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Valid Batches: {valid_batches}/{len(dataloader)}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average Time per Epoch: {total_time/num_epochs:.2f}s\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, batch_size=1, max_new_tokens=256):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PHASE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test Samples: {len(test_data)}\")\n",
    "    print(f\"Max Generation Length: {max_new_tokens}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    model.eval()\n",
    "    all_metrics = {\n",
    "        'bleu_1': [], 'bleu_2': [], 'bleu_4': [],\n",
    "        'meteor': [], 'rouge_l': [], 'exact_match': [],\n",
    "        'chrf': [], 'syntax_similarity': []\n",
    "    }\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    compilation_success = 0\n",
    "    start_time = time.time()\n",
    "    total_tokens = 0\n",
    "    print(\"Generating translations...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(test_data):\n",
    "            try:\n",
    "                java_code = item['java_code']\n",
    "                csharp_code = item['csharp_code']\n",
    "                prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "                inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=384)\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                outputs = model.llm.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                hypothesis = generated_text[len(prompt):].strip()\n",
    "                references.append(csharp_code)\n",
    "                hypotheses.append(hypothesis)\n",
    "                metrics = calculate_code_similarity_metrics(csharp_code, hypothesis)\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(metrics[key])\n",
    "                if check_compilation(hypothesis):\n",
    "                    compilation_success += 1\n",
    "                total_tokens += len(outputs[0])\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    print(f\"  Processed {idx + 1}/{len(test_data)} samples\")\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Sample {idx + 1} failed - {str(e)}\")\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(0.0)\n",
    "                references.append(\"\")\n",
    "                hypotheses.append(\"\")\n",
    "                continue\n",
    "    inference_time = time.time() - start_time\n",
    "    results = {\n",
    "        'bleu_1': np.mean(all_metrics['bleu_1']),\n",
    "        'bleu_1_std': np.std(all_metrics['bleu_1']),\n",
    "        'bleu_2': np.mean(all_metrics['bleu_2']),\n",
    "        'bleu_2_std': np.std(all_metrics['bleu_2']),\n",
    "        'bleu_4': np.mean(all_metrics['bleu_4']),\n",
    "        'bleu_4_std': np.std(all_metrics['bleu_4']),\n",
    "        'meteor': np.mean(all_metrics['meteor']),\n",
    "        'meteor_std': np.std(all_metrics['meteor']),\n",
    "        'rouge_l': np.mean(all_metrics['rouge_l']),\n",
    "        'rouge_l_std': np.std(all_metrics['rouge_l']),\n",
    "        'exact_match': np.mean(all_metrics['exact_match']),\n",
    "        'chrf': np.mean(all_metrics['chrf']),\n",
    "        'chrf_std': np.std(all_metrics['chrf']),\n",
    "        'syntax_similarity': np.mean(all_metrics['syntax_similarity']),\n",
    "        'syntax_similarity_std': np.std(all_metrics['syntax_similarity']),\n",
    "        'compilation_rate': (compilation_success / len(test_data)) * 100,\n",
    "        'inference_time': inference_time,\n",
    "        'throughput': len(test_data) / inference_time,\n",
    "        'tokens_per_sec': total_tokens / inference_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_sample': total_tokens / len(test_data)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"N-GRAM BASED METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"BLEU-1: {results['bleu_1']:.4f} (±{results['bleu_1_std']:.4f})\")\n",
    "    print(f\"BLEU-2: {results['bleu_2']:.4f} (±{results['bleu_2_std']:.4f})\")\n",
    "    print(f\"BLEU-4: {results['bleu_4']:.4f} (±{results['bleu_4_std']:.4f})\")\n",
    "    print(f\"chrF: {results['chrf']:.4f} (±{results['chrf_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SEMANTIC SIMILARITY METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f} (±{results['meteor_std']:.4f})\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f} (±{results['rouge_l_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CODE-SPECIFIC METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Syntax Similarity: {results['syntax_similarity']:.4f} (±{results['syntax_similarity_std']:.4f})\")\n",
    "    print(f\"Exact Match: {results['exact_match']:.4f}\")\n",
    "    print(f\"Compilation Success Rate: {results['compilation_rate']:.2f}%\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Inference Time: {results['inference_time']:.2f}s ({results['inference_time']/60:.2f} min)\")\n",
    "    print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
    "    print(f\"Token Generation Rate: {results['tokens_per_sec']:.2f} tokens/sec\")\n",
    "    print(f\"Total Tokens Generated: {results['total_tokens']}\")\n",
    "    print(f\"Average Tokens per Sample: {results['avg_tokens_per_sample']:.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"JAVA TO C# CODE TRANSLATION\")\n",
    "    print(\"Using GateRA-Enhanced Mistral-7B\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print_system_info()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    cache_dir = 'hf_cache'\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded\\n\")\n",
    "    gatera_config = GateRAConfig(\n",
    "        rank=16,\n",
    "        alpha=16.0,\n",
    "        dropout=0.0,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        entropy_reg_weight=0.01\n",
    "    )\n",
    "    print(\"Initializing model...\")\n",
    "    model = GateRAMistral(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        gatera_config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = load_and_process_data(train_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data loaded. Exiting.\")\n",
    "        return\n",
    "    train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2, warmup_steps=100)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TEST DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = load_and_process_data(test_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(test_data) == 0:\n",
    "        print(\"ERROR: No test data loaded. Exiting.\")\n",
    "        return\n",
    "    results = evaluate_model(model, tokenizer, test_data, batch_size=1)\n",
    "    print_evaluation_results(results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb98a7b-4388-4ebc-96b6-1f7abb2e99f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2e835-9154-447d-82cc-563db691f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "\n",
    "class LoRAConfig:\n",
    "    def __init__(self, r: int = 8, lora_alpha: int = 16, lora_dropout: float = 0.1):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.scaling = lora_alpha / r\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, config: LoRAConfig):\n",
    "        super().__init__()\n",
    "        self.r = config.r\n",
    "        self.lora_alpha = config.lora_alpha\n",
    "        self.scaling = config.scaling\n",
    "        self.lora_dropout = nn.Dropout(p=config.lora_dropout)\n",
    "        self.lora_A = nn.Parameter(torch.zeros((in_features, config.r)))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((config.r, out_features)))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x, original_weight):\n",
    "        original_output = F.linear(x, original_weight)\n",
    "        lora_output = F.linear(F.linear(self.lora_dropout(x), self.lora_A.T), self.lora_B.T)\n",
    "        return original_output + lora_output * self.scaling\n",
    "\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class LoRAMistral(nn.Module):\n",
    "    def __init__(self, model_name: str, lora_config: LoRAConfig, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nInitializing model across {num_gpus} GPUs\")\n",
    "        for i in range(num_gpus):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name} - {props.total_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        self.primary_device = 'cuda:0'\n",
    "        \n",
    "        print(f\"\\nLoading base model: {model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            cache_dir=cache_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.llm = base_model\n",
    "        self.lora_config = lora_config\n",
    "        \n",
    "        num_layers = len(base_model.model.layers)\n",
    "        print(f\"Model has {num_layers} transformer layers\")\n",
    "        \n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.lora_layers = nn.ModuleDict()\n",
    "        self._inject_lora_layers()\n",
    "        \n",
    "        print(\"Model initialization complete!\\n\")\n",
    "    \n",
    "    def _inject_lora_layers(self):\n",
    "        num_layers = len(self.llm.model.layers)\n",
    "        hidden_size = self.llm.config.hidden_size\n",
    "        \n",
    "        print(f\"Injecting LoRA layers (r={self.lora_config.r}) into {num_layers} layers...\")\n",
    "        \n",
    "        for layer_idx in range(num_layers):\n",
    "            layer = self.llm.model.layers[layer_idx]\n",
    "            device = next(layer.parameters()).device\n",
    "            \n",
    "            self_attn = layer.self_attn\n",
    "            q_proj = self_attn.q_proj\n",
    "            v_proj = self_attn.v_proj\n",
    "            \n",
    "            lora_q = LoRALayer(hidden_size, q_proj.out_features, self.lora_config).to(device).to(torch.bfloat16)\n",
    "            lora_v = LoRALayer(hidden_size, v_proj.out_features, self.lora_config).to(device).to(torch.bfloat16)\n",
    "            \n",
    "            self.lora_layers[f'layer_{layer_idx}_q'] = lora_q\n",
    "            self.lora_layers[f'layer_{layer_idx}_v'] = lora_v\n",
    "            \n",
    "            original_q_weight = q_proj.weight.detach()\n",
    "            original_v_weight = v_proj.weight.detach()\n",
    "            \n",
    "            def make_lora_forward(lora_layer, orig_weight):\n",
    "                def new_forward(x):\n",
    "                    return lora_layer(x, orig_weight)\n",
    "                return new_forward\n",
    "            \n",
    "            q_proj.forward = make_lora_forward(lora_q, original_q_weight)\n",
    "            v_proj.forward = make_lora_forward(lora_v, original_v_weight)\n",
    "        \n",
    "        print(f\"LoRA injection complete\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                loss = None\n",
    "        \n",
    "        return {'logits': outputs.logits, 'loss': loss, 'hidden_states': outputs.hidden_states}\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Initial RAM Usage: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    \n",
    "    virtual_mem = psutil.virtual_memory()\n",
    "    print(f\"Total System RAM: {virtual_mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {virtual_mem.available / 1024**3:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    lora_params = sum(p.numel() for p in model.lora_layers.parameters())\n",
    "    trainable_params = lora_params\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PARAMETER STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable LoRA Parameters: {lora_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    print(f\"Parameter Efficiency: {total_params / trainable_params:.2f}x reduction\")\n",
    "    print(f\"LoRA Rank (r): {model.lora_config.r}\")\n",
    "    print(f\"LoRA Alpha: {model.lora_config.lora_alpha}\")\n",
    "    print(f\"LoRA Scaling Factor: {model.lora_config.scaling}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"GPU MEMORY ALLOCATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.2f} GB total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_and_process_data(csv_path, tokenizer, max_length=384, java_col='java', csharp_col='C#'):\n",
    "    print(f\"Loading dataset from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset loaded: {len(df)} samples\\n\")\n",
    "    \n",
    "    data_list = []\n",
    "    skipped = 0\n",
    "    \n",
    "    print(\"Processing dataset...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            java_code = str(row[java_col]).strip()\n",
    "            csharp_code = str(row[csharp_col]).strip()\n",
    "            \n",
    "            if len(java_code) < 5 or len(csharp_code) < 5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "            full_text = f\"{prompt} {csharp_code}\"\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                full_text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            labels = inputs['input_ids'].clone()\n",
    "            \n",
    "            prompt_tokens = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "            prompt_length = prompt_tokens.shape[1]\n",
    "            labels[0, :prompt_length] = -100\n",
    "            \n",
    "            data_list.append({\n",
    "                'input_ids': inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "                'labels': labels.squeeze(0),\n",
    "                'java_code': java_code,\n",
    "                'csharp_code': csharp_code\n",
    "            })\n",
    "            \n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(df)} samples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"Processing complete: {len(data_list)} valid samples, {skipped} skipped\\n\")\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis, max_n=4):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    \n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)])\n",
    "        hyp_ngrams = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens) - n + 1)])\n",
    "        \n",
    "        matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "        total = sum(hyp_ngrams.values())\n",
    "        \n",
    "        precision = matches / total if total > 0 else 0.0\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0.0\n",
    "    \n",
    "    weights = [1.0 / max_n] * max_n\n",
    "    log_precisions = [w * math.log(p) if p > 0 else float('-inf') for w, p in zip(weights, precisions)]\n",
    "    \n",
    "    if any(lp == float('-inf') for lp in log_precisions):\n",
    "        return 0.0\n",
    "    \n",
    "    geometric_mean = math.exp(sum(log_precisions))\n",
    "    \n",
    "    brevity_penalty = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0\n",
    "    \n",
    "    return brevity_penalty * geometric_mean\n",
    "\n",
    "\n",
    "def calculate_meteor_score(reference, hypothesis):\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    hyp_tokens = set(hypothesis.lower().split())\n",
    "    \n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    matches = len(ref_tokens & hyp_tokens)\n",
    "    precision = matches / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f_mean = (10 * precision * recall) / (9 * precision + recall)\n",
    "    return f_mean\n",
    "\n",
    "\n",
    "def calculate_rouge_l(reference, hypothesis):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    \n",
    "    if len(ref_tokens) == 0 or len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    lcs_length = 0\n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    \n",
    "    lcs_length = dp[m][n]\n",
    "    \n",
    "    precision = lcs_length / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = lcs_length / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_exact_match(reference, hypothesis):\n",
    "    return 1.0 if reference.strip() == hypothesis.strip() else 0.0\n",
    "\n",
    "\n",
    "def calculate_chrf_score(reference, hypothesis, n=6):\n",
    "    def get_char_ngrams(text, n):\n",
    "        chars = list(text)\n",
    "        ngrams = []\n",
    "        for i in range(len(chars) - n + 1):\n",
    "            ngrams.append(''.join(chars[i:i+n]))\n",
    "        return ngrams\n",
    "    \n",
    "    ref_ngrams = Counter(get_char_ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(get_char_ngrams(hypothesis, n))\n",
    "    \n",
    "    if len(hyp_ngrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    precision = matches / sum(hyp_ngrams.values()) if sum(hyp_ngrams.values()) > 0 else 0.0\n",
    "    recall = matches / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0.0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def extract_syntax_features(code):\n",
    "    features = {\n",
    "        'methods': len(re.findall(r'\\b(public|private|protected|static)\\s+\\w+\\s+\\w+\\s*\\(', code)),\n",
    "        'classes': len(re.findall(r'\\bclass\\s+\\w+', code)),\n",
    "        'if_statements': len(re.findall(r'\\bif\\s*\\(', code)),\n",
    "        'for_loops': len(re.findall(r'\\bfor\\s*\\(', code)),\n",
    "        'while_loops': len(re.findall(r'\\bwhile\\s*\\(', code)),\n",
    "        'return_statements': len(re.findall(r'\\breturn\\b', code)),\n",
    "        'variables': len(re.findall(r'\\b\\w+\\s*=\\s*', code)),\n",
    "        'try_catch': len(re.findall(r'\\btry\\s*\\{', code)),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_syntax_similarity(ref_features, hyp_features):\n",
    "    if not ref_features or not hyp_features:\n",
    "        return 0.0\n",
    "    \n",
    "    total_diff = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for key in ref_features:\n",
    "        ref_val = ref_features[key]\n",
    "        hyp_val = hyp_features.get(key, 0)\n",
    "        \n",
    "        max_val = max(ref_val, hyp_val)\n",
    "        if max_val > 0:\n",
    "            similarity = 1.0 - abs(ref_val - hyp_val) / max_val\n",
    "            total_diff += similarity\n",
    "            total_count += 1\n",
    "    \n",
    "    return total_diff / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_code_similarity_metrics(reference, hypothesis):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['bleu_1'] = calculate_bleu_score(reference, hypothesis, max_n=1)\n",
    "    metrics['bleu_2'] = calculate_bleu_score(reference, hypothesis, max_n=2)\n",
    "    metrics['bleu_4'] = calculate_bleu_score(reference, hypothesis, max_n=4)\n",
    "    metrics['meteor'] = calculate_meteor_score(reference, hypothesis)\n",
    "    metrics['rouge_l'] = calculate_rouge_l(reference, hypothesis)\n",
    "    metrics['exact_match'] = calculate_exact_match(reference, hypothesis)\n",
    "    metrics['chrf'] = calculate_chrf_score(reference, hypothesis)\n",
    "    \n",
    "    ref_features = extract_syntax_features(reference)\n",
    "    hyp_features = extract_syntax_features(hypothesis)\n",
    "    metrics['syntax_similarity'] = calculate_syntax_similarity(ref_features, hyp_features)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def check_compilation(code, language='csharp'):\n",
    "    try:\n",
    "        if language == 'csharp':\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.cs', delete=False) as f:\n",
    "                wrapper = f\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\n\\nnamespace TempCompile {{\\n    public class TempClass {{\\n        {code}\\n    }}\\n}}\"\n",
    "                f.write(wrapper)\n",
    "                temp_file = f.name\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                ['csc', '/nologo', '/t:library', '/noconfig', temp_file],\n",
    "                capture_output=True,\n",
    "                timeout=5,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            success = result.returncode == 0\n",
    "            \n",
    "            try:\n",
    "                os.unlink(temp_file)\n",
    "                dll_file = temp_file.replace('.cs', '.dll')\n",
    "                if os.path.exists(dll_file):\n",
    "                    os.unlink(dll_file)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return success\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch]),\n",
    "        'java_code': [item['java_code'] for item in batch],\n",
    "        'csharp_code': [item['csharp_code'] for item in batch]\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Number of Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Total Training Samples: {len(train_data)}\")\n",
    "    print(f\"Total Batches per Epoch: {len(train_data) // batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    lora_params = [p for p in model.lora_layers.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(lora_params, lr=learning_rate, weight_decay=0.01, eps=1e-8)\n",
    "    \n",
    "    dataset = CodeTranslationDataset(train_data)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].cuda()\n",
    "                attention_mask = batch['attention_mask'].cuda()\n",
    "                labels = batch['labels'].cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                    \n",
    "                    if (batch_idx + 1) % 100 == 0:\n",
    "                        avg_loss = total_loss / valid_batches\n",
    "                        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                if (batch_idx + 1) % 500 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Batch {batch_idx + 1} failed - {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Valid Batches: {valid_batches}/{len(dataloader)}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average Time per Epoch: {total_time/num_epochs:.2f}s\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, batch_size=1, max_new_tokens=256):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PHASE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test Samples: {len(test_data)}\")\n",
    "    print(f\"Max Generation Length: {max_new_tokens}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_metrics = {\n",
    "        'bleu_1': [], 'bleu_2': [], 'bleu_4': [],\n",
    "        'meteor': [], 'rouge_l': [], 'exact_match': [],\n",
    "        'chrf': [], 'syntax_similarity': []\n",
    "    }\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    compilation_success = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_tokens = 0\n",
    "    \n",
    "    print(\"Generating translations...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(test_data):\n",
    "            try:\n",
    "                java_code = item['java_code']\n",
    "                csharp_code = item['csharp_code']\n",
    "                \n",
    "                prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "                \n",
    "                inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=384)\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                outputs = model.llm.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                hypothesis = generated_text[len(prompt):].strip()\n",
    "                \n",
    "                references.append(csharp_code)\n",
    "                hypotheses.append(hypothesis)\n",
    "                \n",
    "                metrics = calculate_code_similarity_metrics(csharp_code, hypothesis)\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(metrics[key])\n",
    "                \n",
    "                if check_compilation(hypothesis):\n",
    "                    compilation_success += 1\n",
    "                \n",
    "                total_tokens += len(outputs[0])\n",
    "                \n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    print(f\"  Processed {idx + 1}/{len(test_data)} samples\")\n",
    "                \n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Sample {idx + 1} failed - {str(e)}\")\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(0.0)\n",
    "                references.append(\"\")\n",
    "                hypotheses.append(\"\")\n",
    "                continue\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    results = {\n",
    "        'bleu_1': np.mean(all_metrics['bleu_1']),\n",
    "        'bleu_1_std': np.std(all_metrics['bleu_1']),\n",
    "        'bleu_2': np.mean(all_metrics['bleu_2']),\n",
    "        'bleu_2_std': np.std(all_metrics['bleu_2']),\n",
    "        'bleu_4': np.mean(all_metrics['bleu_4']),\n",
    "        'bleu_4_std': np.std(all_metrics['bleu_4']),\n",
    "        'meteor': np.mean(all_metrics['meteor']),\n",
    "        'meteor_std': np.std(all_metrics['meteor']),\n",
    "        'rouge_l': np.mean(all_metrics['rouge_l']),\n",
    "        'rouge_l_std': np.std(all_metrics['rouge_l']),\n",
    "        'exact_match': np.mean(all_metrics['exact_match']),\n",
    "        'chrf': np.mean(all_metrics['chrf']),\n",
    "        'chrf_std': np.std(all_metrics['chrf']),\n",
    "        'syntax_similarity': np.mean(all_metrics['syntax_similarity']),\n",
    "        'syntax_similarity_std': np.std(all_metrics['syntax_similarity']),\n",
    "        'compilation_rate': (compilation_success / len(test_data)) * 100,\n",
    "        'inference_time': inference_time,\n",
    "        'throughput': len(test_data) / inference_time,\n",
    "        'tokens_per_sec': total_tokens / inference_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_sample': total_tokens / len(test_data)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"N-GRAM BASED METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"BLEU-1: {results['bleu_1']:.4f} (±{results['bleu_1_std']:.4f})\")\n",
    "    print(f\"BLEU-2: {results['bleu_2']:.4f} (±{results['bleu_2_std']:.4f})\")\n",
    "    print(f\"BLEU-4: {results['bleu_4']:.4f} (±{results['bleu_4_std']:.4f})\")\n",
    "    print(f\"chrF: {results['chrf']:.4f} (±{results['chrf_std']:.4f})\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SEMANTIC SIMILARITY METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f} (±{results['meteor_std']:.4f})\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f} (±{results['rouge_l_std']:.4f})\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CODE-SPECIFIC METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Syntax Similarity: {results['syntax_similarity']:.4f} (±{results['syntax_similarity_std']:.4f})\")\n",
    "    print(f\"Exact Match: {results['exact_match']:.4f}\")\n",
    "    print(f\"Compilation Success Rate: {results['compilation_rate']:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Inference Time: {results['inference_time']:.2f}s ({results['inference_time']/60:.2f} min)\")\n",
    "    print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
    "    print(f\"Token Generation Rate: {results['tokens_per_sec']:.2f} tokens/sec\")\n",
    "    print(f\"Total Tokens Generated: {results['total_tokens']}\")\n",
    "    print(f\"Average Tokens per Sample: {results['avg_tokens_per_sample']:.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"JAVA TO C# CODE TRANSLATION\")\n",
    "    print(\"Using LoRA-Enhanced Mistral-7B\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print_system_info()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    cache_dir = '/hf_cache'\n",
    "    \n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded\\n\")\n",
    "    \n",
    "    lora_config = LoRAConfig(r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "    \n",
    "    print(\"Initializing model...\")\n",
    "    model = LoRAMistral(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        lora_config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    print_parameter_statistics(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    train_csv = '/train.csv'\n",
    "    train_data = load_and_process_data(train_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TEST DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_csv = '/test.csv'\n",
    "    test_data = load_and_process_data(test_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    \n",
    "    if len(test_data) == 0:\n",
    "        print(\"ERROR: No test data loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    results = evaluate_model(model, tokenizer, test_data, batch_size=1)\n",
    "    \n",
    "    print_evaluation_results(results)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da64aec-9d25-44ad-811e-b1fb473c9602",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BitFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a8d52-6781-4163-a0a6-c50c824ea039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "\n",
    "class BitFitConfig:\n",
    "    def __init__(self, bias_terms='all'):\n",
    "        self.bias_terms = bias_terms\n",
    "\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class BitFitMistral(nn.Module):\n",
    "    def __init__(self, model_name: str, bitfit_config: BitFitConfig, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nInitializing model across {num_gpus} GPUs\")\n",
    "        for i in range(num_gpus):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name} - {props.total_memory / 1024**3:.2f} GB\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(f\"\\nLoading base model: {model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            cache_dir=cache_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        self.bitfit_config = bitfit_config\n",
    "        num_layers = len(base_model.model.layers)\n",
    "        print(f\"Model has {num_layers} transformer layers\")\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._add_and_enable_bias_training()\n",
    "        print(\"Model initialization complete!\\n\")\n",
    "    \n",
    "    def _add_and_enable_bias_training(self):\n",
    "        num_layers = len(self.llm.model.layers)\n",
    "        print(f\"Adding and enabling bias terms for {num_layers} layers...\")\n",
    "        trainable_biases = 0\n",
    "        for layer_idx in range(num_layers):\n",
    "            layer = self.llm.model.layers[layer_idx]\n",
    "            if hasattr(layer.self_attn, 'q_proj'):\n",
    "                if layer.self_attn.q_proj.bias is None:\n",
    "                    layer.self_attn.q_proj.bias = nn.Parameter(torch.zeros(layer.self_attn.q_proj.out_features, dtype=torch.bfloat16, device=layer.self_attn.q_proj.weight.device))\n",
    "                layer.self_attn.q_proj.bias.requires_grad = True\n",
    "                trainable_biases += layer.self_attn.q_proj.bias.numel()\n",
    "            if hasattr(layer.self_attn, 'k_proj'):\n",
    "                if layer.self_attn.k_proj.bias is None:\n",
    "                    layer.self_attn.k_proj.bias = nn.Parameter(torch.zeros(layer.self_attn.k_proj.out_features, dtype=torch.bfloat16, device=layer.self_attn.k_proj.weight.device))\n",
    "                layer.self_attn.k_proj.bias.requires_grad = True\n",
    "                trainable_biases += layer.self_attn.k_proj.bias.numel()\n",
    "            if hasattr(layer.self_attn, 'v_proj'):\n",
    "                if layer.self_attn.v_proj.bias is None:\n",
    "                    layer.self_attn.v_proj.bias = nn.Parameter(torch.zeros(layer.self_attn.v_proj.out_features, dtype=torch.bfloat16, device=layer.self_attn.v_proj.weight.device))\n",
    "                layer.self_attn.v_proj.bias.requires_grad = True\n",
    "                trainable_biases += layer.self_attn.v_proj.bias.numel()\n",
    "            if hasattr(layer.self_attn, 'o_proj'):\n",
    "                if layer.self_attn.o_proj.bias is None:\n",
    "                    layer.self_attn.o_proj.bias = nn.Parameter(torch.zeros(layer.self_attn.o_proj.out_features, dtype=torch.bfloat16, device=layer.self_attn.o_proj.weight.device))\n",
    "                layer.self_attn.o_proj.bias.requires_grad = True\n",
    "                trainable_biases += layer.self_attn.o_proj.bias.numel()\n",
    "            if hasattr(layer, 'mlp'):\n",
    "                if hasattr(layer.mlp, 'gate_proj'):\n",
    "                    if layer.mlp.gate_proj.bias is None:\n",
    "                        layer.mlp.gate_proj.bias = nn.Parameter(torch.zeros(layer.mlp.gate_proj.out_features, dtype=torch.bfloat16, device=layer.mlp.gate_proj.weight.device))\n",
    "                    layer.mlp.gate_proj.bias.requires_grad = True\n",
    "                    trainable_biases += layer.mlp.gate_proj.bias.numel()\n",
    "                if hasattr(layer.mlp, 'up_proj'):\n",
    "                    if layer.mlp.up_proj.bias is None:\n",
    "                        layer.mlp.up_proj.bias = nn.Parameter(torch.zeros(layer.mlp.up_proj.out_features, dtype=torch.bfloat16, device=layer.mlp.up_proj.weight.device))\n",
    "                    layer.mlp.up_proj.bias.requires_grad = True\n",
    "                    trainable_biases += layer.mlp.up_proj.bias.numel()\n",
    "                if hasattr(layer.mlp, 'down_proj'):\n",
    "                    if layer.mlp.down_proj.bias is None:\n",
    "                        layer.mlp.down_proj.bias = nn.Parameter(torch.zeros(layer.mlp.down_proj.out_features, dtype=torch.bfloat16, device=layer.mlp.down_proj.weight.device))\n",
    "                    layer.mlp.down_proj.bias.requires_grad = True\n",
    "                    trainable_biases += layer.mlp.down_proj.bias.numel()\n",
    "        if hasattr(self.llm, 'lm_head'):\n",
    "            if self.llm.lm_head.bias is None:\n",
    "                self.llm.lm_head.bias = nn.Parameter(torch.zeros(self.llm.lm_head.out_features, dtype=torch.bfloat16, device=self.llm.lm_head.weight.device))\n",
    "            self.llm.lm_head.bias.requires_grad = True\n",
    "            trainable_biases += self.llm.lm_head.bias.numel()\n",
    "        print(f\"BitFit training enabled: {trainable_biases} bias parameters\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                loss = None\n",
    "        return {'logits': outputs.logits, 'loss': loss, 'hidden_states': outputs.hidden_states}\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Initial RAM Usage: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    virtual_mem = psutil.virtual_memory()\n",
    "    print(f\"Total System RAM: {virtual_mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {virtual_mem.available / 1024**3:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.llm.parameters() if p.requires_grad)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PARAMETER STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Bias Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / total_params * 100):.4f}%\")\n",
    "    if trainable_params > 0:\n",
    "        print(f\"Parameter Efficiency: {total_params / trainable_params:.2f}x reduction\")\n",
    "    print(f\"BitFit Mode: {model.bitfit_config.bias_terms}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"GPU MEMORY ALLOCATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.2f} GB total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_and_process_data(csv_path, tokenizer, max_length=384, java_col='java', csharp_col='C#'):\n",
    "    print(f\"Loading dataset from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset loaded: {len(df)} samples\\n\")\n",
    "    data_list = []\n",
    "    skipped = 0\n",
    "    print(\"Processing dataset...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            java_code = str(row[java_col]).strip()\n",
    "            csharp_code = str(row[csharp_col]).strip()\n",
    "            if len(java_code) < 5 or len(csharp_code) < 5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "            full_text = f\"{prompt} {csharp_code}\"\n",
    "            inputs = tokenizer(\n",
    "                full_text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            labels = inputs['input_ids'].clone()\n",
    "            prompt_tokens = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "            prompt_length = prompt_tokens.shape[1]\n",
    "            labels[0, :prompt_length] = -100\n",
    "            data_list.append({\n",
    "                'input_ids': inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "                'labels': labels.squeeze(0),\n",
    "                'java_code': java_code,\n",
    "                'csharp_code': csharp_code\n",
    "            })\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    print(f\"Processing complete: {len(data_list)} valid samples, {skipped} skipped\\n\")\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis, max_n=4):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)])\n",
    "        hyp_ngrams = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens) - n + 1)])\n",
    "        matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "        total = sum(hyp_ngrams.values())\n",
    "        precision = matches / total if total > 0 else 0.0\n",
    "        precisions.append(precision)\n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0.0\n",
    "    weights = [1.0 / max_n] * max_n\n",
    "    log_precisions = [w * math.log(p) if p > 0 else float('-inf') for w, p in zip(weights, precisions)]\n",
    "    if any(lp == float('-inf') for lp in log_precisions):\n",
    "        return 0.0\n",
    "    geometric_mean = math.exp(sum(log_precisions))\n",
    "    brevity_penalty = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0\n",
    "    return brevity_penalty * geometric_mean\n",
    "\n",
    "\n",
    "def calculate_meteor_score(reference, hypothesis):\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    hyp_tokens = set(hypothesis.lower().split())\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    matches = len(ref_tokens & hyp_tokens)\n",
    "    precision = matches / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f_mean = (10 * precision * recall) / (9 * precision + recall)\n",
    "    return f_mean\n",
    "\n",
    "\n",
    "def calculate_rouge_l(reference, hypothesis):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(ref_tokens) == 0 or len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    lcs_length = 0\n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs_length = dp[m][n]\n",
    "    precision = lcs_length / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = lcs_length / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_exact_match(reference, hypothesis):\n",
    "    return 1.0 if reference.strip() == hypothesis.strip() else 0.0\n",
    "\n",
    "\n",
    "def calculate_chrf_score(reference, hypothesis, n=6):\n",
    "    def get_char_ngrams(text, n):\n",
    "        chars = list(text)\n",
    "        ngrams = []\n",
    "        for i in range(len(chars) - n + 1):\n",
    "            ngrams.append(''.join(chars[i:i+n]))\n",
    "        return ngrams\n",
    "    ref_ngrams = Counter(get_char_ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(get_char_ngrams(hypothesis, n))\n",
    "    if len(hyp_ngrams) == 0:\n",
    "        return 0.0\n",
    "    matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    precision = matches / sum(hyp_ngrams.values()) if sum(hyp_ngrams.values()) > 0 else 0.0\n",
    "    recall = matches / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def extract_syntax_features(code):\n",
    "    features = {\n",
    "        'methods': len(re.findall(r'\\b(public|private|protected|static)\\s+\\w+\\s+\\w+\\s*\\(', code)),\n",
    "        'classes': len(re.findall(r'\\bclass\\s+\\w+', code)),\n",
    "        'if_statements': len(re.findall(r'\\bif\\s*\\(', code)),\n",
    "        'for_loops': len(re.findall(r'\\bfor\\s*\\(', code)),\n",
    "        'while_loops': len(re.findall(r'\\bwhile\\s*\\(', code)),\n",
    "        'return_statements': len(re.findall(r'\\breturn\\b', code)),\n",
    "        'variables': len(re.findall(r'\\b\\w+\\s*=\\s*', code)),\n",
    "        'try_catch': len(re.findall(r'\\btry\\s*\\{', code)),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_syntax_similarity(ref_features, hyp_features):\n",
    "    if not ref_features or not hyp_features:\n",
    "        return 0.0\n",
    "    total_diff = 0\n",
    "    total_count = 0\n",
    "    for key in ref_features:\n",
    "        ref_val = ref_features[key]\n",
    "        hyp_val = hyp_features.get(key, 0)\n",
    "        max_val = max(ref_val, hyp_val)\n",
    "        if max_val > 0:\n",
    "            similarity = 1.0 - abs(ref_val - hyp_val) / max_val\n",
    "            total_diff += similarity\n",
    "            total_count += 1\n",
    "    return total_diff / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_code_similarity_metrics(reference, hypothesis):\n",
    "    metrics = {}\n",
    "    metrics['bleu_1'] = calculate_bleu_score(reference, hypothesis, max_n=1)\n",
    "    metrics['bleu_2'] = calculate_bleu_score(reference, hypothesis, max_n=2)\n",
    "    metrics['bleu_4'] = calculate_bleu_score(reference, hypothesis, max_n=4)\n",
    "    metrics['meteor'] = calculate_meteor_score(reference, hypothesis)\n",
    "    metrics['rouge_l'] = calculate_rouge_l(reference, hypothesis)\n",
    "    metrics['exact_match'] = calculate_exact_match(reference, hypothesis)\n",
    "    metrics['chrf'] = calculate_chrf_score(reference, hypothesis)\n",
    "    ref_features = extract_syntax_features(reference)\n",
    "    hyp_features = extract_syntax_features(hypothesis)\n",
    "    metrics['syntax_similarity'] = calculate_syntax_similarity(ref_features, hyp_features)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def check_compilation(code, language='csharp'):\n",
    "    try:\n",
    "        if language == 'csharp':\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.cs', delete=False) as f:\n",
    "                wrapper = f\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\n\\nnamespace TempCompile {{\\n    public class TempClass {{\\n        {code}\\n    }}\\n}}\"\n",
    "                f.write(wrapper)\n",
    "                temp_file = f.name\n",
    "            result = subprocess.run(\n",
    "                ['csc', '/nologo', '/t:library', '/noconfig', temp_file],\n",
    "                capture_output=True,\n",
    "                timeout=5,\n",
    "                text=True\n",
    "            )\n",
    "            success = result.returncode == 0\n",
    "            try:\n",
    "                os.unlink(temp_file)\n",
    "                dll_file = temp_file.replace('.cs', '.dll')\n",
    "                if os.path.exists(dll_file):\n",
    "                    os.unlink(dll_file)\n",
    "            except:\n",
    "                pass\n",
    "            return success\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch]),\n",
    "        'java_code': [item['java_code'] for item in batch],\n",
    "        'csharp_code': [item['csharp_code'] for item in batch]\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Number of Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Total Training Samples: {len(train_data)}\")\n",
    "    print(f\"Total Batches per Epoch: {len(train_data) // batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    bias_params = [p for p in model.llm.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.AdamW(bias_params, lr=learning_rate, weight_decay=0.01, eps=1e-8)\n",
    "    dataset = CodeTranslationDataset(train_data)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 80)\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].cuda()\n",
    "                attention_mask = batch['attention_mask'].cuda()\n",
    "                labels = batch['labels'].cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(bias_params, max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                    if (batch_idx + 1) % 100 == 0:\n",
    "                        avg_loss = total_loss / valid_batches\n",
    "                        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "                if (batch_idx + 1) % 500 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Batch {batch_idx + 1} failed - {str(e)}\")\n",
    "                continue\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0\n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Valid Batches: {valid_batches}/{len(dataloader)}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average Time per Epoch: {total_time/num_epochs:.2f}s\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, batch_size=1, max_new_tokens=256):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PHASE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test Samples: {len(test_data)}\")\n",
    "    print(f\"Max Generation Length: {max_new_tokens}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    model.eval()\n",
    "    all_metrics = {\n",
    "        'bleu_1': [], 'bleu_2': [], 'bleu_4': [],\n",
    "        'meteor': [], 'rouge_l': [], 'exact_match': [],\n",
    "        'chrf': [], 'syntax_similarity': []\n",
    "    }\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    compilation_success = 0\n",
    "    start_time = time.time()\n",
    "    total_tokens = 0\n",
    "    print(\"Generating translations...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(test_data):\n",
    "            try:\n",
    "                java_code = item['java_code']\n",
    "                csharp_code = item['csharp_code']\n",
    "                prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "                inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=384)\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                outputs = model.llm.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                hypothesis = generated_text[len(prompt):].strip()\n",
    "                references.append(csharp_code)\n",
    "                hypotheses.append(hypothesis)\n",
    "                metrics = calculate_code_similarity_metrics(csharp_code, hypothesis)\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(metrics[key])\n",
    "                if check_compilation(hypothesis):\n",
    "                    compilation_success += 1\n",
    "                total_tokens += len(outputs[0])\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    print(f\"  Processed {idx + 1}/{len(test_data)} samples\")\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Sample {idx + 1} failed - {str(e)}\")\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(0.0)\n",
    "                references.append(\"\")\n",
    "                hypotheses.append(\"\")\n",
    "                continue\n",
    "    inference_time = time.time() - start_time\n",
    "    results = {\n",
    "        'bleu_1': np.mean(all_metrics['bleu_1']),\n",
    "        'bleu_1_std': np.std(all_metrics['bleu_1']),\n",
    "        'bleu_2': np.mean(all_metrics['bleu_2']),\n",
    "        'bleu_2_std': np.std(all_metrics['bleu_2']),\n",
    "        'bleu_4': np.mean(all_metrics['bleu_4']),\n",
    "        'bleu_4_std': np.std(all_metrics['bleu_4']),\n",
    "        'meteor': np.mean(all_metrics['meteor']),\n",
    "        'meteor_std': np.std(all_metrics['meteor']),\n",
    "        'rouge_l': np.mean(all_metrics['rouge_l']),\n",
    "        'rouge_l_std': np.std(all_metrics['rouge_l']),\n",
    "        'exact_match': np.mean(all_metrics['exact_match']),\n",
    "        'chrf': np.mean(all_metrics['chrf']),\n",
    "        'chrf_std': np.std(all_metrics['chrf']),\n",
    "        'syntax_similarity': np.mean(all_metrics['syntax_similarity']),\n",
    "        'syntax_similarity_std': np.std(all_metrics['syntax_similarity']),\n",
    "        'compilation_rate': (compilation_success / len(test_data)) * 100,\n",
    "        'inference_time': inference_time,\n",
    "        'throughput': len(test_data) / inference_time,\n",
    "        'tokens_per_sec': total_tokens / inference_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_sample': total_tokens / len(test_data)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"N-GRAM BASED METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"BLEU-1: {results['bleu_1']:.4f} (±{results['bleu_1_std']:.4f})\")\n",
    "    print(f\"BLEU-2: {results['bleu_2']:.4f} (±{results['bleu_2_std']:.4f})\")\n",
    "    print(f\"BLEU-4: {results['bleu_4']:.4f} (±{results['bleu_4_std']:.4f})\")\n",
    "    print(f\"chrF: {results['chrf']:.4f} (±{results['chrf_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SEMANTIC SIMILARITY METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f} (±{results['meteor_std']:.4f})\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f} (±{results['rouge_l_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CODE-SPECIFIC METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Syntax Similarity: {results['syntax_similarity']:.4f} (±{results['syntax_similarity_std']:.4f})\")\n",
    "    print(f\"Exact Match: {results['exact_match']:.4f}\")\n",
    "    print(f\"Compilation Success Rate: {results['compilation_rate']:.2f}%\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Inference Time: {results['inference_time']:.2f}s ({results['inference_time']/60:.2f} min)\")\n",
    "    print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
    "    print(f\"Token Generation Rate: {results['tokens_per_sec']:.2f} tokens/sec\")\n",
    "    print(f\"Total Tokens Generated: {results['total_tokens']}\")\n",
    "    print(f\"Average Tokens per Sample: {results['avg_tokens_per_sample']:.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"JAVA TO C# CODE TRANSLATION\")\n",
    "    print(\"Using BitFit-Enhanced Mistral-7B\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print_system_info()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    cache_dir = '/hf_cache'\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded\\n\")\n",
    "    bitfit_config = BitFitConfig(bias_terms='all')\n",
    "    print(\"Initializing model...\")\n",
    "    model = BitFitMistral(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        bitfit_config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = load_and_process_data(train_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data loaded. Exiting.\")\n",
    "        return\n",
    "    train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TEST DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = load_and_process_data(test_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(test_data) == 0:\n",
    "        print(\"ERROR: No test data loaded. Exiting.\")\n",
    "        return\n",
    "    results = evaluate_model(model, tokenizer, test_data, batch_size=1)\n",
    "    print_evaluation_results(results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b2d88-da75-4e1e-aece-ff9fba8efc26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031364ad-91de-427c-aa76-a519139012d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "\n",
    "class PrefixTuningConfig:\n",
    "    def __init__(self, prefix_length=10, num_layers=32, hidden_size=4096, num_attention_heads=32):\n",
    "        self.prefix_length = prefix_length\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class PrefixEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.prefix_length = config.prefix_length\n",
    "        self.num_layers = config.num_layers\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        self.prefix_tokens = nn.Parameter(torch.randn(self.prefix_length, self.hidden_size))\n",
    "        self.prefix_mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch_size):\n",
    "        prefix_embeds = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        prefix_embeds = self.prefix_mlp(prefix_embeds)\n",
    "        return prefix_embeds.to(torch.bfloat16)\n",
    "\n",
    "class PrefixTuningMistral(nn.Module):\n",
    "    def __init__(self, model_name: str, prefix_config: PrefixTuningConfig, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nInitializing model across {num_gpus} GPUs\")\n",
    "        for i in range(num_gpus):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name} - {props.total_memory / 1024**3:.2f} GB\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(f\"\\nLoading base model: {model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            cache_dir=cache_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        self.prefix_config = prefix_config\n",
    "        num_layers = len(base_model.model.layers)\n",
    "        print(f\"Model has {num_layers} transformer layers\")\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.prefix_encoder = PrefixEncoder(prefix_config).to(self.primary_device)\n",
    "        self.prefix_encoder = self.prefix_encoder.to(torch.bfloat16)  # ADD THIS LINE\n",
    "        print(f\"Prefix encoder initialized with {self.prefix_config.prefix_length} prefix tokens\")\n",
    "        print(\"Model initialization complete!\\n\")\n",
    "    \n",
    "    def get_prompt_embedding(self, input_ids):\n",
    "        inputs_embeds = self.llm.get_input_embeddings()(input_ids)\n",
    "        return inputs_embeds\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        prefix_embeds = self.prefix_encoder(batch_size).to(input_ids.device)\n",
    "        inputs_embeds = self.get_prompt_embedding(input_ids)\n",
    "        inputs_embeds = torch.cat([prefix_embeds, inputs_embeds], dim=1)\n",
    "        \n",
    "        prefix_attention_mask = torch.ones(batch_size, self.prefix_config.prefix_length, device=input_ids.device, dtype=attention_mask.dtype if attention_mask is not None else torch.long)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        else:\n",
    "            attention_mask = torch.cat([prefix_attention_mask, torch.ones(batch_size, input_ids.size(1), device=input_ids.device)], dim=1)\n",
    "        \n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            logits = outputs.logits[:, self.prefix_config.prefix_length:, :]\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                loss = None\n",
    "        \n",
    "        return {'logits': outputs.logits, 'loss': loss, 'hidden_states': outputs.hidden_states}\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Initial RAM Usage: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    virtual_mem = psutil.virtual_memory()\n",
    "    print(f\"Total System RAM: {virtual_mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {virtual_mem.available / 1024**3:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.prefix_encoder.parameters())\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PARAMETER STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Prefix Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / (total_params + trainable_params) * 100):.4f}%\")\n",
    "    if trainable_params > 0:\n",
    "        print(f\"Parameter Efficiency: {total_params / trainable_params:.2f}x reduction\")\n",
    "    print(f\"Prefix Length: {model.prefix_config.prefix_length}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"GPU MEMORY ALLOCATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.2f} GB total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_and_process_data(csv_path, tokenizer, max_length=384, java_col='java', csharp_col='C#'):\n",
    "    print(f\"Loading dataset from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset loaded: {len(df)} samples\\n\")\n",
    "    data_list = []\n",
    "    skipped = 0\n",
    "    print(\"Processing dataset...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            java_code = str(row[java_col]).strip()\n",
    "            csharp_code = str(row[csharp_col]).strip()\n",
    "            if len(java_code) < 5 or len(csharp_code) < 5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "            full_text = f\"{prompt} {csharp_code}\"\n",
    "            inputs = tokenizer(\n",
    "                full_text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            labels = inputs['input_ids'].clone()\n",
    "            prompt_tokens = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "            prompt_length = prompt_tokens.shape[1]\n",
    "            labels[0, :prompt_length] = -100\n",
    "            data_list.append({\n",
    "                'input_ids': inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "                'labels': labels.squeeze(0),\n",
    "                'java_code': java_code,\n",
    "                'csharp_code': csharp_code\n",
    "            })\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    print(f\"Processing complete: {len(data_list)} valid samples, {skipped} skipped\\n\")\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis, max_n=4):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)])\n",
    "        hyp_ngrams = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens) - n + 1)])\n",
    "        matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "        total = sum(hyp_ngrams.values())\n",
    "        precision = matches / total if total > 0 else 0.0\n",
    "        precisions.append(precision)\n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0.0\n",
    "    weights = [1.0 / max_n] * max_n\n",
    "    log_precisions = [w * math.log(p) if p > 0 else float('-inf') for w, p in zip(weights, precisions)]\n",
    "    if any(lp == float('-inf') for lp in log_precisions):\n",
    "        return 0.0\n",
    "    geometric_mean = math.exp(sum(log_precisions))\n",
    "    brevity_penalty = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0\n",
    "    return brevity_penalty * geometric_mean\n",
    "\n",
    "\n",
    "def calculate_meteor_score(reference, hypothesis):\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    hyp_tokens = set(hypothesis.lower().split())\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    matches = len(ref_tokens & hyp_tokens)\n",
    "    precision = matches / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f_mean = (10 * precision * recall) / (9 * precision + recall)\n",
    "    return f_mean\n",
    "\n",
    "\n",
    "def calculate_rouge_l(reference, hypothesis):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(ref_tokens) == 0 or len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    lcs_length = 0\n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs_length = dp[m][n]\n",
    "    precision = lcs_length / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = lcs_length / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_exact_match(reference, hypothesis):\n",
    "    return 1.0 if reference.strip() == hypothesis.strip() else 0.0\n",
    "\n",
    "\n",
    "def calculate_chrf_score(reference, hypothesis, n=6):\n",
    "    def get_char_ngrams(text, n):\n",
    "        chars = list(text)\n",
    "        ngrams = []\n",
    "        for i in range(len(chars) - n + 1):\n",
    "            ngrams.append(''.join(chars[i:i+n]))\n",
    "        return ngrams\n",
    "    ref_ngrams = Counter(get_char_ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(get_char_ngrams(hypothesis, n))\n",
    "    if len(hyp_ngrams) == 0:\n",
    "        return 0.0\n",
    "    matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    precision = matches / sum(hyp_ngrams.values()) if sum(hyp_ngrams.values()) > 0 else 0.0\n",
    "    recall = matches / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def extract_syntax_features(code):\n",
    "    features = {\n",
    "        'methods': len(re.findall(r'\\b(public|private|protected|static)\\s+\\w+\\s+\\w+\\s*\\(', code)),\n",
    "        'classes': len(re.findall(r'\\bclass\\s+\\w+', code)),\n",
    "        'if_statements': len(re.findall(r'\\bif\\s*\\(', code)),\n",
    "        'for_loops': len(re.findall(r'\\bfor\\s*\\(', code)),\n",
    "        'while_loops': len(re.findall(r'\\bwhile\\s*\\(', code)),\n",
    "        'return_statements': len(re.findall(r'\\breturn\\b', code)),\n",
    "        'variables': len(re.findall(r'\\b\\w+\\s*=\\s*', code)),\n",
    "        'try_catch': len(re.findall(r'\\btry\\s*\\{', code)),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_syntax_similarity(ref_features, hyp_features):\n",
    "    if not ref_features or not hyp_features:\n",
    "        return 0.0\n",
    "    total_diff = 0\n",
    "    total_count = 0\n",
    "    for key in ref_features:\n",
    "        ref_val = ref_features[key]\n",
    "        hyp_val = hyp_features.get(key, 0)\n",
    "        max_val = max(ref_val, hyp_val)\n",
    "        if max_val > 0:\n",
    "            similarity = 1.0 - abs(ref_val - hyp_val) / max_val\n",
    "            total_diff += similarity\n",
    "            total_count += 1\n",
    "    return total_diff / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_code_similarity_metrics(reference, hypothesis):\n",
    "    metrics = {}\n",
    "    metrics['bleu_1'] = calculate_bleu_score(reference, hypothesis, max_n=1)\n",
    "    metrics['bleu_2'] = calculate_bleu_score(reference, hypothesis, max_n=2)\n",
    "    metrics['bleu_4'] = calculate_bleu_score(reference, hypothesis, max_n=4)\n",
    "    metrics['meteor'] = calculate_meteor_score(reference, hypothesis)\n",
    "    metrics['rouge_l'] = calculate_rouge_l(reference, hypothesis)\n",
    "    metrics['exact_match'] = calculate_exact_match(reference, hypothesis)\n",
    "    metrics['chrf'] = calculate_chrf_score(reference, hypothesis)\n",
    "    ref_features = extract_syntax_features(reference)\n",
    "    hyp_features = extract_syntax_features(hypothesis)\n",
    "    metrics['syntax_similarity'] = calculate_syntax_similarity(ref_features, hyp_features)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def check_compilation(code, language='csharp'):\n",
    "    try:\n",
    "        if language == 'csharp':\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.cs', delete=False) as f:\n",
    "                wrapper = f\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\n\\nnamespace TempCompile {{\\n    public class TempClass {{\\n        {code}\\n    }}\\n}}\"\n",
    "                f.write(wrapper)\n",
    "                temp_file = f.name\n",
    "            result = subprocess.run(\n",
    "                ['csc', '/nologo', '/t:library', '/noconfig', temp_file],\n",
    "                capture_output=True,\n",
    "                timeout=5,\n",
    "                text=True\n",
    "            )\n",
    "            success = result.returncode == 0\n",
    "            try:\n",
    "                os.unlink(temp_file)\n",
    "                dll_file = temp_file.replace('.cs', '.dll')\n",
    "                if os.path.exists(dll_file):\n",
    "                    os.unlink(dll_file)\n",
    "            except:\n",
    "                pass\n",
    "            return success\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch]),\n",
    "        'java_code': [item['java_code'] for item in batch],\n",
    "        'csharp_code': [item['csharp_code'] for item in batch]\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Number of Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Total Training Samples: {len(train_data)}\")\n",
    "    print(f\"Total Batches per Epoch: {len(train_data) // batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    prefix_params = list(model.prefix_encoder.parameters())\n",
    "    optimizer = torch.optim.AdamW(prefix_params, lr=learning_rate, weight_decay=0.01, eps=1e-8)\n",
    "    dataset = CodeTranslationDataset(train_data)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 80)\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].cuda()\n",
    "                attention_mask = batch['attention_mask'].cuda()\n",
    "                labels = batch['labels'].cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs['loss']\n",
    "                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(prefix_params, max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                    if (batch_idx + 1) % 100 == 0:\n",
    "                        avg_loss = total_loss / valid_batches\n",
    "                        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "                if (batch_idx + 1) % 500 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Batch {batch_idx + 1} failed - {str(e)}\")\n",
    "                continue\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0\n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Valid Batches: {valid_batches}/{len(dataloader)}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average Time per Epoch: {total_time/num_epochs:.2f}s\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, batch_size=1, max_new_tokens=256):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PHASE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test Samples: {len(test_data)}\")\n",
    "    print(f\"Max Generation Length: {max_new_tokens}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    model.eval()\n",
    "    all_metrics = {\n",
    "        'bleu_1': [], 'bleu_2': [], 'bleu_4': [],\n",
    "        'meteor': [], 'rouge_l': [], 'exact_match': [],\n",
    "        'chrf': [], 'syntax_similarity': []\n",
    "    }\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    compilation_success = 0\n",
    "    start_time = time.time()\n",
    "    total_tokens = 0\n",
    "    print(\"Generating translations...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(test_data):\n",
    "            try:\n",
    "                java_code = item['java_code']\n",
    "                csharp_code = item['csharp_code']\n",
    "                prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "                inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=384)\n",
    "                input_ids = inputs['input_ids'].cuda()\n",
    "                attention_mask = inputs['attention_mask'].cuda()\n",
    "                \n",
    "                batch_size_gen = input_ids.size(0)\n",
    "                prefix_embeds = model.prefix_encoder(batch_size_gen).to(input_ids.device)\n",
    "                inputs_embeds = model.get_prompt_embedding(input_ids)\n",
    "                inputs_embeds = torch.cat([prefix_embeds, inputs_embeds], dim=1)\n",
    "                \n",
    "                prefix_attention_mask = torch.ones(batch_size_gen, model.prefix_config.prefix_length, device=input_ids.device, dtype=attention_mask.dtype)\n",
    "                full_attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "                \n",
    "                outputs = model.llm.generate(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask=full_attention_mask,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                hypothesis = generated_text[len(prompt):].strip()\n",
    "                \n",
    "                references.append(csharp_code)\n",
    "                hypotheses.append(hypothesis)\n",
    "                metrics = calculate_code_similarity_metrics(csharp_code, hypothesis)\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(metrics[key])\n",
    "                if check_compilation(hypothesis):\n",
    "                    compilation_success += 1\n",
    "                total_tokens += len(outputs[0])\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    print(f\"  Processed {idx + 1}/{len(test_data)} samples\")\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Sample {idx + 1} failed - {str(e)}\")\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(0.0)\n",
    "                references.append(\"\")\n",
    "                hypotheses.append(\"\")\n",
    "                continue\n",
    "    inference_time = time.time() - start_time\n",
    "    results = {\n",
    "        'bleu_1': np.mean(all_metrics['bleu_1']),\n",
    "        'bleu_1_std': np.std(all_metrics['bleu_1']),\n",
    "        'bleu_2': np.mean(all_metrics['bleu_2']),\n",
    "        'bleu_2_std': np.std(all_metrics['bleu_2']),\n",
    "        'bleu_4': np.mean(all_metrics['bleu_4']),\n",
    "        'bleu_4_std': np.std(all_metrics['bleu_4']),\n",
    "        'meteor': np.mean(all_metrics['meteor']),\n",
    "        'meteor_std': np.std(all_metrics['meteor']),\n",
    "        'rouge_l': np.mean(all_metrics['rouge_l']),\n",
    "        'rouge_l_std': np.std(all_metrics['rouge_l']),\n",
    "        'exact_match': np.mean(all_metrics['exact_match']),\n",
    "        'chrf': np.mean(all_metrics['chrf']),\n",
    "        'chrf_std': np.std(all_metrics['chrf']),\n",
    "        'syntax_similarity': np.mean(all_metrics['syntax_similarity']),\n",
    "        'syntax_similarity_std': np.std(all_metrics['syntax_similarity']),\n",
    "        'compilation_rate': (compilation_success / len(test_data)) * 100,\n",
    "        'inference_time': inference_time,\n",
    "        'throughput': len(test_data) / inference_time,\n",
    "        'tokens_per_sec': total_tokens / inference_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_sample': total_tokens / len(test_data)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"N-GRAM BASED METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"BLEU-1: {results['bleu_1']:.4f} (±{results['bleu_1_std']:.4f})\")\n",
    "    print(f\"BLEU-2: {results['bleu_2']:.4f} (±{results['bleu_2_std']:.4f})\")\n",
    "    print(f\"BLEU-4: {results['bleu_4']:.4f} (±{results['bleu_4_std']:.4f})\")\n",
    "    print(f\"chrF: {results['chrf']:.4f} (±{results['chrf_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SEMANTIC SIMILARITY METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f} (±{results['meteor_std']:.4f})\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f} (±{results['rouge_l_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CODE-SPECIFIC METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Syntax Similarity: {results['syntax_similarity']:.4f} (±{results['syntax_similarity_std']:.4f})\")\n",
    "    print(f\"Exact Match: {results['exact_match']:.4f}\")\n",
    "    print(f\"Compilation Success Rate: {results['compilation_rate']:.2f}%\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Inference Time: {results['inference_time']:.2f}s ({results['inference_time']/60:.2f} min)\")\n",
    "    print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
    "    print(f\"Token Generation Rate: {results['tokens_per_sec']:.2f} tokens/sec\")\n",
    "    print(f\"Total Tokens Generated: {results['total_tokens']}\")\n",
    "    print(f\"Average Tokens per Sample: {results['avg_tokens_per_sample']:.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"JAVA TO C# CODE TRANSLATION\")\n",
    "    print(\"Using Prefix Tuning-Enhanced Mistral-7B\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print_system_info()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    cache_dir = '/hf_cache'\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded\\n\")\n",
    "    prefix_config = PrefixTuningConfig(\n",
    "        prefix_length=10,\n",
    "        num_layers=32,\n",
    "        hidden_size=4096,\n",
    "        num_attention_heads=32\n",
    "    )\n",
    "    print(\"Initializing model...\")\n",
    "    model = PrefixTuningMistral(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        prefix_config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = load_and_process_data(train_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data loaded. Exiting.\")\n",
    "        return\n",
    "    train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=6)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TEST DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = load_and_process_data(test_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(test_data) == 0:\n",
    "        print(\"ERROR: No test data loaded. Exiting.\")\n",
    "        return\n",
    "    results = evaluate_model(model, tokenizer, test_data, batch_size=4)\n",
    "    print_evaluation_results(results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f384ac-c14d-4dea-87fb-a137e376a8d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# adpater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ccdf6-b9fa-43b2-9089-cbbcb835bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "\n",
    "class AdapterConfig:\n",
    "    def __init__(self, adapter_size=64, num_layers=32, hidden_size=4096):\n",
    "        self.adapter_size = adapter_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.adapter_dropout = 0.1\n",
    "\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class AdapterLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, adapter_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(hidden_size, adapter_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.up_project = nn.Linear(adapter_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.down_project(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.up_project(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states + residual\n",
    "\n",
    "\n",
    "class AdapterTuningMistral(nn.Module):\n",
    "    def __init__(self, model_name: str, adapter_config: AdapterConfig, cache_dir: str = None):\n",
    "        super().__init__()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nInitializing model across {num_gpus} GPUs\")\n",
    "        for i in range(num_gpus):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name} - {props.total_memory / 1024**3:.2f} GB\")\n",
    "        self.primary_device = 'cuda:0'\n",
    "        print(f\"\\nLoading base model: {model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            cache_dir=cache_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.llm = base_model\n",
    "        self.adapter_config = adapter_config\n",
    "        num_layers = len(base_model.model.layers)\n",
    "        print(f\"Model has {num_layers} transformer layers\")\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.adapters = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            adapter = AdapterLayer(\n",
    "                adapter_config.hidden_size,\n",
    "                adapter_config.adapter_size,\n",
    "                adapter_config.adapter_dropout\n",
    "            )\n",
    "            self.adapters.append(adapter)\n",
    "        device_map = self.llm.hf_device_map if hasattr(self.llm, 'hf_device_map') else {}\n",
    "        for i, adapter in enumerate(self.adapters):\n",
    "            layer_key = f'model.layers.{i}'\n",
    "            if layer_key in device_map:\n",
    "                target_device = device_map[layer_key]\n",
    "            else:\n",
    "                target_device = self.primary_device\n",
    "            adapter.to(target_device).to(torch.bfloat16)\n",
    "        self._register_forward_hooks()\n",
    "        print(f\"Adapters initialized with size {self.adapter_config.adapter_size}\")\n",
    "        print(\"Model initialization complete!\\n\")\n",
    "    \n",
    "    def _register_forward_hooks(self):\n",
    "\n",
    "        def create_hook(adapter_idx):\n",
    "            def hook(module, input, output):\n",
    "                hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "                adapter = self.adapters[adapter_idx]\n",
    "                adapter_device = next(adapter.parameters()).device\n",
    "                hidden_states = hidden_states.to(adapter_device)\n",
    "                adapted = adapter(hidden_states)\n",
    "                if isinstance(output, tuple):\n",
    "                    return (adapted,) + output[1:]\n",
    "                return adapted\n",
    "            return hook\n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            layer.register_forward_hook(create_hook(i))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.llm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def print_system_info():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Initial RAM Usage: {mem_info.rss / 1024**3:.2f} GB\")\n",
    "    virtual_mem = psutil.virtual_memory()\n",
    "    print(f\"Total System RAM: {virtual_mem.total / 1024**3:.2f} GB\")\n",
    "    print(f\"Available RAM: {virtual_mem.available / 1024**3:.2f} GB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def print_parameter_statistics(model):\n",
    "    total_params = sum(p.numel() for p in model.llm.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.adapters.parameters())\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PARAMETER STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total LLM Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Adapter Parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage: {(trainable_params / (total_params + trainable_params) * 100):.4f}%\")\n",
    "    if trainable_params > 0:\n",
    "        print(f\"Parameter Efficiency: {total_params / trainable_params:.2f}x reduction\")\n",
    "    print(f\"Adapter Size: {model.adapter_config.adapter_size}\")\n",
    "    print(f\"Number of Adapter Layers: {len(model.adapters)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"GPU MEMORY ALLOCATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.2f} GB total\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def load_and_process_data(csv_path, tokenizer, max_length=384, java_col='java', csharp_col='C#'):\n",
    "    print(f\"Loading dataset from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Dataset loaded: {len(df)} samples\\n\")\n",
    "    data_list = []\n",
    "    skipped = 0\n",
    "    print(\"Processing dataset...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            java_code = str(row[java_col]).strip()\n",
    "            csharp_code = str(row[csharp_col]).strip()\n",
    "            if len(java_code) < 5 or len(csharp_code) < 5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "            full_text = f\"{prompt} {csharp_code}\"\n",
    "            inputs = tokenizer(\n",
    "                full_text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            labels = inputs['input_ids'].clone()\n",
    "            prompt_tokens = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "            prompt_length = prompt_tokens.shape[1]\n",
    "            labels[0, :prompt_length] = -100\n",
    "            data_list.append({\n",
    "                'input_ids': inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "                'labels': labels.squeeze(0),\n",
    "                'java_code': java_code,\n",
    "                'csharp_code': csharp_code\n",
    "            })\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    print(f\"Processing complete: {len(data_list)} valid samples, {skipped} skipped\\n\")\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis, max_n=4):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)])\n",
    "        hyp_ngrams = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens) - n + 1)])\n",
    "        matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "        total = sum(hyp_ngrams.values())\n",
    "        precision = matches / total if total > 0 else 0.0\n",
    "        precisions.append(precision)\n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0.0\n",
    "    weights = [1.0 / max_n] * max_n\n",
    "    log_precisions = [w * math.log(p) if p > 0 else float('-inf') for w, p in zip(weights, precisions)]\n",
    "    if any(lp == float('-inf') for lp in log_precisions):\n",
    "        return 0.0\n",
    "    geometric_mean = math.exp(sum(log_precisions))\n",
    "    brevity_penalty = min(1.0, math.exp(1 - len(ref_tokens) / len(hyp_tokens))) if len(hyp_tokens) > 0 else 0.0\n",
    "    return brevity_penalty * geometric_mean\n",
    "\n",
    "\n",
    "def calculate_meteor_score(reference, hypothesis):\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    hyp_tokens = set(hypothesis.lower().split())\n",
    "    if len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    matches = len(ref_tokens & hyp_tokens)\n",
    "    precision = matches / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f_mean = (10 * precision * recall) / (9 * precision + recall)\n",
    "    return f_mean\n",
    "\n",
    "\n",
    "def calculate_rouge_l(reference, hypothesis):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    if len(ref_tokens) == 0 or len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    lcs_length = 0\n",
    "    m, n = len(ref_tokens), len(hyp_tokens)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == hyp_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs_length = dp[m][n]\n",
    "    precision = lcs_length / len(hyp_tokens) if len(hyp_tokens) > 0 else 0.0\n",
    "    recall = lcs_length / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_exact_match(reference, hypothesis):\n",
    "    return 1.0 if reference.strip() == hypothesis.strip() else 0.0\n",
    "\n",
    "\n",
    "def calculate_chrf_score(reference, hypothesis, n=6):\n",
    "    def get_char_ngrams(text, n):\n",
    "        chars = list(text)\n",
    "        ngrams = []\n",
    "        for i in range(len(chars) - n + 1):\n",
    "            ngrams.append(''.join(chars[i:i+n]))\n",
    "        return ngrams\n",
    "    ref_ngrams = Counter(get_char_ngrams(reference, n))\n",
    "    hyp_ngrams = Counter(get_char_ngrams(hypothesis, n))\n",
    "    if len(hyp_ngrams) == 0:\n",
    "        return 0.0\n",
    "    matches = sum((ref_ngrams & hyp_ngrams).values())\n",
    "    precision = matches / sum(hyp_ngrams.values()) if sum(hyp_ngrams.values()) > 0 else 0.0\n",
    "    recall = matches / sum(ref_ngrams.values()) if sum(ref_ngrams.values()) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def extract_syntax_features(code):\n",
    "    features = {\n",
    "        'methods': len(re.findall(r'\\b(public|private|protected|static)\\s+\\w+\\s+\\w+\\s*\\(', code)),\n",
    "        'classes': len(re.findall(r'\\bclass\\s+\\w+', code)),\n",
    "        'if_statements': len(re.findall(r'\\bif\\s*\\(', code)),\n",
    "        'for_loops': len(re.findall(r'\\bfor\\s*\\(', code)),\n",
    "        'while_loops': len(re.findall(r'\\bwhile\\s*\\(', code)),\n",
    "        'return_statements': len(re.findall(r'\\breturn\\b', code)),\n",
    "        'variables': len(re.findall(r'\\b\\w+\\s*=\\s*', code)),\n",
    "        'try_catch': len(re.findall(r'\\btry\\s*\\{', code)),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_syntax_similarity(ref_features, hyp_features):\n",
    "    if not ref_features or not hyp_features:\n",
    "        return 0.0\n",
    "    total_diff = 0\n",
    "    total_count = 0\n",
    "    for key in ref_features:\n",
    "        ref_val = ref_features[key]\n",
    "        hyp_val = hyp_features.get(key, 0)\n",
    "        max_val = max(ref_val, hyp_val)\n",
    "        if max_val > 0:\n",
    "            similarity = 1.0 - abs(ref_val - hyp_val) / max_val\n",
    "            total_diff += similarity\n",
    "            total_count += 1\n",
    "    return total_diff / total_count if total_count > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_code_similarity_metrics(reference, hypothesis):\n",
    "    metrics = {}\n",
    "    metrics['bleu_1'] = calculate_bleu_score(reference, hypothesis, max_n=1)\n",
    "    metrics['bleu_2'] = calculate_bleu_score(reference, hypothesis, max_n=2)\n",
    "    metrics['bleu_4'] = calculate_bleu_score(reference, hypothesis, max_n=4)\n",
    "    metrics['meteor'] = calculate_meteor_score(reference, hypothesis)\n",
    "    metrics['rouge_l'] = calculate_rouge_l(reference, hypothesis)\n",
    "    metrics['exact_match'] = calculate_exact_match(reference, hypothesis)\n",
    "    metrics['chrf'] = calculate_chrf_score(reference, hypothesis)\n",
    "    ref_features = extract_syntax_features(reference)\n",
    "    hyp_features = extract_syntax_features(hypothesis)\n",
    "    metrics['syntax_similarity'] = calculate_syntax_similarity(ref_features, hyp_features)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def check_compilation(code, language='csharp'):\n",
    "    try:\n",
    "        if language == 'csharp':\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.cs', delete=False) as f:\n",
    "                wrapper = f\"using System;\\nusing System.Collections.Generic;\\nusing System.Linq;\\n\\nnamespace TempCompile {{\\n    public class TempClass {{\\n        {code}\\n    }}\\n}}\"\n",
    "                f.write(wrapper)\n",
    "                temp_file = f.name\n",
    "            result = subprocess.run(\n",
    "                ['csc', '/nologo', '/t:library', '/noconfig', temp_file],\n",
    "                capture_output=True,\n",
    "                timeout=5,\n",
    "                text=True\n",
    "            )\n",
    "            success = result.returncode == 0\n",
    "            try:\n",
    "                os.unlink(temp_file)\n",
    "                dll_file = temp_file.replace('.cs', '.dll')\n",
    "                if os.path.exists(dll_file):\n",
    "                    os.unlink(dll_file)\n",
    "            except:\n",
    "                pass\n",
    "            return success\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch]),\n",
    "        'java_code': [item['java_code'] for item in batch],\n",
    "        'csharp_code': [item['csharp_code'] for item in batch]\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=2):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Number of Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Total Training Samples: {len(train_data)}\")\n",
    "    print(f\"Total Batches per Epoch: {len(train_data) // batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    adapter_params = list(model.adapters.parameters())\n",
    "    optimizer = torch.optim.AdamW(adapter_params, lr=learning_rate, weight_decay=0.01, eps=1e-8)\n",
    "    dataset = CodeTranslationDataset(train_data)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0.0\n",
    "        valid_batches = 0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 80)\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].cuda()\n",
    "                attention_mask = batch['attention_mask'].cuda()\n",
    "                labels = batch['labels'].cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(adapter_params, max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                    if (batch_idx + 1) % 100 == 0:\n",
    "                        avg_loss = total_loss / valid_batches\n",
    "                        print(f\"  Batch {batch_idx + 1}/{len(dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
    "                if (batch_idx + 1) % 500 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Batch {batch_idx + 1} failed - {str(e)}\")\n",
    "                continue\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0\n",
    "        print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Valid Batches: {valid_batches}/{len(dataloader)}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            print(f\"  GPU {i} Memory: {allocated:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average Time per Epoch: {total_time/num_epochs:.2f}s\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, test_data, batch_size=1, max_new_tokens=256):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PHASE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test Samples: {len(test_data)}\")\n",
    "    print(f\"Max Generation Length: {max_new_tokens}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    model.eval()\n",
    "    all_metrics = {\n",
    "        'bleu_1': [], 'bleu_2': [], 'bleu_4': [],\n",
    "        'meteor': [], 'rouge_l': [], 'exact_match': [],\n",
    "        'chrf': [], 'syntax_similarity': []\n",
    "    }\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    compilation_success = 0\n",
    "    start_time = time.time()\n",
    "    total_tokens = 0\n",
    "    print(\"Generating translations...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(test_data):\n",
    "            try:\n",
    "                java_code = item['java_code']\n",
    "                csharp_code = item['csharp_code']\n",
    "                prompt = f\"[INST] Translate this Java code to C#:\\n{java_code}\\n[/INST]\"\n",
    "                inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=384)\n",
    "                input_ids = inputs['input_ids'].cuda()\n",
    "                attention_mask = inputs['attention_mask'].cuda()\n",
    "                outputs = model.llm.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                hypothesis = generated_text[len(prompt):].strip()\n",
    "                references.append(csharp_code)\n",
    "                hypotheses.append(hypothesis)\n",
    "                metrics = calculate_code_similarity_metrics(csharp_code, hypothesis)\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(metrics[key])\n",
    "                if check_compilation(hypothesis):\n",
    "                    compilation_success += 1\n",
    "                total_tokens += len(outputs[0])\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    print(f\"  Processed {idx + 1}/{len(test_data)} samples\")\n",
    "                if (idx + 1) % 100 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Sample {idx + 1} failed - {str(e)}\")\n",
    "                for key in all_metrics:\n",
    "                    all_metrics[key].append(0.0)\n",
    "                references.append(\"\")\n",
    "                hypotheses.append(\"\")\n",
    "                continue\n",
    "    inference_time = time.time() - start_time\n",
    "    results = {\n",
    "        'bleu_1': np.mean(all_metrics['bleu_1']),\n",
    "        'bleu_1_std': np.std(all_metrics['bleu_1']),\n",
    "        'bleu_2': np.mean(all_metrics['bleu_2']),\n",
    "        'bleu_2_std': np.std(all_metrics['bleu_2']),\n",
    "        'bleu_4': np.mean(all_metrics['bleu_4']),\n",
    "        'bleu_4_std': np.std(all_metrics['bleu_4']),\n",
    "        'meteor': np.mean(all_metrics['meteor']),\n",
    "        'meteor_std': np.std(all_metrics['meteor']),\n",
    "        'rouge_l': np.mean(all_metrics['rouge_l']),\n",
    "        'rouge_l_std': np.std(all_metrics['rouge_l']),\n",
    "        'exact_match': np.mean(all_metrics['exact_match']),\n",
    "        'chrf': np.mean(all_metrics['chrf']),\n",
    "        'chrf_std': np.std(all_metrics['chrf']),\n",
    "        'syntax_similarity': np.mean(all_metrics['syntax_similarity']),\n",
    "        'syntax_similarity_std': np.std(all_metrics['syntax_similarity']),\n",
    "        'compilation_rate': (compilation_success / len(test_data)) * 100,\n",
    "        'inference_time': inference_time,\n",
    "        'throughput': len(test_data) / inference_time,\n",
    "        'tokens_per_sec': total_tokens / inference_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_sample': total_tokens / len(test_data)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"N-GRAM BASED METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"BLEU-1: {results['bleu_1']:.4f} (±{results['bleu_1_std']:.4f})\")\n",
    "    print(f\"BLEU-2: {results['bleu_2']:.4f} (±{results['bleu_2_std']:.4f})\")\n",
    "    print(f\"BLEU-4: {results['bleu_4']:.4f} (±{results['bleu_4_std']:.4f})\")\n",
    "    print(f\"chrF: {results['chrf']:.4f} (±{results['chrf_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SEMANTIC SIMILARITY METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"METEOR: {results['meteor']:.4f} (±{results['meteor_std']:.4f})\")\n",
    "    print(f\"ROUGE-L: {results['rouge_l']:.4f} (±{results['rouge_l_std']:.4f})\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CODE-SPECIFIC METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Syntax Similarity: {results['syntax_similarity']:.4f} (±{results['syntax_similarity_std']:.4f})\")\n",
    "    print(f\"Exact Match: {results['exact_match']:.4f}\")\n",
    "    print(f\"Compilation Success Rate: {results['compilation_rate']:.2f}%\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Inference Time: {results['inference_time']:.2f}s ({results['inference_time']/60:.2f} min)\")\n",
    "    print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
    "    print(f\"Token Generation Rate: {results['tokens_per_sec']:.2f} tokens/sec\")\n",
    "    print(f\"Total Tokens Generated: {results['total_tokens']}\")\n",
    "    print(f\"Average Tokens per Sample: {results['avg_tokens_per_sample']:.2f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"JAVA TO C# CODE TRANSLATION\")\n",
    "    print(\"Using Adapter Tuning-Enhanced Mistral-7B\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print_system_info()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    cache_dir = '/hf_cache'\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded\\n\")\n",
    "    adapter_config = AdapterConfig(\n",
    "        adapter_size=64,\n",
    "        num_layers=32,\n",
    "        hidden_size=4096\n",
    "    )\n",
    "    print(\"Initializing model...\")\n",
    "    model = AdapterTuningMistral(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        adapter_config,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    print_parameter_statistics(model)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TRAINING DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    train_csv = '/train.csv'\n",
    "    train_data = load_and_process_data(train_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(train_data) == 0:\n",
    "        print(\"ERROR: No training data loaded. Exiting.\")\n",
    "        return\n",
    "    train_model(model, train_data, num_epochs=5, learning_rate=2e-5, batch_size=6)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LOADING TEST DATA\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    test_csv = '/test.csv'\n",
    "    test_data = load_and_process_data(test_csv, tokenizer, java_col='java', csharp_col='C#')\n",
    "    if len(test_data) == 0:\n",
    "        print(\"ERROR: No test data loaded. Exiting.\")\n",
    "        return\n",
    "    results = evaluate_model(model, tokenizer, test_data, batch_size=4)\n",
    "    print_evaluation_results(results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXECUTION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
