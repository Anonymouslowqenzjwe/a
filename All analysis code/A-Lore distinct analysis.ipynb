{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0d0873-412c-449b-b4ff-75795b3cd4cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# dataset compelxity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09bf010-2555-4bea-a518-350d9c332e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2cae5c-a526-4faf-93e1-ea0457758b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import h_visit, mi_visit\n",
    "import re\n",
    "import warnings\n",
    "from math import pi\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('deep')\n",
    "task_order = ['Vulnerability Detection', 'Code Search', 'Code Translation', 'Clone Detection']\n",
    "task_colors = sns.color_palette('deep', 4)\n",
    "\n",
    "def calculate_ast_depth(code):\n",
    "    code = '' if code is None else str(code)\n",
    "    if not code.strip():\n",
    "        return 0.0\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "        def get_depth(node, depth=0):\n",
    "            depths = [depth]\n",
    "            for child in ast.iter_child_nodes(node):\n",
    "                depths.append(get_depth(child, depth + 1))\n",
    "            return max(depths)\n",
    "        return float(get_depth(tree))\n",
    "    except:\n",
    "        return float(max(1, code.count('\\n') + 1))\n",
    "\n",
    "def calculate_cyclomatic_complexity(code):\n",
    "    code = '' if code is None else str(code)\n",
    "    if not code.strip():\n",
    "        return 1.0\n",
    "    try:\n",
    "        results = cc_visit(code)\n",
    "        if results:\n",
    "            return float(sum(item.complexity for item in results) / len(results))\n",
    "    except:\n",
    "        pass\n",
    "    decisions = len(re.findall(r'\\b(if|elif|else|for|while|case|catch|switch|and|or)\\b', code, re.I))\n",
    "    ternary = len(re.findall(r'\\?', code))\n",
    "    logical_ops = len(re.findall(r'(&&|\\|\\|)', code))\n",
    "    return float(max(1, 1 + decisions + ternary + logical_ops))\n",
    "\n",
    "def calculate_halstead_metrics(code):\n",
    "    code = '' if code is None else str(code)\n",
    "    if not code.strip():\n",
    "        return {'volume': 0.0, 'difficulty': 0.0, 'effort': 0.0}\n",
    "    try:\n",
    "        result = h_visit(code)\n",
    "        return {\n",
    "            'volume': float(result.total.volume or 0),\n",
    "            'difficulty': float(result.total.difficulty or 0),\n",
    "            'effort': float(result.total.effort or 0)\n",
    "        }\n",
    "    except:\n",
    "        ops = re.findall(r'[\\+\\-\\*\\/\\=\\<\\>\\!\\&\\|\\^%~]', code)\n",
    "        ids = re.findall(r'\\b[A-Za-z_]\\w*\\b', code)\n",
    "        n1 = len(set(ops)) or 1\n",
    "        n2 = len(set(ids)) or 1\n",
    "        N1 = len(ops) or 1\n",
    "        N2 = len(ids) or 1\n",
    "        n = n1 + n2\n",
    "        N = N1 + N2\n",
    "        vol = N * np.log2(n) if n > 1 else N\n",
    "        diff = (n1 / 2.0) * (N2 / n2) if n2 > 0 else 1.0\n",
    "        return {'volume': float(vol), 'difficulty': float(diff), 'effort': float(vol * diff)}\n",
    "\n",
    "def count_operators_operands(code):\n",
    "    code = '' if code is None else str(code)\n",
    "    ops = len(re.findall(r'[\\+\\-\\*\\/\\=\\<\\>\\!\\&\\|\\^%~]', code))\n",
    "    words = re.findall(r'\\b\\w+\\b', code)\n",
    "    keywords = {'if','elif','else','for','while','return','def','class','import','from','try','except','finally','with','as','lambda','yield','in','not','and','or'}\n",
    "    operands = len([w for w in words if w not in keywords])\n",
    "    return float(ops), float(operands)\n",
    "\n",
    "def calculate_maintainability_index(code):\n",
    "    code = '' if code is None else str(code)\n",
    "    if not code.strip():\n",
    "        return 100.0\n",
    "    try:\n",
    "        return float(mi_visit(code, True))\n",
    "    except:\n",
    "        return 100.0\n",
    "\n",
    "def analyze_dataframe(df, code_column, keep_label=False, label_column=None):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        code = row.get(code_column, '')\n",
    "        if pd.isna(code) or not str(code).strip():\n",
    "            continue\n",
    "        ast_depth = calculate_ast_depth(code)\n",
    "        cyclomatic = calculate_cyclomatic_complexity(code)\n",
    "        halstead = calculate_halstead_metrics(code)\n",
    "        operators, operands = count_operators_operands(code)\n",
    "        maintainability = calculate_maintainability_index(code)\n",
    "        loc = len(str(code).split('\\n'))\n",
    "        entry = {\n",
    "            'ast_depth': ast_depth,\n",
    "            'cyclomatic': cyclomatic,\n",
    "            'halstead_volume': halstead['volume'],\n",
    "            'halstead_difficulty': halstead['difficulty'],\n",
    "            'halstead_effort': halstead['effort'],\n",
    "            'operators': operators,\n",
    "            'operands': operands,\n",
    "            'maintainability': maintainability,\n",
    "            'loc': float(loc)\n",
    "        }\n",
    "        if keep_label and label_column in df.columns:\n",
    "            entry['label'] = row[label_column]\n",
    "        rows.append(entry)\n",
    "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n",
    "\n",
    "def analyze_vulnerability_detection(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return analyze_dataframe(df, 'func', keep_label=True, label_column='label')\n",
    "\n",
    "def analyze_code_search(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return analyze_dataframe(df, 'code')\n",
    "\n",
    "def analyze_code_translation(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return analyze_dataframe(df, 'java')\n",
    "\n",
    "def analyze_clone_detection(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return analyze_dataframe(df, 'func1', keep_label=True, label_column='label')\n",
    "\n",
    "def build_task_dataframe():\n",
    "    paths = {\n",
    "        'Vulnerability Detection': '/traincodex.csv',\n",
    "        'Code Search': '/train (1).csv',\n",
    "        'Code Translation': '/train.csv',\n",
    "        'Clone Detection': 'train.csv'\n",
    "    }\n",
    "    frames = []\n",
    "    for task, path in paths.items():\n",
    "        analyzer = {\n",
    "            'Vulnerability Detection': analyze_vulnerability_detection,\n",
    "            'Code Search': analyze_code_search,\n",
    "            'Code Translation': analyze_code_translation,\n",
    "            'Clone Detection': analyze_clone_detection\n",
    "        }[task]\n",
    "        df = analyzer(path)\n",
    "        if not df.empty:\n",
    "            df['task'] = task\n",
    "            frames.append(df)\n",
    "    all_df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    metric_cols = ['ast_depth', 'cyclomatic', 'halstead_volume', 'halstead_difficulty',\n",
    "                   'halstead_effort', 'operators', 'operands', 'maintainability', 'loc']\n",
    "    for col in metric_cols:\n",
    "        if col in all_df.columns:\n",
    "            all_df[col] = pd.to_numeric(all_df[col], errors='coerce')\n",
    "    return all_df\n",
    "\n",
    "def create_summary_table(all_df):\n",
    "    metrics = ['loc', 'ast_depth', 'cyclomatic', 'halstead_volume', 'halstead_difficulty', 'maintainability']\n",
    "    stats = all_df.groupby('task')[metrics].agg(['mean', 'median', 'std']).round(2)\n",
    "    stats.columns = ['_'.join(col) for col in stats.columns]\n",
    "    stats.to_csv('dataset_metrics_summary.csv')\n",
    "    print(\"\\n=== Summary Statistics per Task ===\")\n",
    "    print(stats)\n",
    "\n",
    "def create_kde_distributions(all_df):\n",
    "    metrics = ['loc', 'cyclomatic', 'ast_depth', 'halstead_volume']\n",
    "    log_metrics = ['loc', 'halstead_volume']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Kernel Density Estimates of Code Metrics across PEFT Tasks', fontsize=20, fontweight='bold')\n",
    "    for ax, metric in zip(axes.flat, metrics):\n",
    "        for i, task in enumerate(task_order):\n",
    "            data = all_df[all_df['task'] == task][metric].dropna()\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            sns.kdeplot(data, ax=ax, fill=True, alpha=0.4, label=task, color=task_colors[i], log_scale=(metric in log_metrics))\n",
    "            mean_val = data.mean()\n",
    "            median_val = data.median()\n",
    "            ax.axvline(mean_val, color=task_colors[i], linestyle='--', linewidth=1.5)\n",
    "            ax.text(mean_val, ax.get_ylim()[1]*0.9, f'Mean: {mean_val:.1f}', color=task_colors[i], fontsize=10, ha='left')\n",
    "            ax.axvline(median_val, color=task_colors[i], linestyle=':', linewidth=1.5)\n",
    "            ax.text(median_val, ax.get_ylim()[1]*0.8, f'Median: {median_val:.1f}', color=task_colors[i], fontsize=10, ha='left')\n",
    "        ax.set_xlabel(metric.replace('_', ' ').title(), fontsize=14)\n",
    "        ax.set_ylabel('Density', fontsize=14)\n",
    "        ax.legend(title='Task', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_kde_distributions.png', dpi=400, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_violin_plots(all_df):\n",
    "    metrics = ['cyclomatic', 'ast_depth', 'halstead_difficulty', 'maintainability']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Violin Plots with Mean/Median Annotations across PEFT Tasks', fontsize=20, fontweight='bold')\n",
    "    for ax, metric in zip(axes.flat, metrics):\n",
    "        sns.violinplot(data=all_df, x='task', y=metric, order=task_order, ax=ax, inner='quartile', cut=0)\n",
    "        for i, task in enumerate(task_order):\n",
    "            data = all_df[all_df['task'] == task][metric].dropna()\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            mean_val = data.mean()\n",
    "            median_val = data.median()\n",
    "            ax.text(i, mean_val, f'{mean_val:.1f}', ha='center', va='bottom', fontweight='bold', color='black', fontsize=10)\n",
    "            ax.text(i, median_val, f'{median_val:.1f}', ha='center', va='top', color='white', fontweight='bold', fontsize=10)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=14)\n",
    "        ax.tick_params(axis='x', rotation=12)\n",
    "        ax.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_violin_plots.png', dpi=400, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_regression_scatters(all_df):\n",
    "    pairs = [('loc', 'cyclomatic'), ('loc', 'halstead_volume'), ('cyclomatic', 'ast_depth'), ('halstead_difficulty', 'maintainability')]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    fig.suptitle('Key Metric Correlations with Regression Lines and Stats per Task', fontsize=20, fontweight='bold')\n",
    "    for ax, (x, y) in zip(axes.flat, pairs):\n",
    "        subset = all_df[[x, y, 'task']].dropna()\n",
    "        if len(subset) > 15000:\n",
    "            subset = subset.sample(15000, random_state=42)\n",
    "        sns.scatterplot(data=subset, x=x, y=y, hue='task', hue_order=task_order, alpha=0.5, s=30, ax=ax, edgecolor=None)\n",
    "        sns.regplot(data=subset, x=x, y=y, scatter=False, ax=ax, ci=95, truncate=False, color='black', line_kws={'linewidth':1.5})\n",
    "        y_pos = 0.95\n",
    "        for task in task_order:\n",
    "            task_data = subset[subset['task'] == task]\n",
    "            if len(task_data) < 10:\n",
    "                continue\n",
    "            x_vals = task_data[x]\n",
    "            y_vals = task_data[y]\n",
    "            if x_vals.nunique() <= 1 or y_vals.nunique() <= 1:\n",
    "                ax.text(0.05, y_pos, f'{task}: Insufficient variation for regression', \n",
    "                        transform=ax.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "            else:\n",
    "                slope, intercept, r, p, se = stats.linregress(x_vals, y_vals)\n",
    "                r2 = r**2\n",
    "                ax.text(0.05, y_pos, f'{task}: Slope={slope:.2f}, R²={r2:.3f}, p={p:.2e}',\n",
    "                        transform=ax.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.8), \n",
    "                        color=task_colors[task_order.index(task)])\n",
    "            y_pos -= 0.08\n",
    "        ax.set_xlabel(x.replace('_', ' ').title(), fontsize=14)\n",
    "        ax.set_ylabel(y.replace('_', ' ').title(), fontsize=14)\n",
    "        if x in ['loc', 'halstead_volume'] or y in ['loc', 'halstead_volume']:\n",
    "            if x in ['loc', 'halstead_volume']:\n",
    "                ax.set_xscale('log')\n",
    "            if y in ['loc', 'halstead_volume']:\n",
    "                ax.set_yscale('log')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend().remove()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_regression_scatters.png', dpi=400, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_per_task_correlation_heatmaps(all_df):\n",
    "    metrics = ['loc', 'ast_depth', 'cyclomatic', 'halstead_volume', 'halstead_difficulty', 'operators', 'operands', 'maintainability']\n",
    "    tasks = all_df['task'].unique()\n",
    "    n_tasks = len(tasks)\n",
    "    cols = 2\n",
    "    rows = (n_tasks + 1) // 2\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12 * cols, 10 * rows))\n",
    "    fig.suptitle('Spearman Correlation Matrix of Code Complexity Metrics per Task', fontsize=24, fontweight='bold', y=0.98)\n",
    "    if n_tasks == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1:\n",
    "        axes = np.array([axes]) if cols == 1 else axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    for idx, task in enumerate(tasks):\n",
    "        ax = axes[idx]\n",
    "        task_df = all_df[all_df['task'] == task][metrics].dropna()\n",
    "        if len(task_df) < 2:\n",
    "            ax.text(0.5, 0.5, 'Insufficient data for correlation', ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "        else:\n",
    "            corr = task_df.corr(method='spearman').round(2)\n",
    "            sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, square=True, linewidths=0.5,\n",
    "                        ax=ax, cbar_kws={\"shrink\": 0.8}, fmt='.2f')\n",
    "        ax.set_title(task, fontsize=18, fontweight='bold', pad=20)\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig('metrics_per_task_correlation_heatmaps.png', dpi=400, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_radar_chart(all_df):\n",
    "    metrics = ['loc', 'ast_depth', 'cyclomatic', 'halstead_volume', 'halstead_difficulty', 'maintainability']\n",
    "    means = all_df.groupby('task')[metrics].mean()\n",
    "    means_norm = (means - means.min()) / (means.max() - means.min() + 1e-8)\n",
    "    categories = [m.replace('_', ' ').title() for m in metrics]\n",
    "    N = len(categories)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    fig, ax = plt.subplots(figsize=(11, 11), subplot_kw=dict(polar=True))\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    plt.xticks(angles[:-1], categories, size=13)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], size=10)\n",
    "    for i, task in enumerate(task_order):\n",
    "        if task not in means_norm.index:\n",
    "            continue\n",
    "        values = means_norm.loc[task].tolist() + [means_norm.loc[task].iloc[0]]\n",
    "        ax.plot(angles, values, linewidth=3.5, linestyle='solid', label=task, color=task_colors[i])\n",
    "        ax.fill(angles, values, alpha=0.3, color=task_colors[i])\n",
    "        for j in range(N):\n",
    "            val = means.loc[task, metrics[j]]\n",
    "            ax.text(angles[j], values[j] + 0.05, f'{val:.1f}', ha='center', va='center', fontsize=9, color=task_colors[i], fontweight='bold')\n",
    "    ax.set_title('Normalized Average Complexity Profile across PEFT Tasks\\n(with raw mean values annotated)', size=20, fontweight='bold', pad=40)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=12)\n",
    "    plt.savefig('metrics_radar_chart.png', dpi=400, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_ecdf_plots(all_df):\n",
    "    metrics = ['loc', 'cyclomatic', 'ast_depth', 'halstead_volume']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Empirical Cumulative Distribution Functions with Median Annotations', fontsize=20, fontweight='bold')\n",
    "    for ax, metric in zip(axes.flat, metrics):\n",
    "        for i, task in enumerate(task_order):\n",
    "            data = all_df[all_df['task'] == task][metric].dropna().sort_values()\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            sns.ecdfplot(data=data, ax=ax, label=task, color=task_colors[i], log_scale=(metric in ['loc', 'halstead_volume']))\n",
    "            median_val = data.median()\n",
    "            ax.axvline(median_val, color=task_colors[i], linestyle='--', alpha=0.7)\n",
    "            ax.text(median_val, 0.5, f'{task} Median: {median_val:.1f}', rotation=90, va='center', color=task_colors[i], fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "        ax.set_xlabel(metric.replace('_', ' ').title(), fontsize=14)\n",
    "        ax.set_ylabel('Proportion', fontsize=14)\n",
    "        ax.legend(title='Task', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metrics_ecdf.png', dpi=400, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    print('Generating enhanced research-grade visualizations with detailed annotations...')\n",
    "    all_df = build_task_dataframe()\n",
    "    if all_df.empty:\n",
    "        print('No data loaded. Check paths/columns.')\n",
    "        return\n",
    "    print(f'Total valid code samples: {len(all_df)}')\n",
    "    create_summary_table(all_df)\n",
    "    create_kde_distributions(all_df)\n",
    "    print('→ metrics_kde_distributions.png (annotated means/medians)')\n",
    "    create_violin_plots(all_df)\n",
    "    print('→ metrics_violin_plots.png (annotated means/medians)')\n",
    "    create_regression_scatters(all_df)\n",
    "    print('→ metrics_regression_scatters.png (per-task regression stats - fixed)')\n",
    "    create_per_task_correlation_heatmaps(all_df)\n",
    "    print('→ metrics_per_task_correlation_heatmaps.png (separate heatmap per task)')\n",
    "    create_radar_chart(all_df)\n",
    "    print('→ metrics_radar_chart.png (annotated raw means)')\n",
    "    create_ecdf_plots(all_df)\n",
    "    print('→ metrics_ecdf.png (annotated medians)')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb3f48-d5a2-4bda-a771-9f29196119f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d89d11-70ce-40d6-a662-09ef534dfc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, cohen_kappa_score, jaccard_score\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_len=512):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.codes = df['code'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.codes[idx], \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_len, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return (\n",
    "            enc['input_ids'].squeeze(0), \n",
    "            enc['attention_mask'].squeeze(0), \n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "class AttentionAlignedAdapter(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, rank=64, pooling='mean', fusion='add', scaled=False):\n",
    "        super().__init__()\n",
    "        self.Wd = nn.Linear(hidden_dim, rank, bias=False)\n",
    "        self.Wu = nn.Linear(rank, hidden_dim, bias=False)\n",
    "        self.pooling = pooling\n",
    "        self.fusion = fusion\n",
    "        self.scaled = scaled\n",
    "        if scaled:\n",
    "            self.alpha = nn.Parameter(torch.ones(1))\n",
    "        if fusion == 'concat':\n",
    "            self.fusion_proj = nn.Linear(hidden_dim * 2, hidden_dim, bias=False)\n",
    "        \n",
    "    def forward(self, X, S, A):\n",
    "        if self.pooling == 'mean':\n",
    "            S_pooled = S.mean(dim=1)\n",
    "        elif self.pooling == 'max':\n",
    "            S_pooled = S.max(dim=1)[0]\n",
    "        \n",
    "        Z = self.Wd(X)\n",
    "        Zp = torch.matmul(S_pooled, Z)\n",
    "        deltaA = self.Wu(Zp)\n",
    "        \n",
    "        if self.scaled:\n",
    "            deltaA = self.alpha * deltaA\n",
    "        \n",
    "        if self.fusion == 'add':\n",
    "            return A + deltaA\n",
    "        elif self.fusion == 'concat':\n",
    "            return self.fusion_proj(torch.cat([A, deltaA], dim=-1))\n",
    "        \n",
    "        return A + deltaA\n",
    "\n",
    "class PEFTSelfAttention(nn.Module):\n",
    "    def __init__(self, attn_self, attn_output, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.self_attn = attn_self\n",
    "        self.out_dense = attn_output.dense\n",
    "        self.out_bias = attn_output.dense.bias\n",
    "        \n",
    "        if config['adapter']:\n",
    "            self.adapter = AttentionAlignedAdapter(\n",
    "                hidden_dim=768, \n",
    "                rank=config['rank'],\n",
    "                pooling=config.get('pooling', 'mean'),\n",
    "                fusion=config.get('fusion', 'add'),\n",
    "                scaled=config.get('scaled', False)\n",
    "            )\n",
    "        else:\n",
    "            self.adapter = None\n",
    "        \n",
    "        self.out_dense.weight.requires_grad = False\n",
    "        self.out_bias.requires_grad = config['bias']\n",
    "        \n",
    "        if config.get('trainable_attn', False):\n",
    "            for p in self.self_attn.parameters():\n",
    "                p.requires_grad = True\n",
    "        else:\n",
    "            for p in self.self_attn.parameters():\n",
    "                p.requires_grad = False\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        B, T, H = hidden_states.size()\n",
    "        q = self.self_attn.query(hidden_states)\n",
    "        k = self.self_attn.key(hidden_states)\n",
    "        v = self.self_attn.value(hidden_states)\n",
    "        \n",
    "        q = q.view(B, T, self.self_attn.num_attention_heads, -1).transpose(1, 2)\n",
    "        k = k.view(B, T, self.self_attn.num_attention_heads, -1).transpose(1, 2)\n",
    "        v = v.view(B, T, self.self_attn.num_attention_heads, -1).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(q.size(-1))\n",
    "        scores = scores + attention_mask\n",
    "        S = F.softmax(scores, dim=-1)\n",
    "        A = torch.matmul(S, v)\n",
    "        A = A.transpose(1, 2).contiguous().view(B, T, H)\n",
    "        \n",
    "        if self.adapter is not None:\n",
    "            A = self.adapter(hidden_states, S, A)\n",
    "        \n",
    "        entropy = (-S * torch.log(S + 1e-9)).sum(dim=-1).mean()\n",
    "        return self.out_dense(A), entropy\n",
    "\n",
    "class PEFTEncoderLayer(nn.Module):\n",
    "    def __init__(self, layer, config, layer_idx, apply_peft):\n",
    "        super().__init__()\n",
    "        self.apply_peft = apply_peft\n",
    "        \n",
    "        if apply_peft:\n",
    "            self.attn = PEFTSelfAttention(layer.attention.self, layer.attention.output, config)\n",
    "        else:\n",
    "            self.attn = PEFTSelfAttention(\n",
    "                layer.attention.self, \n",
    "                layer.attention.output, \n",
    "                {'adapter': False, 'bias': False, 'rank': 64, 'trainable_attn': False}\n",
    "            )\n",
    "        \n",
    "        self.attn_ln = layer.attention.output.LayerNorm\n",
    "        self.ffn_int = layer.intermediate.dense\n",
    "        self.ffn_out = layer.output.dense\n",
    "        self.ffn_ln = layer.output.LayerNorm\n",
    "        \n",
    "        for p in self.attn_ln.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.ffn_int.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.ffn_out.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.ffn_ln.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def forward(self, x, attn_mask):\n",
    "        attn_out, entropy = self.attn(x, attn_mask)\n",
    "        x = self.attn_ln(x + attn_out)\n",
    "        ff = self.ffn_out(F.gelu(self.ffn_int(x)))\n",
    "        x = self.ffn_ln(x + ff)\n",
    "        return x, entropy\n",
    "\n",
    "class PEFTUniXcoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = AutoModel.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.embeddings = self.model.embeddings\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        num_layers = len(self.model.encoder.layer)\n",
    "        layer_scope = config.get('layer_scope', 'all')\n",
    "        \n",
    "        for i, l in enumerate(self.model.encoder.layer):\n",
    "            apply_peft = False\n",
    "            \n",
    "            if layer_scope == 'all':\n",
    "                apply_peft = True\n",
    "            elif layer_scope == 'middle':\n",
    "                apply_peft = (i >= 4 and i <= 7)\n",
    "            elif layer_scope == 'specific':\n",
    "                apply_peft = (i < config.get('num_layers', num_layers))\n",
    "            \n",
    "            self.layers.append(PEFTEncoderLayer(l, config, i, apply_peft))\n",
    "        \n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        attn_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * -10000.0\n",
    "        x = self.embeddings(input_ids)\n",
    "        entropies = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, ent = layer(x, attn_mask)\n",
    "            entropies.append(ent)\n",
    "        \n",
    "        logits = self.classifier(x[:, 0, :])\n",
    "        return logits, torch.stack(entropies).mean()\n",
    "\n",
    "def compute_metrics(y_true, y_pred, avg=\"binary\"):\n",
    "    return {\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"prec\": precision_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"rec\": recall_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"jacc\": jaccard_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "        \"kappa\": cohen_kappa_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(config, train_loader, val_loader, test_loader, device):\n",
    "    model = PEFTUniXcoder(config).to(device)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=2e-4\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'config': config,\n",
    "        'trainable_params': trainable_params,\n",
    "        'total_params': total_params,\n",
    "        'trainable_ratio': trainable_params / total_params * 100,\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    total_train_time = 0\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        train_loss, y_true, y_pred = 0, [], []\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for ids, mask, labels in train_loader:\n",
    "            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
    "            \n",
    "            logits, _ = model(ids, mask)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_train_time += epoch_time\n",
    "        train_metrics = compute_metrics(y_true, y_pred)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, y_true, y_pred = 0, [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for ids, mask, labels in val_loader:\n",
    "                ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
    "                logits, _ = model(ids, mask)\n",
    "                val_loss += F.cross_entropy(logits, labels).item()\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        \n",
    "        val_metrics = compute_metrics(y_true, y_pred)\n",
    "        \n",
    "        results['epochs'].append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss / len(train_loader),\n",
    "            'train_f1': train_metrics['f1'],\n",
    "            'val_loss': val_loss / len(val_loader),\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'epoch_time': epoch_time\n",
    "        })\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    infer_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ids, mask, labels in test_loader:\n",
    "            ids, mask = ids.to(device), mask.to(device)\n",
    "            logits, _ = model(ids, mask)\n",
    "            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "            y_true.extend(labels.numpy())\n",
    "    \n",
    "    infer_time = time.time() - infer_start\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2 if torch.cuda.is_available() else 0\n",
    "    \n",
    "    test_metrics = compute_metrics(y_true, y_pred, avg=\"macro\")\n",
    "    \n",
    "    results['test_metrics'] = test_metrics\n",
    "    results['total_train_time'] = total_train_time\n",
    "    results['inference_time'] = infer_time\n",
    "    results['peak_memory_mb'] = peak_memory\n",
    "    results['throughput'] = len(test_loader.dataset) / infer_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "    \n",
    "    full_train = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    val_size = int(0.1 * len(full_train))\n",
    "    train_size = len(full_train) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, num_workers=4)\n",
    "    \n",
    "    ablation_configs = [\n",
    "        {\n",
    "            'name': 'baseline_frozen',\n",
    "            'adapter': False,\n",
    "            'bias': False,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'all',\n",
    "            'trainable_attn': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'bias_only',\n",
    "            'adapter': False,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'all',\n",
    "            'trainable_attn': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'trainable_attention',\n",
    "            'adapter': False,\n",
    "            'bias': False,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'all',\n",
    "            'trainable_attn': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'full_method_mean',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'all',\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'mean',\n",
    "            'fusion': 'add',\n",
    "            'scaled': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'full_method_max',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'all',\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'max',\n",
    "            'fusion': 'add',\n",
    "            'scaled': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'fusion_concat',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'all',\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'mean',\n",
    "            'fusion': 'concat',\n",
    "            'scaled': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'scaled_fusion',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'all',\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'mean',\n",
    "            'fusion': 'add',\n",
    "            'scaled': True\n",
    "        },\n",
    "        {\n",
    "            'name': 'middle_layers_only',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'middle',\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'mean',\n",
    "            'fusion': 'add',\n",
    "            'scaled': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'first_1_layer',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'specific',\n",
    "            'num_layers': 1,\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'mean',\n",
    "            'fusion': 'add',\n",
    "            'scaled': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'first_3_layers',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'specific',\n",
    "            'num_layers': 3,\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'mean',\n",
    "            'fusion': 'add',\n",
    "            'scaled': False\n",
    "        },\n",
    "        {\n",
    "            'name': 'first_6_layers',\n",
    "            'adapter': True,\n",
    "            'bias': True,\n",
    "            'rank': 64,\n",
    "            'layer_scope': 'specific',\n",
    "            'num_layers': 6,\n",
    "            'trainable_attn': False,\n",
    "            'pooling': 'mean',\n",
    "            'fusion': 'add',\n",
    "            'scaled': False\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, config in enumerate(ablation_configs):\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ABLATION {i+1}/{len(ablation_configs)}: {config['name']}\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        try:\n",
    "            results = train_and_evaluate(config, train_loader, val_loader, test_loader, device)\n",
    "            all_results.append(results)\n",
    "            \n",
    "            print(f\"\\nTrainable Parameters: {results['trainable_params']:,} ({results['trainable_ratio']:.4f}%)\")\n",
    "            print(f\"Final Val F1: {results['epochs'][-1]['val_f1']:.4f}\")\n",
    "            print(f\"Test F1: {results['test_metrics']['f1']:.4f}\")\n",
    "            print(f\"Total Training Time: {results['total_train_time']:.2f}s\")\n",
    "            print(f\"Inference Time: {results['inference_time']:.2f}s\")\n",
    "            print(f\"Peak Memory: {results['peak_memory_mb']:.2f}MB\")\n",
    "            print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    with open('ablation_results.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ABLATION STUDY SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for res in all_results:\n",
    "        print(f\"\\n{res['config']['name']}:\")\n",
    "        print(f\"  Trainable: {res['trainable_params']:,} ({res['trainable_ratio']:.4f}%)\")\n",
    "        print(f\"  Test F1: {res['test_metrics']['f1']:.4f}\")\n",
    "        print(f\"  Train Time: {res['total_train_time']:.2f}s\")\n",
    "        print(f\"  Inference: {res['inference_time']:.2f}s\")\n",
    "        print(f\"  Memory: {res['peak_memory_mb']:.2f}MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1e6c5-f19e-4c1d-b93b-90655946be74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A-LoRE train-test, loss, F1 loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ae2b8-2c00-4c9f-9f36-35e4e81b82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_len=512):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.codes = df['code'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.codes[idx], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return enc['input_ids'].squeeze(0), enc['attention_mask'].squeeze(0), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "class AttentionAlignedAdapter(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, rank=64):\n",
    "        super().__init__()\n",
    "        self.Wd = nn.Linear(hidden_dim, rank, bias=False)\n",
    "        self.Wu = nn.Linear(rank, hidden_dim, bias=False)\n",
    "    def forward(self, X, S):\n",
    "        Z = self.Wd(X)\n",
    "        Zp = torch.matmul(S, Z)\n",
    "        return self.Wu(Zp)\n",
    "\n",
    "class PEFTSelfAttention(nn.Module):\n",
    "    def __init__(self, attn_self, attn_output, hidden_dim=768, rank=64):\n",
    "        super().__init__()\n",
    "        self.self_attn = attn_self\n",
    "        self.Wo = attn_output.dense\n",
    "        self.bo = attn_output.dense.bias\n",
    "        self.adapter = AttentionAlignedAdapter(hidden_dim, rank)\n",
    "        \n",
    "        self.Wo.weight.requires_grad = False\n",
    "        self.bo.requires_grad = True\n",
    "        self.stored_S = None\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        B,T,H = hidden_states.size()\n",
    "        q = self.self_attn.query(hidden_states)\n",
    "        k = self.self_attn.key(hidden_states)\n",
    "        v = self.self_attn.value(hidden_states)\n",
    "        \n",
    "        q = q.view(B,T,self.self_attn.num_attention_heads,-1).transpose(1,2)\n",
    "        k = k.view(B,T,self.self_attn.num_attention_heads,-1).transpose(1,2)\n",
    "        v = v.view(B,T,self.self_attn.num_attention_heads,-1).transpose(1,2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / np.sqrt(q.size(-1))\n",
    "        scores = scores + attention_mask\n",
    "        S = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        if not S.requires_grad:\n",
    "            S.requires_grad_(True)\n",
    "        S.retain_grad() \n",
    "        self.stored_S = S \n",
    "        \n",
    "        A = torch.matmul(S, v)\n",
    "        A = A.transpose(1,2).contiguous().view(B,T,H)\n",
    "        \n",
    "        deltaA = self.adapter(hidden_states, S.mean(dim=1))\n",
    "        A = A + deltaA\n",
    "        \n",
    "        entropy = (-S * torch.log(S + 1e-9)).sum(dim=-1).mean()\n",
    "        return self.Wo(A), entropy\n",
    "\n",
    "class PEFTEncoderLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.attn = PEFTSelfAttention(layer.attention.self, layer.attention.output)\n",
    "        self.attn_ln = layer.attention.output.LayerNorm\n",
    "        self.ffn_int = layer.intermediate.dense\n",
    "        self.ffn_out = layer.output.dense\n",
    "        self.ffn_ln = layer.output.LayerNorm\n",
    "        \n",
    "        for p in self.attn_ln.parameters(): p.requires_grad = False\n",
    "        for p in self.ffn_int.parameters(): p.requires_grad = False\n",
    "        for p in self.ffn_out.parameters(): p.requires_grad = False\n",
    "        for p in self.ffn_ln.parameters(): p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        attn_out, entropy = self.attn(x, attn_mask)\n",
    "        x = self.attn_ln(x + attn_out)\n",
    "        ff = self.ffn_out(F.gelu(self.ffn_int(x)))\n",
    "        x = self.ffn_ln(x + ff)\n",
    "        return x, entropy\n",
    "\n",
    "class PEFTUniXcoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.layers = nn.ModuleList([PEFTEncoderLayer(l) for l in self.model.encoder.layer])\n",
    "        self.embeddings = self.model.embeddings\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        attn_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * -10000.0\n",
    "        x = self.embeddings(input_ids)\n",
    "        entropies = []\n",
    "        for layer in self.layers:\n",
    "            x, ent = layer(x, attn_mask)\n",
    "            entropies.append(ent)\n",
    "        logits = self.classifier(x[:,0,:])\n",
    "        return logits, torch.stack(entropies).mean()\n",
    "\n",
    "def compute_metrics(y_true, y_pred, avg=\"binary\"):\n",
    "    return {\n",
    "        \"f1\": f1_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"acc\": accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "full_train = CodeDataset('/traincodex.csv', tokenizer)\n",
    "test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "\n",
    "val_size = int(0.1 * len(full_train))\n",
    "train_size = len(full_train) - val_size\n",
    "train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "model = PEFTUniXcoder().to(device)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4)\n",
    "\n",
    "total_steps = len(train_loader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_model_state = None\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': []}\n",
    "\n",
    "print(\"TRAINABLE PARAMETERS:\")\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"✓ {name}: {p.numel():,}\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss, y_true, y_pred = 0, [], []\n",
    "    \n",
    "    for ids, mask, labels in train_loader:\n",
    "        ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
    "        logits, _ = model(ids, mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        \n",
    "    epoch_train_loss = train_loss/len(train_loader)\n",
    "    epoch_train_f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, y_true, y_pred = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for ids, mask, labels in val_loader:\n",
    "            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
    "            logits, _ = model(ids, mask)\n",
    "            val_loss += F.cross_entropy(logits, labels).item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "            \n",
    "    epoch_val_loss = val_loss/len(val_loader)\n",
    "    epoch_val_f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    \n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['train_f1'].append(epoch_train_f1)\n",
    "    history['val_f1'].append(epoch_val_f1)\n",
    "\n",
    "    print(f\"EPOCH {epoch+1}: Train Loss {epoch_train_loss:.4f} | Val F1: {epoch_val_f1:.4f}\")\n",
    "\n",
    "    if epoch_val_f1 > best_val_f1:\n",
    "        best_val_f1 = epoch_val_f1\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for ids, mask, labels in test_loader:\n",
    "        ids, mask = ids.to(device), mask.to(device)\n",
    "        logits, _ = model(ids, mask)\n",
    "        y_pred.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "print(f\"\\nFINAL TEST F1 (Best Epoch): {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_f1'], label='Train F1')\n",
    "plt.plot(history['val_f1'], label='Val F1')\n",
    "plt.title('F1 Score Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig('performance_graphs.png')\n",
    "print(\"Graphs saved to performance_graphs.png\")\n",
    "\n",
    "print(\"\\nSTATISTICAL PROOF (GRADIENT NORM)\")\n",
    "model.train()\n",
    "subset_loader = DataLoader(Subset(test_dataset, np.random.choice(len(test_dataset), 50, replace=False)), batch_size=1)\n",
    "grad_norms = []\n",
    "\n",
    "for idx, (ids, mask, labels) in enumerate(subset_loader):\n",
    "    ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, _ = model(ids, mask)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    S = model.layers[-1].attn.stored_S\n",
    "    if S.grad is not None:\n",
    "        grad_norms.append(torch.norm(S.grad).item())\n",
    "\n",
    "avg_norm = np.mean(grad_norms)\n",
    "print(f\"Avg Gradient Norm on S: {avg_norm:.6f}\")\n",
    "if avg_norm > 0:\n",
    "    print(\"SUCCESS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442adc3a-c4da-41f0-83a3-89124f1d7543",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# otivational analysis of A-LoRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a986bd-126f-4812-9020-904904b7eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class AttentionAlignedAdapter(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, rank=64):\n",
    "        super().__init__()\n",
    "        self.Wd = nn.Linear(hidden_dim, rank, bias=False)\n",
    "        self.Wu = nn.Linear(rank, hidden_dim, bias=False)\n",
    "        \n",
    "    def forward(self, X, S):\n",
    "        Z = self.Wd(X)\n",
    "        Zp = torch.matmul(S, Z) \n",
    "        return self.Wu(Zp)\n",
    "\n",
    "class PEFTSelfAttention(nn.Module):\n",
    "    def __init__(self, attn_self, attn_output, hidden_dim=768, rank=64):\n",
    "        super().__init__()\n",
    "        self.self_attn = attn_self\n",
    "        self.out_dense = attn_output.dense\n",
    "        self.out_bias = attn_output.dense.bias\n",
    "        self.adapter = AttentionAlignedAdapter(hidden_dim, rank)\n",
    "        \n",
    "        self.out_dense.weight.requires_grad = False\n",
    "        self.out_bias.requires_grad = True\n",
    "        self.adapter_enabled = True \n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        B,T,H = hidden_states.size()\n",
    "        q = self.self_attn.query(hidden_states)\n",
    "        k = self.self_attn.key(hidden_states)\n",
    "        v = self.self_attn.value(hidden_states)\n",
    "        \n",
    "        q = q.view(B,T,self.self_attn.num_attention_heads,-1).transpose(1,2)\n",
    "        k = k.view(B,T,self.self_attn.num_attention_heads,-1).transpose(1,2)\n",
    "        v = v.view(B,T,self.self_attn.num_attention_heads,-1).transpose(1,2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / np.sqrt(q.size(-1))\n",
    "        scores = scores + attention_mask\n",
    "        S = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        A = torch.matmul(S, v)\n",
    "        A = A.transpose(1,2).contiguous().view(B,T,H)\n",
    "        \n",
    "        if self.adapter_enabled:\n",
    "            deltaA = self.adapter(hidden_states, S.mean(dim=1))\n",
    "            A = A + deltaA\n",
    "            \n",
    "        return self.out_dense(A)\n",
    "\n",
    "class PEFTEncoderLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.attn = PEFTSelfAttention(layer.attention.self, layer.attention.output)\n",
    "        self.attn_ln = layer.attention.output.LayerNorm\n",
    "        self.ffn_int = layer.intermediate.dense\n",
    "        self.ffn_out = layer.output.dense\n",
    "        self.ffn_ln = layer.output.LayerNorm\n",
    "        \n",
    "        for p in self.attn_ln.parameters(): p.requires_grad = False\n",
    "        for p in self.ffn_int.parameters(): p.requires_grad = False\n",
    "        for p in self.ffn_out.parameters(): p.requires_grad = False\n",
    "        for p in self.ffn_ln.parameters(): p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        attn_out = self.attn(x, attn_mask)\n",
    "        x = self.attn_ln(x + attn_out)\n",
    "        ff = self.ffn_out(F.gelu(self.ffn_int(x)))\n",
    "        x = self.ffn_ln(x + ff)\n",
    "        return x\n",
    "\n",
    "class PEFTUniXcoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.layers = nn.ModuleList([PEFTEncoderLayer(l) for l in self.model.encoder.layer])\n",
    "        self.embeddings = self.model.embeddings\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        attn_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * -10000.0\n",
    "        x = self.embeddings(input_ids)\n",
    "        x.requires_grad_(True)\n",
    "        x.retain_grad()\n",
    "        self.input_embeddings = x \n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        logits = self.classifier(x[:,0,:])\n",
    "        return logits\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "\n",
    "code_sample = \"\"\"\n",
    "static int subframe_count_exact(FlacEncodeContext *s, FlacSubframe *sub, int pred_order)\n",
    "{\n",
    "    int p, porder, psize;\n",
    "    int i, part_end;\n",
    "    int count = 0;\n",
    "    count += 8;\n",
    "    if (sub->type == FLAC_SUBFRAME_CONSTANT) {\n",
    "        count += sub->obits;\n",
    "    } else {\n",
    "        count += pred_order * sub->obits;\n",
    "        porder = sub->rc.porder;\n",
    "        psize  = s->frame.blocksize >> porder;\n",
    "        for (p = 0; p < 1 << porder; p++) {\n",
    "            int k = sub->rc.params[p];\n",
    "            count += rice_count_exact(&sub->residual[i], part_end - i, k);\n",
    "            i = part_end;\n",
    "        }\n",
    "    }\n",
    "    return count;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(code_sample, return_tensors='pt', truncation=True, max_length=512)\n",
    "ids = inputs['input_ids'].to(device)\n",
    "mask = inputs['attention_mask'].to(device)\n",
    "label = torch.tensor([1], device=device)\n",
    "\n",
    "def get_saliency_map(model, ids, mask, label, use_adapter=True):\n",
    "    for layer in model.layers:\n",
    "        layer.attn.adapter_enabled = use_adapter\n",
    "        \n",
    "    model.zero_grad()\n",
    "    logits = model(ids, mask)\n",
    "    loss = F.cross_entropy(logits, label)\n",
    "    loss.backward()\n",
    "    \n",
    "    grads = model.input_embeddings.grad\n",
    "    if grads is None:\n",
    "        return np.zeros(ids.shape[1])\n",
    "        \n",
    "    saliency = torch.abs(grads).sum(dim=2).squeeze(0)\n",
    "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-9)\n",
    "    return saliency.detach().cpu().numpy()\n",
    "\n",
    "def micro_train_simulation(model, ids, mask, label, steps=20, use_adapter=True):\n",
    "    trainable_params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    if not trainable_params:\n",
    "        return\n",
    "\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=1e-3)\n",
    "    model.train()\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.attn.adapter_enabled = use_adapter\n",
    "        \n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model = PEFTUniXcoder().to(device)\n",
    "\n",
    "saliency_frozen = get_saliency_map(model, ids, mask, label, use_adapter=False)\n",
    "\n",
    "micro_train_simulation(model, ids, mask, label, steps=30, use_adapter=True)\n",
    "saliency_peft = get_saliency_map(model, ids, mask, label, use_adapter=True)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids[0])\n",
    "clean_tokens = [t.replace('Ġ', '') for t in tokens]\n",
    "\n",
    "start_idx = 0\n",
    "for i, t in enumerate(clean_tokens):\n",
    "    if 'for' in t:\n",
    "        start_idx = max(0, i - 10)\n",
    "        break\n",
    "end_idx = min(len(clean_tokens), start_idx + 80)\n",
    "\n",
    "x_range = np.arange(start_idx, end_idx)\n",
    "y_frozen = saliency_frozen[start_idx:end_idx]\n",
    "y_peft = saliency_peft[start_idx:end_idx]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "plt.fill_between(x_range, y_frozen, color='gray', alpha=0.3, label='Frozen Model (Baseline Limit)')\n",
    "plt.plot(x_range, y_frozen, color='gray', linestyle='--', linewidth=1.5)\n",
    "\n",
    "plt.fill_between(x_range, y_peft, color='crimson', alpha=0.4, label='PEFT (Our Mitigation)')\n",
    "plt.plot(x_range, y_peft, color='crimson', linewidth=2.5)\n",
    "\n",
    "plt.title(\"Figure 4: Impact Analysis - Limits vs. Mitigation\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Code Tokens\", fontsize=14)\n",
    "plt.ylabel(\"Saliency / Feature Importance\", fontsize=14)\n",
    "plt.xticks(x_range, clean_tokens[start_idx:end_idx], rotation=90, fontsize=10)\n",
    "plt.legend(fontsize=12, loc='upper left', frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"peft_impact_analysis.png\", dpi=300)\n",
    "print(\"Observation generated: peft_impact_analysis.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
