{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df7eecd-ea97-45fb-a8c5-85234e05cdf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ts peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9ae27-01d2-422b-8e16-81523b96f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row['func'])\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4, alpha=1.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        return result * self.scaling\n",
    "\n",
    "\n",
    "class TSPEFTLayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank=4, alpha=1.0, dropout=0.0, s=4e-5, lambda_reg=1e-5, beta1=0.9, beta2=0.98, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.base_layer.requires_grad_(False)\n",
    "        \n",
    "        self.lora = LoRALayer(\n",
    "            base_layer.in_features,\n",
    "            base_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        self.s = s\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.register_buffer('tau', torch.tensor(0.0))\n",
    "        self.register_buffer('m', torch.tensor(0.0))\n",
    "        self.register_buffer('v', torch.tensor(0.0))\n",
    "        self.register_buffer('step', torch.tensor(0))\n",
    "        \n",
    "    def compute_relative_magnitude(self, base_output, lora_output):\n",
    "        base_norm = torch.norm(base_output, p=2, dim=-1, keepdim=True)\n",
    "        lora_norm = torch.norm(lora_output, p=2, dim=-1, keepdim=True)\n",
    "        r_i = lora_norm / (base_norm + self.eps)\n",
    "        return r_i.squeeze(-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        base_output = self.base_layer(x)\n",
    "        lora_output = self.lora(x)\n",
    "        \n",
    "        if not self.training:\n",
    "            r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "            gate = (r_i >= self.tau).float().unsqueeze(-1)\n",
    "            return base_output + gate * lora_output\n",
    "        \n",
    "        r_i = self.compute_relative_magnitude(base_output, lora_output)\n",
    "        gate = (r_i >= self.tau).float()\n",
    "        \n",
    "        gated_output = base_output + gate.unsqueeze(-1) * lora_output\n",
    "        \n",
    "        self._cache_for_backward = {\n",
    "            'r_i': r_i,\n",
    "            'gate': gate,\n",
    "            'lora_output': lora_output,\n",
    "            'base_output': base_output,\n",
    "        }\n",
    "        \n",
    "        return gated_output\n",
    "    \n",
    "    def compute_threshold_gradient(self, grad_output):\n",
    "        if not hasattr(self, '_cache_for_backward'):\n",
    "            return 0.0\n",
    "            \n",
    "        cache = self._cache_for_backward\n",
    "        r_i = cache['r_i']\n",
    "        gate = cache['gate']\n",
    "        lora_output = cache['lora_output']\n",
    "        \n",
    "        mu_i = (grad_output * lora_output).sum(dim=-1)\n",
    "        \n",
    "        consistency_mask = ((mu_i >= 0).float() == gate).float()\n",
    "        sparsity_mask = gate\n",
    "        \n",
    "        grad_loss = -self.s * (consistency_mask * mu_i).sum()\n",
    "        grad_sparsity = -self.s * (sparsity_mask * self.lambda_reg).sum()\n",
    "        \n",
    "        g_k = grad_loss + grad_sparsity\n",
    "        \n",
    "        return g_k.item()\n",
    "    \n",
    "    def update_threshold(self, grad_output, lr=1.0):\n",
    "        if not self.training:\n",
    "            return\n",
    "            \n",
    "        g_k = self.compute_threshold_gradient(grad_output)\n",
    "        \n",
    "        self.step += 1\n",
    "        \n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g_k\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g_k ** 2)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1 ** self.step.item())\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.step.item())\n",
    "        \n",
    "        tau_update = lr * self.s * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "        self.tau = torch.clamp(self.tau + tau_update, min=0.0)\n",
    "        \n",
    "        if hasattr(self, '_cache_for_backward'):\n",
    "            delattr(self, '_cache_for_backward')\n",
    "\n",
    "\n",
    "class TSPEFTVulnerabilityModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, rank=32, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        self.ts_peft_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, block in enumerate(encoder.block):\n",
    "            q_proj = block.layer[0].SelfAttention.q\n",
    "            v_proj = block.layer[0].SelfAttention.v\n",
    "            \n",
    "            self.ts_peft_layers[f'encoder_q_{i}'] = TSPEFTLayer(\n",
    "                q_proj, rank=rank, alpha=alpha, dropout=dropout, s=s, lambda_reg=lambda_reg\n",
    "            )\n",
    "            self.ts_peft_layers[f'encoder_v_{i}'] = TSPEFTLayer(\n",
    "                v_proj, rank=rank, alpha=alpha, dropout=dropout, s=s, lambda_reg=lambda_reg\n",
    "            )\n",
    "            \n",
    "            block.layer[0].SelfAttention.q = self.ts_peft_layers[f'encoder_q_{i}']\n",
    "            block.layer[0].SelfAttention.v = self.ts_peft_layers[f'encoder_v_{i}']\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def encode_with_ts_peft(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def update_thresholds(self, lr=1.0):\n",
    "        for layer in self.ts_peft_layers.values():\n",
    "            if hasattr(layer, '_cache_for_backward') and layer.training:\n",
    "                grad_output = torch.ones_like(layer._cache_for_backward['base_output'])\n",
    "                layer.update_threshold(grad_output, lr)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        encoded = self.encode_with_ts_peft(input_ids, attention_mask)\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            return loss, logits\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, _ = model(input_ids, attention_mask, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        model.update_thresholds(lr=1.0)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return all_labels, all_preds, all_probs\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing TS-PEFT Model for Vulnerability Detection...\")\n",
    "    model = TSPEFTVulnerabilityModel(model_name, num_classes=2, rank=32, alpha=0.5, dropout=0.05, s=4e-5, lambda_reg=4.5e-5).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING TS-PEFT FOR VULNERABILITY DETECTION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        labels, preds, probs = evaluate(model, test_loader, device)\n",
    "        metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1_binary']:.4f} | AUC: {metrics['roc_auc_binary']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['f1_binary'] > best_f1:\n",
    "            best_f1 = metrics['f1_binary']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    labels, preds, probs = evaluate(model, test_loader, device)\n",
    "    final_metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "    \n",
    "    print_comprehensive_metrics(final_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TS-PEFT VULNERABILITY DETECTION COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef3e93-be57-4531-99d8-3842b0a0d000",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L:oRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff37e0-3178-44d4-a054-56d6a80c3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score, \n",
    "    confusion_matrix, classification_report, average_precision_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / self.rank\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        self.lora = LoRALayer(\n",
    "            linear_layer.in_features,\n",
    "            linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        \n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "class LoRATuningModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.d_model = self.base_model.config.d_model\n",
    "        \n",
    "        self._apply_lora_to_model(rank, alpha)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def _apply_lora_to_model(self, rank, alpha):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        for block in encoder.block:\n",
    "            attention = block.layer[0].SelfAttention\n",
    "            \n",
    "            attention.q = LoRALinear(attention.q, rank=rank, alpha=alpha)\n",
    "            attention.v = LoRALinear(attention.v, rank=rank, alpha=alpha)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing LoRA Tuning Model...\")\n",
    "    model = LoRATuningModel(model_name, num_classes=2, rank=8, alpha=16).to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING LORA TUNING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_labels, val_preds, val_probs = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_metrics = compute_comprehensive_metrics(val_labels, val_preds, val_probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_metrics['accuracy']:.4f} | Val F1 (Macro): {val_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_metrics['f1_macro'] > best_f1:\n",
    "            best_f1 = val_metrics['f1_macro']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_labels, test_preds, test_probs = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    test_metrics = compute_comprehensive_metrics(test_labels, test_preds, test_probs)\n",
    "    \n",
    "    print_comprehensive_metrics(test_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LORA TUNING COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d924e585-55fb-44f6-adb8-dbc55d1e07a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GateRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee1c1f-3532-468a-8f93-8fed0a0001df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row['func'])\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class GatingModule(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.gate_linear = nn.Linear(input_dim, 1, bias=True)\n",
    "        nn.init.zeros_(self.gate_linear.weight)\n",
    "        nn.init.zeros_(self.gate_linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate_linear(x)\n",
    "        gate_values = torch.sigmoid(gate_logits)\n",
    "        return gate_values\n",
    "\n",
    "\n",
    "class GateRALayer(nn.Module):\n",
    "    def __init__(self, base_layer, rank, alpha, dropout, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(input_dim, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, output_dim))\n",
    "        \n",
    "        self.gating_module = GatingModule(input_dim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.base_layer(x)\n",
    "        \n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, hidden_dim = x.shape\n",
    "            x_2d = x.reshape(-1, hidden_dim)\n",
    "        else:\n",
    "            x_2d = x\n",
    "            batch_size = None\n",
    "            seq_len = None\n",
    "        \n",
    "        gate_values = self.gating_module(x_2d)\n",
    "        \n",
    "        lora_output = x_2d @ self.lora_A @ self.lora_B\n",
    "        lora_output = self.dropout_layer(lora_output)\n",
    "        \n",
    "        gated_lora_output = gate_values * lora_output * self.scaling\n",
    "        \n",
    "        if batch_size is not None and seq_len is not None:\n",
    "            gated_lora_output = gated_lora_output.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        final_output = base_output + gated_lora_output\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "\n",
    "class GateRAVulnerabilityModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, rank=16, alpha=16.0, dropout=0.0, entropy_reg_weight=0.01):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        self.d_kv = config.d_kv\n",
    "        self.num_heads = config.num_heads\n",
    "        self.entropy_reg_weight = entropy_reg_weight\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        self.gatera_layers = nn.ModuleDict()\n",
    "        \n",
    "        for i, block in enumerate(encoder.block):\n",
    "            q_proj = block.layer[0].SelfAttention.q\n",
    "            v_proj = block.layer[0].SelfAttention.v\n",
    "            \n",
    "            q_gatera = GateRALayer(\n",
    "                q_proj, rank=rank, alpha=alpha, dropout=dropout, \n",
    "                input_dim=self.d_model, output_dim=self.d_kv * self.num_heads\n",
    "            )\n",
    "            v_gatera = GateRALayer(\n",
    "                v_proj, rank=rank, alpha=alpha, dropout=dropout,\n",
    "                input_dim=self.d_model, output_dim=self.d_kv * self.num_heads\n",
    "            )\n",
    "            \n",
    "            self.gatera_layers[f'encoder_q_{i}'] = q_gatera\n",
    "            self.gatera_layers[f'encoder_v_{i}'] = v_gatera\n",
    "            \n",
    "            block.layer[0].SelfAttention.q = q_gatera\n",
    "            block.layer[0].SelfAttention.v = v_gatera\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def compute_entropy_loss(self, gate_values):\n",
    "        eps = 1e-8\n",
    "        gate_values = torch.clamp(gate_values, eps, 1.0 - eps)\n",
    "        entropy = -gate_values * torch.log(gate_values) - (1 - gate_values) * torch.log(1 - gate_values)\n",
    "        return entropy.mean()\n",
    "    \n",
    "    def encode_with_gatera(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled = sum_hidden / sum_mask\n",
    "        \n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        encoded = self.encode_with_gatera(input_ids, attention_mask)\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        if labels is not None:\n",
    "            classification_loss = F.cross_entropy(logits, labels)\n",
    "            \n",
    "            if self.training and self.entropy_reg_weight > 0:\n",
    "                total_entropy_loss = 0.0\n",
    "                gate_count = 0\n",
    "                \n",
    "                for name, layer in self.gatera_layers.items():\n",
    "                    if hasattr(layer, 'gating_module'):\n",
    "                        try:\n",
    "                            dummy_input = torch.randn(\n",
    "                                256,\n",
    "                                layer.lora_A.shape[0],\n",
    "                                device=input_ids.device\n",
    "                            )\n",
    "                            gate_vals = layer.gating_module(dummy_input)\n",
    "                            entropy_loss = self.compute_entropy_loss(gate_vals)\n",
    "                            total_entropy_loss += entropy_loss\n",
    "                            gate_count += 1\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "                \n",
    "                if gate_count > 0:\n",
    "                    avg_entropy_loss = total_entropy_loss / gate_count\n",
    "                    total_loss = classification_loss + self.entropy_reg_weight * avg_entropy_loss\n",
    "                else:\n",
    "                    total_loss = classification_loss\n",
    "            else:\n",
    "                total_loss = classification_loss\n",
    "            \n",
    "            return total_loss, logits\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, _ = model(input_ids, attention_mask, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return all_labels, all_preds, all_probs\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    frozen = total - trainable\n",
    "    return trainable, total, frozen\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing GateRA Model for Vulnerability Detection...\")\n",
    "    model = GateRAVulnerabilityModel(model_name, num_classes=2, rank=16, alpha=16.0, dropout=0.0, entropy_reg_weight=0.01).to(device)\n",
    "    \n",
    "    trainable_params, total_params, frozen_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters:     {frozen_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING GateRA FOR VULNERABILITY DETECTION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        labels, preds, probs = evaluate(model, test_loader, device)\n",
    "        metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1_binary']:.4f} | AUC: {metrics['roc_auc_binary']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if metrics['f1_binary'] > best_f1:\n",
    "            best_f1 = metrics['f1_binary']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    labels, preds, probs = evaluate(model, test_loader, device)\n",
    "    final_metrics = compute_comprehensive_metrics(labels, preds, probs)\n",
    "    \n",
    "    print_comprehensive_metrics(final_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GateRA VULNERABILITY DETECTION COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eff8c7-3228-46d5-83ab-6e08ba8dfd19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440baa6e-1b7d-4637-80ab-fc998d35e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score, \n",
    "    confusion_matrix, classification_report, average_precision_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class PrefixEncoder(nn.Module):\n",
    "    def __init__(self, prefix_length, num_layers, num_heads, head_dim, prefix_hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(prefix_length, prefix_hidden_size)\n",
    "        \n",
    "        self.trans = nn.Sequential(\n",
    "            nn.Linear(prefix_hidden_size, prefix_hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(prefix_hidden_size, num_layers * 2 * num_heads * head_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_size):\n",
    "        prefix_tokens = torch.arange(self.prefix_length).to(self.embedding.weight.device)\n",
    "        prefix_tokens = prefix_tokens.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        past_key_values = self.trans(self.embedding(prefix_tokens))\n",
    "        \n",
    "        past_key_values = past_key_values.view(\n",
    "            batch_size,\n",
    "            self.prefix_length,\n",
    "            self.num_layers * 2,\n",
    "            self.num_heads,\n",
    "            self.head_dim\n",
    "        )\n",
    "        \n",
    "        past_key_values = past_key_values.permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        past_key_values_list = past_key_values.split(2)\n",
    "        \n",
    "        return past_key_values_list\n",
    "\n",
    "\n",
    "class PrefixTuningModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, prefix_length=10, prefix_hidden_size=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        self.num_layers = config.num_layers\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.d_kv\n",
    "        \n",
    "        self.prefix_length = prefix_length\n",
    "        \n",
    "        self.prefix_encoder = PrefixEncoder(\n",
    "            prefix_length=prefix_length,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=self.num_heads,\n",
    "            head_dim=self.head_dim,\n",
    "            prefix_hidden_size=prefix_hidden_size\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def get_prompt(self, batch_size):\n",
    "        past_key_values = self.prefix_encoder(batch_size)\n",
    "        \n",
    "        past_key_values_formatted = []\n",
    "        for layer_past in past_key_values:\n",
    "            key_past = layer_past[0]\n",
    "            value_past = layer_past[1]\n",
    "            past_key_values_formatted.append((key_past, value_past))\n",
    "        \n",
    "        return tuple(past_key_values_formatted)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        \n",
    "        prefix_attention_mask = torch.ones(\n",
    "            batch_size, self.prefix_length\n",
    "        ).to(attention_mask.device)\n",
    "        \n",
    "        attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing Prefix Tuning Model...\")\n",
    "    model = PrefixTuningModel(model_name, num_classes=2, prefix_length=10, prefix_hidden_size=512).to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING PREFIX TUNING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_labels, val_preds, val_probs = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_metrics = compute_comprehensive_metrics(val_labels, val_preds, val_probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_metrics['accuracy']:.4f} | Val F1 (Macro): {val_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_metrics['f1_macro'] > best_f1:\n",
    "            best_f1 = val_metrics['f1_macro']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_labels, test_preds, test_probs = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    test_metrics = compute_comprehensive_metrics(test_labels, test_preds, test_probs)\n",
    "    \n",
    "    print_comprehensive_metrics(test_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREFIX TUNING COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bb135-3b96-400b-8ec0-6aff11a0abdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Adpater tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075ed89-ace5-44f7-acfd-65560a3784a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score, \n",
    "    confusion_matrix, classification_report, average_precision_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class AdapterModule(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim, non_linearity='gelu'):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(input_dim, bottleneck_dim)\n",
    "        self.up_project = nn.Linear(bottleneck_dim, input_dim)\n",
    "        \n",
    "        if non_linearity == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif non_linearity == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = nn.Tanh()\n",
    "        \n",
    "        nn.init.normal_(self.down_project.weight, std=1e-3)\n",
    "        nn.init.normal_(self.up_project.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.down_project.bias)\n",
    "        nn.init.zeros_(self.up_project.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.down_project(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.up_project(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class AdapterTuningModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2, reduction_factor=16, non_linearity='gelu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        self.num_layers = config.num_layers\n",
    "        self.reduction_factor = reduction_factor\n",
    "        \n",
    "        self.bottleneck_dim = self.d_model // reduction_factor\n",
    "        \n",
    "        self.adapters = nn.ModuleList()\n",
    "        for _ in range(self.num_layers * 2):\n",
    "            adapter = AdapterModule(self.d_model, self.bottleneck_dim, non_linearity)\n",
    "            self.adapters.append(adapter)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        all_hidden_states = outputs.hidden_states\n",
    "        \n",
    "        hidden_states = all_hidden_states[0]\n",
    "        \n",
    "        adapter_idx = 0\n",
    "        for layer_idx in range(1, len(all_hidden_states)):\n",
    "            layer_hidden = all_hidden_states[layer_idx]\n",
    "            layer_hidden = self.adapters[adapter_idx](layer_hidden)\n",
    "            adapter_idx += 1\n",
    "            \n",
    "            if adapter_idx < len(self.adapters):\n",
    "                layer_hidden = self.adapters[adapter_idx](layer_hidden)\n",
    "                adapter_idx += 1\n",
    "            \n",
    "            hidden_states = layer_hidden\n",
    "        \n",
    "        pooled_output = hidden_states[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing Adapter Tuning Model...\")\n",
    "    model = AdapterTuningModel(model_name, num_classes=2, reduction_factor=16, non_linearity='gelu').to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING ADAPTER TUNING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_labels, val_preds, val_probs = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_metrics = compute_comprehensive_metrics(val_labels, val_preds, val_probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_metrics['accuracy']:.4f} | Val F1 (Macro): {val_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_metrics['f1_macro'] > best_f1:\n",
    "            best_f1 = val_metrics['f1_macro']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_labels, test_preds, test_probs = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    test_metrics = compute_comprehensive_metrics(test_labels, test_preds, test_probs)\n",
    "    \n",
    "    print_comprehensive_metrics(test_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ADAPTER TUNING COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0a89c-fdd8-4bb1-866a-7dfae769adc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# BitFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d36857-7ef5-430f-a821-37b5ce3ca13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score, \n",
    "    confusion_matrix, classification_report, average_precision_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_length=512):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        code = str(row.iloc[0])\n",
    "        label = int(row.iloc[1]) if len(row) > 1 else 0\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            code,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class BitFitTuningModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "        \n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self._enable_bias_training()\n",
    "        \n",
    "        config = self.base_model.config\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, 256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def _enable_bias_training(self):\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        encoder = self.base_model.get_encoder()\n",
    "        if hasattr(encoder, 'embed_tokens'):\n",
    "            if encoder.embed_tokens.weight is not None:\n",
    "                encoder.embed_tokens.weight.requires_grad = False\n",
    "        \n",
    "        for block in encoder.block:\n",
    "            for layer in block.layer:\n",
    "                if hasattr(layer, 'layer_norm'):\n",
    "                    if layer.layer_norm.weight is not None:\n",
    "                        layer.layer_norm.weight.requires_grad = True\n",
    "        \n",
    "        if hasattr(encoder, 'final_layer_norm'):\n",
    "            if encoder.final_layer_norm.weight is not None:\n",
    "                encoder.final_layer_norm.weight.requires_grad = True\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoder = self.base_model.get_encoder()\n",
    "        \n",
    "        outputs = encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "\n",
    "def compute_comprehensive_metrics(labels, preds, probs):\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(labels, preds)\n",
    "    \n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    metrics['precision_macro'] = prec_macro\n",
    "    metrics['recall_macro'] = rec_macro\n",
    "    metrics['f1_macro'] = f1_macro\n",
    "    \n",
    "    prec_binary, rec_binary, f1_binary, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    metrics['precision_binary'] = prec_binary\n",
    "    metrics['recall_binary'] = rec_binary\n",
    "    metrics['f1_binary'] = f1_binary\n",
    "    \n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    metrics['precision_weighted'] = prec_weighted\n",
    "    metrics['recall_weighted'] = rec_weighted\n",
    "    metrics['f1_weighted'] = f1_weighted\n",
    "    \n",
    "    prec_per_class, rec_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    metrics['precision_per_class'] = prec_per_class\n",
    "    metrics['recall_per_class'] = rec_per_class\n",
    "    metrics['f1_per_class'] = f1_per_class\n",
    "    metrics['support_per_class'] = support\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_binary'] = roc_auc_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['roc_auc_binary'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['roc_auc_macro'] = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        metrics['roc_auc_macro'] = 0.5\n",
    "    \n",
    "    try:\n",
    "        metrics['pr_auc'] = average_precision_score(labels, probs[:, 1])\n",
    "    except:\n",
    "        metrics['pr_auc'] = 0.5\n",
    "    \n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds)\n",
    "    \n",
    "    metrics['mcc'] = matthews_corrcoef(labels, preds)\n",
    "    \n",
    "    metrics['cohen_kappa'] = cohen_kappa_score(labels, preds)\n",
    "    \n",
    "    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()\n",
    "    metrics['true_negatives'] = tn\n",
    "    metrics['false_positives'] = fp\n",
    "    metrics['false_negatives'] = fn\n",
    "    metrics['true_positives'] = tp\n",
    "    \n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    metrics['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    metrics['fnr'] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    metrics['npv'] = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    metrics['fdr'] = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_comprehensive_metrics(metrics, phase=\"Test\"):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{phase.upper()} EVALUATION METRICS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  Matthews Correlation:    {metrics['mcc']:.4f}\")\n",
    "    print(f\"  Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision (Macro):       {metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro):          {metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):        {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Macro):         {metrics['roc_auc_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBinary Metrics:\")\n",
    "    print(f\"  Precision (Binary):      {metrics['precision_binary']:.4f}\")\n",
    "    print(f\"  Recall (Binary):         {metrics['recall_binary']:.4f}\")\n",
    "    print(f\"  F1-Score (Binary):       {metrics['f1_binary']:.4f}\")\n",
    "    print(f\"  ROC-AUC (Binary):        {metrics['roc_auc_binary']:.4f}\")\n",
    "    print(f\"  PR-AUC:                  {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWeighted Metrics:\")\n",
    "    print(f\"  Precision (Weighted):    {metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):       {metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted):     {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i in range(len(metrics['precision_per_class'])):\n",
    "        print(f\"  Class {i}:\")\n",
    "        print(f\"    Precision:  {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:     {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:   {metrics['f1_per_class'][i]:.4f}\")\n",
    "        print(f\"    Support:    {metrics['support_per_class'][i]}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Components:\")\n",
    "    print(f\"  True Positives:          {metrics['true_positives']}\")\n",
    "    print(f\"  True Negatives:          {metrics['true_negatives']}\")\n",
    "    print(f\"  False Positives:         {metrics['false_positives']}\")\n",
    "    print(f\"  False Negatives:         {metrics['false_negatives']}\")\n",
    "    \n",
    "    print(f\"\\nAdditional Binary Metrics:\")\n",
    "    print(f\"  Sensitivity (TPR):       {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity (TNR):       {metrics['specificity']:.4f}\")\n",
    "    print(f\"  False Positive Rate:     {metrics['fpr']:.4f}\")\n",
    "    print(f\"  False Negative Rate:     {metrics['fnr']:.4f}\")\n",
    "    print(f\"  Negative Pred. Value:    {metrics['npv']:.4f}\")\n",
    "    print(f\"  False Discovery Rate:    {metrics['fdr']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f'Device: {device}')\n",
    "    \n",
    "    model_name = 'Salesforce/codet5p-220m'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = CodeDataset('/traincodex.csv', tokenizer)\n",
    "    test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(\"\\nInitializing BitFit Tuning Model...\")\n",
    "    model = BitFitTuningModel(model_name, num_classes=2).to(device)\n",
    "    \n",
    "    trainable_params, total_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL PARAMETERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Parameters:      {total_params:,}\")\n",
    "    print(f\"Trainable Parameters:  {trainable_params:,}\")\n",
    "    print(f\"Trainable Percentage:  {100 * trainable_params / total_params:.4f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=3e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TRAINING BITFIT TUNING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_labels, val_preds, val_probs = evaluate(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        val_metrics = compute_comprehensive_metrics(val_labels, val_preds, val_probs)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_metrics['accuracy']:.4f} | Val F1 (Macro): {val_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        if val_metrics['f1_macro'] > best_f1:\n",
    "            best_f1 = val_metrics['f1_macro']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL TEST EVALUATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    test_loss, test_labels, test_preds, test_probs = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    test_metrics = compute_comprehensive_metrics(test_labels, test_preds, test_probs)\n",
    "    \n",
    "    print_comprehensive_metrics(test_metrics, phase=\"Test\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BITFIT TUNING COMPLETED\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de251b02-64d3-4f29-bfbc-7183bee0575e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A-Lore Code vul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4aa06-36f1-43ec-bc27-51cbcce49370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, balanced_accuracy_score, \n",
    "                             precision_score, recall_score, f1_score, matthews_corrcoef, \n",
    "                             cohen_kappa_score, jaccard_score)\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_len=512):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.codes = df['code'].astype(str).tolist()\n",
    "        self.labels = df['label'].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.codes[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return (\n",
    "            enc['input_ids'].squeeze(0),\n",
    "            enc['attention_mask'].squeeze(0),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "class AttentionAlignedAdapter(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, rank=64):\n",
    "        super().__init__()\n",
    "        self.Wd = nn.Linear(hidden_dim, rank, bias=False)\n",
    "        self.Wu = nn.Linear(rank, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, X, S):\n",
    "        Z = self.Wd(X)\n",
    "        Zp = torch.matmul(S, Z)\n",
    "        return self.Wu(Zp)\n",
    "\n",
    "class PEFTSelfAttention(nn.Module):\n",
    "    def __init__(self, t5_attn, hidden_dim=768, rank=64):\n",
    "        super().__init__()\n",
    "        self.t5_attn = t5_attn\n",
    "        self.adapter = AttentionAlignedAdapter(hidden_dim, rank)\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        \n",
    "        self.dim = self.t5_attn.d_model\n",
    "        self.n_heads = self.t5_attn.n_heads\n",
    "        self.key_value_proj_dim = self.t5_attn.key_value_proj_dim\n",
    "\n",
    "    def forward(self, hidden_states, mask=None, position_bias=None, **kwargs):\n",
    "        batch_size, seq_length = hidden_states.shape[:2]\n",
    "\n",
    "        q = self.t5_attn.q(hidden_states)\n",
    "        k = self.t5_attn.k(hidden_states)\n",
    "        v = self.t5_attn.v(hidden_states)\n",
    "\n",
    "        def shape(x):\n",
    "            return x.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
    "\n",
    "        q = shape(q)\n",
    "        k = shape(k)\n",
    "        v = shape(v)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(3, 2))\n",
    "\n",
    "        if position_bias is None:\n",
    "            if self.t5_attn.has_relative_attention_bias:\n",
    "                position_bias = self.t5_attn.compute_bias(seq_length, seq_length)\n",
    "            else:\n",
    "                position_bias = torch.zeros_like(scores)\n",
    "\n",
    "        if position_bias is not None:\n",
    "            scores += position_bias\n",
    "\n",
    "        if mask is not None:\n",
    "            scores += mask\n",
    "\n",
    "        attn_weights = F.softmax(scores.float(), dim=-1).type_as(scores)\n",
    "        \n",
    "        outputs = torch.matmul(attn_weights, v)\n",
    "        outputs = outputs.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)\n",
    "\n",
    "        s_align = attn_weights.mean(dim=1)\n",
    "        delta_a = self.adapter(hidden_states, s_align)\n",
    "        \n",
    "        outputs = outputs + delta_a\n",
    "        \n",
    "        outputs = self.t5_attn.o(outputs)\n",
    "        outputs = outputs + self.b_o\n",
    "\n",
    "        return outputs, position_bias\n",
    "\n",
    "class PEFTCodeT5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5p-220m\")\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        encoder = self.model.encoder\n",
    "        for block in encoder.block:\n",
    "            old_attn = block.layer[0].SelfAttention\n",
    "            peft_attn = PEFTSelfAttention(old_attn)\n",
    "            block.layer[0].SelfAttention = peft_attn\n",
    "\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        logits = self.classifier(last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "def compute_metrics(y_true, y_pred, time_taken=0.0, mem_used=0.0, avg=\"macro\"):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Balanced Acc\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"Jaccard\": jaccard_score(y_true, y_pred, average=avg, zero_division=0),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "        \"Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"Specificity\": tn / (tn + fp + 1e-9),\n",
    "        \"NPV\": tn / (tn + fn + 1e-9),\n",
    "        \"FPR\": fp / (fp + tn + 1e-9),\n",
    "        \"FNR\": fn / (fn + tp + 1e-9),\n",
    "        \"Inference Time (s)\": time_taken,\n",
    "        \"Memory (MB)\": mem_used\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def run_evaluation(model, loader, description=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    loss_accum = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=description, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ids, mask, labels in pbar:\n",
    "            ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
    "            logits = model(ids, mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_accum += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            \n",
    "    end_time = time.time()\n",
    "    mem_used = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
    "    \n",
    "    metrics = compute_metrics(y_true, y_pred, end_time - start_time, mem_used, avg=\"macro\")\n",
    "    metrics['Loss'] = loss_accum / len(loader)\n",
    "    return metrics\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-220m\", use_fast=False)\n",
    "full_train = CodeDataset('/traincodex.csv', tokenizer)\n",
    "test_dataset = CodeDataset('/testcodex.csv', tokenizer)\n",
    "\n",
    "val_size = int(0.1 * len(full_train))\n",
    "train_size = len(full_train) - val_size\n",
    "train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model = PEFTCodeT5().to(device)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\", bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, Loss={postfix[0]:.4f}, Acc={postfix[1]:.2f}%]', postfix=[0.0, 0.0])\n",
    "    \n",
    "    for i, (ids, mask, labels) in enumerate(pbar):\n",
    "        ids, mask, labels = ids.to(device), mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        \n",
    "        current_acc = accuracy_score(y_true, y_pred) * 100\n",
    "        pbar.postfix[0] = train_loss / (i + 1)\n",
    "        pbar.postfix[1] = current_acc\n",
    "        pbar.update(0)\n",
    "\n",
    "    val_metrics = run_evaluation(model, val_loader, \"Evaluating\")\n",
    "    \n",
    "    train_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    train_loss_avg = train_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_loss_avg:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss:   {val_metrics['Loss']:.4f} | Val F1:   {val_metrics['F1 Score']:.4f}\\n\")\n",
    "    \n",
    "    if (epoch + 1) == 5 or (epoch + 1) == 10:\n",
    "        print(\"=\"*80)\n",
    "        print(f\"PERFORMING FULL TEST ANALYSIS AT EPOCH {epoch+1}\")\n",
    "        print(\"=\"*80)\n",
    "        test_metrics = run_evaluation(model, test_loader, \"Testing\")\n",
    "        \n",
    "        for k, v in test_metrics.items():\n",
    "            if \"Time\" in k:\n",
    "                print(f\"{k:<20}: {v:.4f} s\")\n",
    "            elif \"Memory\" in k:\n",
    "                print(f\"{k:<20}: {v:.2f} MB\")\n",
    "            else:\n",
    "                print(f\"{k:<20}: {v:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
